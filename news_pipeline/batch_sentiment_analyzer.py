#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æ–°é—»æƒ…æ„Ÿåˆ†æè„šæœ¬ï¼ˆæ”¹è¿›ç‰ˆï¼‰

åŠŸèƒ½è¯´æ˜ï¼š
- è¯»å–period_news_fetcher.pyæŠ“å–çš„æ–°é—»CSVæ–‡ä»¶
- ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ‰¹é‡æƒ…æ„Ÿåˆ†æå’Œè¯„åˆ†
- æ”¯æŒæŒ‰æ—¥æœŸå»é‡ï¼Œé¿å…é‡å¤åˆ†æå·²å¤„ç†çš„æ–°é—»
- å¢é‡å†™å…¥åˆ†æç»“æœï¼Œè¾¹åˆ†æè¾¹å†™å…¥ï¼Œä¿è¯æ•°æ®å®‰å…¨
- å•æ—¥è¯·æ±‚æ•°é‡æ§åˆ¶ï¼Œé¿å…è¶…å‡ºAPIé™é¢

ä½¿ç”¨æ–¹æ³•ï¼š
    python batch_sentiment_analyzer.py --input news_20250901_20250930.csv

å‚æ•°è¯´æ˜ï¼š
    --input: è¾“å…¥çš„æ–°é—»CSVæ–‡ä»¶åï¼ˆä½äºoutput/period_newsç›®å½•ï¼‰
    --batch_size: å•ä¸ªè¯·æ±‚çš„å¹¶è¡Œæ–°é—»æ¡æ•°ï¼Œé»˜è®¤20ï¼ˆå»ºè®®10-50ï¼Œæœ€å¤§100ï¼‰
    --daily_limit: å•æ—¥æœ€å¤§è¯·æ±‚æ•°é‡ï¼Œé»˜è®¤950
    --content_limit: å•æ¡æ–°é—»å†…å®¹é•¿åº¦é™åˆ¶ï¼Œé»˜è®¤1500å­—ç¬¦

è¾“å‡ºæ–‡ä»¶ï¼š
    news_20250901_20250930_analyzed.csv - åˆ†æç»“æœæ–‡ä»¶
"""

import pandas as pd
import json
import requests
import time
import random
from datetime import datetime
from pathlib import Path
import argparse


class BatchSentimentAnalyzer:
    def __init__(self, openrouter_api_key, model,daily_limit=950, content_limit=1500):
        """åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨"""
        print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨...")
        print(f"   æ¨¡å‹: DeepSeek-V3.1 (128K tokens â‰ˆ 20ä¸‡æ±‰å­—)")

        self.api_key = openrouter_api_key
        self.api_url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {openrouter_api_key}",
            "Content-Type": "application/json",
        }
        self.model=model

        # è®¾ç½®æ–‡ä»¶è·¯å¾„
        self.base_dir = Path("D:/projects/q/myQ")
        self.input_dir = self.base_dir / "output" / "period_news"
        self.output_dir = self.base_dir / "output" / "analyzed_news"
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # è¯·æ±‚ç»Ÿè®¡
        self.daily_limit = daily_limit
        self.request_count = 0
        self.content_limit = content_limit
        self.request_log_file = self.output_dir / f"request_log_{datetime.now().strftime('%Y%m%d')}.txt"
        self._load_request_count()

        print(f"âœ… åˆå§‹åŒ–å®Œæˆ")
        print(f"   è¾“å…¥ç›®å½•: {self.input_dir}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   å•æ—¥è¯·æ±‚ä¸Šé™: {self.daily_limit}")
        print(f"   å†…å®¹é•¿åº¦é™åˆ¶: {self.content_limit} å­—ç¬¦")
        print(f"   ä»Šæ—¥å·²ä½¿ç”¨: {self.request_count} æ¬¡")

    def _load_request_count(self):
        """åŠ è½½ä»Šæ—¥è¯·æ±‚è®¡æ•°"""
        if self.request_log_file.exists():
            try:
                with open(self.request_log_file, 'r', encoding='utf-8') as f:
                    self.request_count = int(f.read().strip())
            except:
                self.request_count = 0
        else:
            self.request_count = 0

    def _save_request_count(self):
        """ä¿å­˜è¯·æ±‚è®¡æ•°"""
        with open(self.request_log_file, 'w', encoding='utf-8') as f:
            f.write(str(self.request_count))

    def _check_request_limit(self):
        """æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¯·æ±‚ä¸Šé™"""
        return self.request_count < self.daily_limit

    def _increment_request_count(self):
        """å¢åŠ è¯·æ±‚è®¡æ•°"""
        self.request_count += 1
        self._save_request_count()

    def _suggest_batch_size(self, current_batch_size):
        """æ ¹æ®DeepSeek-V3.1çš„èƒ½åŠ›å»ºè®®batch_size"""
        # ä¼°ç®—å•æ¡æ–°é—»tokenæ•°ï¼ˆä¸­æ–‡çº¦ 1.5-2 å­—ç¬¦ = 1 tokenï¼‰
        avg_tokens_per_news = (self.content_limit // 1.5) + 200  # å†…å®¹ + å…ƒæ•°æ®

        # DeepSeek-V3.1: 128K context (çº¦20ä¸‡æ±‰å­—)
        max_context_tokens = 128000
        base_prompt_tokens = 800  # ç³»ç»Ÿprompt
        response_tokens_reserve = 8000  # é¢„ç•™è¾“å‡ºç©ºé—´

        # å¯ç”¨äºè¾“å…¥çš„tokens
        available_tokens = max_context_tokens - base_prompt_tokens - response_tokens_reserve

        # è®¡ç®—ç†è®ºæœ€å¤§batch_size
        theoretical_max = int(available_tokens // avg_tokens_per_news)

        # å®é™…æ¨èä¸Šé™ï¼ˆè€ƒè™‘ç¨³å®šæ€§å’Œè¾“å‡ºè´¨é‡ï¼‰
        recommended_max = min(100, theoretical_max * 0.8)
        optimal_range = (20, 50)

        print(f"ğŸ“Š Batch Size åˆ†æï¼ˆåŸºäºå†…å®¹é•¿åº¦ {self.content_limit} å­—ç¬¦ï¼‰:")
        print(f"   - ç†è®ºæœ€å¤§å€¼: {theoretical_max}")
        print(f"   - æ¨èä¸Šé™: {int(recommended_max)}")

        if current_batch_size > recommended_max:
            print(f"   âš ï¸ å½“å‰å€¼ {current_batch_size} è¿‡å¤§ï¼Œå»ºè®® â‰¤ {int(recommended_max)}")
        elif current_batch_size < optimal_range[0]:
            print(f"   ğŸ’¡ å½“å‰å€¼ {current_batch_size} åå°ï¼Œå»ºè®® {optimal_range[0]}-{optimal_range[1]} ä»¥æé«˜æ•ˆç‡")
        elif current_batch_size <= optimal_range[1]:
            print(f"   âœ… å½“å‰å€¼ {current_batch_size} è®¾ç½®åˆç†")
        else:
            print(f"   âš¡ å½“å‰å€¼ {current_batch_size} è¾ƒå¤§ï¼Œå……åˆ†åˆ©ç”¨é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›")

    def load_news_csv(self, input_filename):
        """åŠ è½½æ–°é—»CSVæ–‡ä»¶"""
        input_path = self.input_dir / input_filename
        print(f"\nğŸ“ è¯»å–æ–°é—»æ–‡ä»¶: {input_path}")

        if not input_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
            return pd.DataFrame()

        try:
            df = pd.read_csv(input_path, encoding='utf-8-sig')
            print(f"âœ… æˆåŠŸè¯»å– {len(df)} æ¡æ–°é—»")
            return df
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return pd.DataFrame()

    def _load_existing_results(self, output_filename):
        """åŠ è½½å·²å­˜åœ¨çš„åˆ†æç»“æœ"""
        output_path = self.output_dir / output_filename

        if output_path.exists():
            try:
                existing_df = pd.read_csv(output_path, encoding='utf-8-sig')
                print(f"ğŸ“ å‘ç°å·²æœ‰åˆ†æç»“æœ: {len(existing_df)} æ¡è®°å½•")
                return existing_df
            except Exception as e:
                print(f"âš ï¸ è¯»å–å·²æœ‰ç»“æœå¤±è´¥: {e}")
                return pd.DataFrame()
        else:
            print(f"ğŸ“ æœªå‘ç°å·²æœ‰åˆ†æç»“æœï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
            return pd.DataFrame()

    def _detect_duplicates(self, news_df, existing_df):
        """æ£€æµ‹å¹¶è¿‡æ»¤é‡å¤æ–°é—»ï¼ˆåŸºäºæ—¥æœŸå»é‡ï¼Œé¿å…é‡å¤è¯·æ±‚ï¼‰"""
        if existing_df.empty:
            print(f"âœ“ æ— å†å²æ•°æ®ï¼Œæ‰€æœ‰ {len(news_df)} æ¡æ–°é—»éƒ½å°†è¿›è¡Œåˆ†æ")
            return news_df

        print(f"ğŸ” å¼€å§‹æ£€æµ‹é‡å¤æ–°é—»ï¼ˆæŒ‰æ—¥æœŸå»é‡ï¼‰...")

        # æå–æ—¥æœŸï¼ˆä» datetime åˆ—ä¸­æå–æ—¥æœŸéƒ¨åˆ†ï¼‰
        def extract_date(df):
            if 'datetime' not in df.columns:
                return pd.Series([''] * len(df))

            # å°è¯•è§£ææ—¥æœŸ
            dates = pd.to_datetime(df['datetime'], errors='coerce')
            return dates.dt.strftime('%Y-%m-%d').fillna('')

        # ä¸ºæ–°æ•°æ®å’Œå·²æœ‰æ•°æ®æå–æ—¥æœŸ
        news_df['date'] = extract_date(news_df)
        existing_df['date'] = extract_date(existing_df)

        # åˆ›å»ºæ£€æµ‹é”®ï¼ˆæ—¥æœŸ + è‚¡ç¥¨ä»£ç ï¼‰
        news_df['check_key'] = news_df['date'] + '|' + news_df['stock_code'].astype(str)
        existing_df['check_key'] = existing_df['date'] + '|' + existing_df['stock_code'].astype(str)

        # æ£€æµ‹é‡å¤ï¼ˆæŒ‰æ—¥æœŸ+è‚¡ç¥¨ä»£ç ï¼‰
        existing_keys = set(existing_df['check_key'].tolist())
        news_df['is_duplicate'] = news_df['check_key'].isin(existing_keys)

        duplicate_count = news_df['is_duplicate'].sum()
        new_count = (~news_df['is_duplicate']).sum()

        # ç»Ÿè®¡é‡å¤çš„æ—¥æœŸ
        if duplicate_count > 0:
            duplicate_dates = news_df[news_df['is_duplicate']]['date'].unique()
            print(f"ğŸ“Š é‡å¤æ£€æµ‹ç»“æœ:")
            print(f"   - æ€»æ–°é—»æ•°: {len(news_df)}")
            print(f"   - å·²åˆ†æçš„æ—¥æœŸ: {len(duplicate_dates)} å¤© (è·³è¿‡ {duplicate_count} æ¡)")
            print(f"   - å¾…åˆ†æ: {new_count} æ¡")
            print(f"ğŸ’° é¢„è®¡èŠ‚çœ: {duplicate_count} æ¬¡LLMè°ƒç”¨")
        else:
            print(f"ğŸ“Š é‡å¤æ£€æµ‹ç»“æœ:")
            print(f"   - æ€»æ–°é—»æ•°: {len(news_df)}")
            print(f"   - å¾…åˆ†æ: {new_count} æ¡")

        # è¿”å›æœªåˆ†æçš„æ–°é—»
        new_df = news_df[~news_df['is_duplicate']].copy()
        new_df = new_df.drop(['date', 'check_key', 'is_duplicate'], axis=1)

        return new_df

    def analyze_news(self, input_filename, output_filename, batch_size=20):
        """åˆ†ææ–°é—»æƒ…æ„Ÿ"""
        print(f"\nğŸš€ å¼€å§‹æ–°é—»æƒ…æ„Ÿåˆ†æ")

        # æ£€æŸ¥è¯·æ±‚é™é¢
        remaining_requests = self.daily_limit - self.request_count
        if remaining_requests <= 0:
            print(f"âŒ ä»Šæ—¥è¯·æ±‚å·²è¾¾ä¸Šé™ ({self.daily_limit})ï¼Œè¯·æ˜å¤©å†è¯•")
            return

        print(f"ğŸ“Š ä»Šæ—¥å‰©ä½™è¯·æ±‚é¢åº¦: {remaining_requests}/{self.daily_limit}")

        # ç»™å‡ºbatch_sizeå»ºè®®
        self._suggest_batch_size(batch_size)

        # åŠ è½½æ–°é—»æ•°æ®
        news_df = self.load_news_csv(input_filename)
        if news_df.empty:
            print("âŒ æ²¡æœ‰å¯åˆ†æçš„æ–°é—»æ•°æ®")
            return

        # åŠ è½½å·²æœ‰åˆ†æç»“æœ
        existing_df = self._load_existing_results(output_filename)

        # æ£€æµ‹é‡å¤
        new_df = self._detect_duplicates(news_df, existing_df)

        if new_df.empty:
            print(f"\nğŸ‰ æ‰€æœ‰æ–°é—»éƒ½å·²åˆ†æè¿‡ï¼Œæ— éœ€é‡å¤APIè°ƒç”¨ï¼")
            return

        print(f"\nğŸ¤– å¼€å§‹æ‰¹é‡LLMåˆ†æ...")
        print(f"   - å¾…åˆ†æ: {len(new_df)} æ¡")
        print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")

        output_path = self.output_dir / output_filename
        total_batches = (len(new_df) + batch_size - 1) // batch_size

        # è®¡ç®—å®é™…å¯å¤„ç†çš„æ‰¹æ¬¡æ•°
        max_batches = min(total_batches, remaining_requests)
        if max_batches < total_batches:
            print(f"   âš ï¸ ç”±äºè¯·æ±‚é™é¢ï¼Œæœ¬æ¬¡åªèƒ½å¤„ç† {max_batches}/{total_batches} æ‰¹æ¬¡")
            total_batches = max_batches

        print(f"   - æ€»æ‰¹æ¬¡æ•°: {total_batches}\n")

        # åˆ¤æ–­æ˜¯å¦ä¸ºç¬¬ä¸€æ¬¡å†™å…¥
        first_write = not output_path.exists() or existing_df.empty

        # ç»Ÿè®¡
        success_count = 0
        failed_count = 0

        # é€æ‰¹æ¬¡å¤„ç†
        for batch_idx in range(total_batches):
            # å†æ¬¡æ£€æŸ¥è¯·æ±‚é™é¢
            if not self._check_request_limit():
                print(f"\nâš ï¸ å·²è¾¾åˆ°ä»Šæ—¥è¯·æ±‚ä¸Šé™ ({self.daily_limit})ï¼Œåœæ­¢åˆ†æ")
                break

            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(new_df))
            batch_df = new_df.iloc[start_idx:end_idx]

            print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} (æ–°é—» {start_idx + 1}-{end_idx})")
            print(f"   ğŸ“Š å½“å‰è¿›åº¦: {self.request_count}/{self.daily_limit} è¯·æ±‚")

            # è°ƒç”¨LLMåˆ†æ
            batch_results = self._analyze_batch(batch_df)

            if batch_results:
                results_df = pd.DataFrame(batch_results)

                # ç«‹å³å†™å…¥æ–‡ä»¶
                if first_write:
                    results_df.to_csv(output_path, mode='w', index=False, encoding='utf-8-sig')
                    first_write = False
                    print(f"   âœ… å·²å†™å…¥ {len(batch_results)} æ¡ç»“æœï¼ˆåˆ›å»ºæ–‡ä»¶ï¼‰")
                else:
                    results_df.to_csv(output_path, mode='a', header=False, index=False, encoding='utf-8-sig')
                    print(f"   âœ… å·²è¿½åŠ  {len(batch_results)} æ¡ç»“æœ")

                success_count += len(batch_results)
            else:
                print(f"   âŒ æ‰¹æ¬¡åˆ†æå¤±è´¥")
                failed_count += len(batch_df)

            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if batch_idx < total_batches - 1:
                wait_time = 2 + random.uniform(0, 1)
                print(f"   â° ç­‰å¾… {wait_time:.1f} ç§’...\n")
                time.sleep(wait_time)

        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"ğŸ’¾ ç»“æœæ–‡ä»¶: {output_path}")
        print(f"\nğŸ“Š æœ¬æ¬¡ç»Ÿè®¡:")
        print(f"   - æˆåŠŸåˆ†æ: {success_count} æ¡")
        if failed_count > 0:
            print(f"   - åˆ†æå¤±è´¥: {failed_count} æ¡")
        print(f"   - ä½¿ç”¨è¯·æ±‚: {self.request_count}/{self.daily_limit}")

        # ç»Ÿè®¡æœ€ç»ˆç»“æœ
        try:
            final_df = pd.read_csv(output_path, encoding='utf-8-sig')
            print(f"ğŸ“Š æ€»åˆ†ææ•°: {len(final_df)}")
            print(f"ğŸ“Š æ¶‰åŠè‚¡ç¥¨: {final_df['stock_code'].nunique()}")
        except:
            pass

    def _analyze_batch(self, batch_df):
        """æ‰¹é‡åˆ†æä¸€æ‰¹æ–°é—»"""
        prompt = self._build_batch_prompt(batch_df)
        response = self._call_llm_api_with_retry(prompt, batch_df)

        if response:
            # è¯·æ±‚æˆåŠŸï¼Œå¢åŠ è®¡æ•°
            self._increment_request_count()
            return self._parse_batch_response(response, batch_df)
        else:
            return []

    def _build_batch_prompt(self, batch_df):
        """æ„å»ºæ‰¹é‡åˆ†æçš„prompt"""
        prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹å¤šæ¡æ–°é—»å¯¹å¯¹åº”ä¸ªè‚¡çš„å½±å“ã€‚

è¯·å¯¹æ¯æ¡æ–°é—»ç‹¬ç«‹åˆ†æï¼Œä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼š

1. ç›´æ¥å½±å“è¯„ä¼°ï¼ˆ-5åˆ°+5åˆ†ï¼‰
   - æ–°é—»æ˜¯å¦ç›´æ¥æåŠè¯¥å…¬å¸æˆ–å…¶æ ¸å¿ƒä¸šåŠ¡ï¼Ÿ
   - å¯¹å…¬å¸æ”¶å…¥/æˆæœ¬/åˆ©æ¶¦çš„ç›´æ¥å½±å“ï¼Ÿ

2. é—´æ¥å½±å“è¯„ä¼°ï¼ˆ-5åˆ°+5åˆ†ï¼‰
   - å¯¹è¡Œä¸šæ•´ä½“çš„å½±å“ï¼Ÿ
   - å¯¹äº§ä¸šé“¾ä¸Šä¸‹æ¸¸çš„å½±å“ï¼Ÿ
   - å¯¹ç«äº‰æ ¼å±€çš„å½±å“ï¼Ÿ

3. ç¡®å®šæ€§è¯„ä¼°ï¼ˆ0-1ï¼‰
   - å½±å“å‘ç”Ÿçš„å¯èƒ½æ€§æœ‰å¤šå¤§ï¼Ÿ

4. å½±å“æ—¶é—´çª—å£
   - ç«‹å³ã€1å‘¨å†…ã€1ä¸ªæœˆå†…ã€3ä¸ªæœˆå†…ã€6ä¸ªæœˆä»¥ä¸Š

5. ç»¼åˆè¯„åˆ†ï¼ˆ-10åˆ°+10ï¼‰
   - ç»¼åˆè€ƒè™‘ç›´æ¥å½±å“ã€é—´æ¥å½±å“ã€ç¡®å®šæ€§å¾—å‡ºæ€»åˆ†

æ–°é—»åˆ—è¡¨ï¼š
"""

        for idx, (_, row) in enumerate(batch_df.iterrows()):
            content = str(row.get('content', ''))[:self.content_limit]
            prompt += f"""
ã€æ–°é—»{idx+1}ã€‘
è‚¡ç¥¨ï¼š{row['stock_name']}({row['stock_code']})
è¡Œä¸šï¼š{row.get('industry', 'N/A')}
æ ‡é¢˜ï¼š{str(row.get('title', ''))}
å†…å®¹ï¼š{content}
æ—¶é—´ï¼š{row.get('datetime', '')}
"""

        prompt += """

é‡è¦è¦æ±‚ï¼š
1. å¿…é¡»åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–è¯´æ˜æ–‡å­—
2. ä¸è¦ä½¿ç”¨markdownä»£ç å—åŒ…è£¹JSON
3. JSONå¿…é¡»æ˜¯æœ‰æ•ˆæ ¼å¼ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰å¼•å·

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ•°ç»„æ ¼å¼è¿”å›ï¼ˆæ¯ä¸ªå¯¹è±¡å¯¹åº”ä¸€æ¡æ–°é—»ï¼‰ï¼š
[{"news_index":1,"sentiment":"ä¸­æ€§","direct_impact_score":0,"direct_impact_desc":"æè¿°","indirect_impact_score":0,"indirect_impact_desc":"æè¿°","certainty":0.5,"time_to_effect":"1å‘¨å†…","overall_score":0,"risk_factors":"é£é™©","action_suggestion":"å»ºè®®"}]

åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚
"""

        return prompt

    def _call_llm_api_with_retry(self, prompt, batch_df, max_retries=3):
        """è°ƒç”¨LLM APIï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        base_delay = 30

        for attempt in range(max_retries):
            try:
                print(f"   ğŸ”§ è°ƒç”¨LLM API (ç¬¬{attempt+1}æ¬¡)...", end=" ")

                # æ ¹æ®batch_sizeåŠ¨æ€è°ƒæ•´max_tokens
                # æ¯æ¡æ–°é—»çº¦éœ€ 250-350 tokens çš„è¾“å‡º
                estimated_tokens = len(prompt.split()) * 1.5 + 350 * len(batch_df)
                max_tokens = min(16000, max(2000, int(estimated_tokens * 1.2)))

                payload = {
                    "model": self.model,
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": max_tokens
                }

                response = requests.post(
                    self.api_url,
                    headers=self.headers,
                    json=payload,
                    timeout=60
                )

                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    print(f"âœ“")
                    return content

                elif response.status_code == 429:
                    wait_time = base_delay * (2 ** attempt) + random.uniform(0, 1)
                    print(f"âš ï¸ é€Ÿç‡é™åˆ¶(429)")
                    print(f"   ç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•...")
                    if attempt < max_retries - 1:
                        time.sleep(wait_time)
                    continue

                else:
                    wait_time = base_delay * (1.5 ** attempt)
                    print(f"âš ï¸ å¤±è´¥(çŠ¶æ€:{response.status_code})")
                    print(f"   ç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•...")
                    if attempt < max_retries - 1:
                        time.sleep(wait_time)
                    continue

            except Exception as e:
                wait_time = base_delay * (2 ** attempt)
                print(f"âŒ å¼‚å¸¸: {str(e)[:50]}")
                print(f"   ç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•...")
                if attempt < max_retries - 1:
                    time.sleep(wait_time)
                continue

        print(f"   âŒ APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
        return None

    def _parse_batch_response(self, content, batch_df):
        """è§£ææ‰¹é‡å“åº”ï¼ˆå¢å¼ºå®¹é”™ï¼‰"""
        results = []

        try:
            # ä¿å­˜åŸå§‹å“åº”ç”¨äºè°ƒè¯•
            debug_file = self.output_dir / f"debug_response_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

            # æ¸…ç†å†…å®¹
            content = content.strip()

            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            if content.startswith('```json'):
                content = content[7:]
            elif content.startswith('```'):
                content = content[3:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()

            # æŸ¥æ‰¾JSONæ•°ç»„çš„å¼€å§‹å’Œç»“æŸ
            start = content.find('[')
            end = content.rfind(']') + 1

            if start == -1 or end <= start:
                print(f"   âŒ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ•°ç»„ï¼ˆå¯èƒ½è¢«æˆªæ–­ï¼‰")
                # æ£€æŸ¥æ˜¯å¦æ˜¯æˆªæ–­é—®é¢˜
                if start != -1 and len(content) > 10000:
                    print(f"   âš ï¸ å“åº”å¾ˆé•¿ä½†ç¼ºå°‘ç»“æŸç¬¦ï¼Œå¯èƒ½æ˜¯batch_sizeè¿‡å¤§å¯¼è‡´")
                    print(f"   ğŸ’¡ å»ºè®®: å‡å° --batch_size å‚æ•°ï¼ˆå½“å‰å»ºè®®5-10ï¼‰")
                # ä¿å­˜å¤±è´¥çš„å“åº”ç”¨äºè°ƒè¯•
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(f"åŸå§‹å“åº”é•¿åº¦: {len(content)}\n")
                    f.write(f"æŸ¥æ‰¾ç»“æœ: start={start}, end={end}\n")
                    f.write(f"å“åº”å†…å®¹:\n{content}\n")
                return []

            # æå–JSONå­—ç¬¦ä¸²
            json_str = content[start:end]

            # å°è¯•è§£æJSON
            try:
                parsed_array = json.loads(json_str)
            except json.JSONDecodeError as e:
                print(f"   âŒ JSONè§£æé”™è¯¯åœ¨ä½ç½® {e.pos}: {str(e)[:50]}")
                # å°è¯•ä¿®å¤å¸¸è§é—®é¢˜
                json_str_fixed = json_str.replace("'", '"')  # å•å¼•å·æ”¹åŒå¼•å·
                json_str_fixed = json_str_fixed.replace('ï¼š', ':')  # ä¸­æ–‡å†’å·æ”¹è‹±æ–‡
                try:
                    parsed_array = json.loads(json_str_fixed)
                    print(f"   âœ“ JSONä¿®å¤æˆåŠŸ")
                except:
                    # ä¿å­˜å¤±è´¥çš„JSONç”¨äºè°ƒè¯•
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        f.write(f"JSONè§£æå¤±è´¥\né”™è¯¯: {e}\n\n")
                        f.write(f"JSONå†…å®¹:\n{json_str[:2000]}\n")
                    return []

            if not isinstance(parsed_array, list):
                print(f"   âŒ å“åº”ä¸æ˜¯æ•°ç»„æ ¼å¼")
                return []

            # å¤„ç†è§£æç»“æœ
            for i, analysis_result in enumerate(parsed_array):
                if i >= len(batch_df):
                    break

                row = batch_df.iloc[i]
                result = row.to_dict()
                result['analysis_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                # éªŒè¯å¹¶å¡«å……å¿…è¦å­—æ®µ
                required_fields = {
                    'sentiment': 'ä¸­æ€§',
                    'overall_score': 0,
                    'certainty': 0.5,
                    'direct_impact_score': 0,
                    'indirect_impact_score': 0,
                    'direct_impact_desc': '',
                    'indirect_impact_desc': '',
                    'time_to_effect': 'æœªçŸ¥',
                    'risk_factors': '',
                    'action_suggestion': ''
                }

                for field, default_value in required_fields.items():
                    if field not in analysis_result:
                        analysis_result[field] = default_value

                result.update(analysis_result)
                results.append(result)

            print(f"   ğŸ“ è§£ææˆåŠŸ: {len(results)}/{len(batch_df)} æ¡")
            return results

        except Exception as e:
            print(f"   âŒ è§£æå‡ºé”™: {str(e)[:100]}")
            # ä¿å­˜å¼‚å¸¸ä¿¡æ¯
            debug_file = self.output_dir / f"debug_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(f"å¼‚å¸¸: {e}\n\n")
                f.write(f"å“åº”å†…å®¹:\n{content[:2000] if 'content' in locals() else 'N/A'}\n")
            return []


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ‰¹é‡æ–°é—»æƒ…æ„Ÿåˆ†æå·¥å…·ï¼ˆæ”¹è¿›ç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹ï¼š
  # ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆæ¨èé…ç½®ï¼‰
  python batch_sentiment_analyzer.py --input news_20250901_20250930.csv

  # æé€Ÿæ¨¡å¼ï¼šå……åˆ†åˆ©ç”¨DeepSeek-V3.1çš„128Kä¸Šä¸‹æ–‡ï¼ˆ20ä¸‡æ±‰å­—ï¼‰
  python batch_sentiment_analyzer.py --input news_20250901_20250930.csv --batch_size 50

  # è¶…å¤§æ‰¹æ¬¡æ¨¡å¼ï¼ˆé€‚åˆå¤§é‡æ–°é—»ï¼‰
  python batch_sentiment_analyzer.py --input news_20250901_20250930.csv --batch_size 80

  # é«˜è´¨é‡æ¨¡å¼ï¼šæ›´é•¿å†…å®¹ + ä¸­ç­‰æ‰¹æ¬¡
  python batch_sentiment_analyzer.py --input news_20250901_20250930.csv --batch_size 30 --content_limit 2000

è¯´æ˜ï¼š
  - batch_sizeï¼šå•ä¸ªè¯·æ±‚å¤„ç†çš„æ–°é—»æ¡æ•°ï¼Œé»˜è®¤20ï¼Œå»ºè®®10-50ï¼ˆæœ€å¤§100ï¼‰
  - daily_limitï¼šå•æ—¥æœ€å¤§è¯·æ±‚æ•°é‡ï¼Œé»˜è®¤950ï¼Œé¿å…è¶…å‡ºAPIé™é¢
  - content_limitï¼šå•æ¡æ–°é—»å†…å®¹é•¿åº¦é™åˆ¶ï¼Œé»˜è®¤1500å­—ç¬¦
  - æ”¯æŒæŒ‰æ—¥æœŸå»é‡ï¼Œé¿å…é‡å¤åˆ†æåŒä¸€å¤©çš„æ–°é—»
  - è¾¹åˆ†æè¾¹å†™å…¥ï¼Œæ”¯æŒå¢é‡è¿½åŠ 
  - ä¼˜åŒ–é€‚é…DeepSeek-V3.1çš„128Kä¸Šä¸‹æ–‡èƒ½åŠ›ï¼ˆçº¦20ä¸‡æ±‰å­—ï¼‰
        """
    )
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥çš„æ–°é—»CSVæ–‡ä»¶å')
    parser.add_argument('--batch_size', type=int, default=10, help='å•ä¸ªè¯·æ±‚çš„å¹¶è¡Œæ–°é—»æ¡æ•°ï¼Œé»˜è®¤5ï¼ˆGLMæ¨¡å‹å»ºè®®3-10ï¼‰')
    parser.add_argument('--daily_limit', type=int, default=950, help='å•æ—¥æœ€å¤§è¯·æ±‚æ•°é‡ï¼Œé»˜è®¤950')
    parser.add_argument('--content_limit', type=int, default=1500, help='å•æ¡æ–°é—»å†…å®¹é•¿åº¦é™åˆ¶ï¼ˆå­—ç¬¦ï¼‰ï¼Œé»˜è®¤1500')

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if args.batch_size < 1:
        print(f"âŒ batch_sizeå¿…é¡»å¤§äº0")
        return
    elif args.batch_size > 100:
        print(f"âš ï¸ batch_size={args.batch_size} è¶…è¿‡æœ€å¤§é™åˆ¶100")
        print(f"   å¼ºåˆ¶è®¾ç½®ä¸º100")
        args.batch_size = 100

    if args.daily_limit < 1:
        print(f"âŒ daily_limitå¿…é¡»å¤§äº0")
        return

    if args.content_limit < 100:
        print(f"âš ï¸ content_limit={args.content_limit} è¿‡å°ï¼Œå»ºè®®è‡³å°‘300å­—ç¬¦")
    elif args.content_limit > 3000:
        print(f"âš ï¸ content_limit={args.content_limit} è¿‡å¤§ï¼Œå¯èƒ½å½±å“æ•ˆç‡")

    # å¯¼å…¥é…ç½®
    import sys
    sys.path.append(str(Path(__file__).parent.parent / "config"))
    from api_config import OPENROUTER_API_KEY,DEFAULT_MODEL

    print(f"ğŸ”‘ é…ç½®æ£€æŸ¥:")
    print(f"   - OpenRouter Key: {'å·²é…ç½®' if OPENROUTER_API_KEY != 'your_openrouter_api_key_here' else 'æœªé…ç½®'}")
    print(f"   - Batch Size: {args.batch_size}")
    print(f"   - Daily Limit: {args.daily_limit}")
    print(f"   - Content Limit: {args.content_limit} å­—ç¬¦")

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = BatchSentimentAnalyzer(
        OPENROUTER_API_KEY,
        DEFAULT_MODEL,
        daily_limit=args.daily_limit,
        content_limit=args.content_limit
    )

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    input_name = args.input.replace('.csv', '')
    output_filename = f"{input_name}_analyzed.csv"

    # æ‰§è¡Œåˆ†æ
    analyzer.analyze_news(args.input, output_filename, args.batch_size)


if __name__ == "__main__":
    main()
