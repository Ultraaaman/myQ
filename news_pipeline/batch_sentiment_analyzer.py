#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æ–°é—»æƒ…æ„Ÿåˆ†æè„šæœ¬

åŠŸèƒ½è¯´æ˜ï¼š
- è¯»å–period_news_fetcher.pyæŠ“å–çš„æ–°é—»CSVæ–‡ä»¶
- ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ‰¹é‡æƒ…æ„Ÿåˆ†æå’Œè¯„åˆ†
- æ”¯æŒé‡å¤æ£€æµ‹ï¼Œé¿å…é‡å¤åˆ†æå·²å¤„ç†çš„æ–°é—»
- å¢é‡å†™å…¥åˆ†æç»“æœï¼Œä¿è¯æ•°æ®å®‰å…¨

ä½¿ç”¨æ–¹æ³•ï¼š
    python batch_sentiment_analyzer.py --input news_20250901_20250930.csv

å‚æ•°è¯´æ˜ï¼š
    --input: è¾“å…¥çš„æ–°é—»CSVæ–‡ä»¶åï¼ˆä½äºoutput/period_newsç›®å½•ï¼‰
    --batch_size: æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤4æ¡æ–°é—»ä¸€æ‰¹

è¾“å‡ºæ–‡ä»¶ï¼š
    news_20250901_20250930_analyzed.csv - åˆ†æç»“æœæ–‡ä»¶
"""

import pandas as pd
import json
import requests
import time
import random
from datetime import datetime
from pathlib import Path
import argparse


class BatchSentimentAnalyzer:
    def __init__(self, openrouter_api_key):
        """åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨"""
        print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨...")

        self.api_key = openrouter_api_key
        self.api_url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {openrouter_api_key}",
            "Content-Type": "application/json",
        }

        # è®¾ç½®æ–‡ä»¶è·¯å¾„
        self.base_dir = Path("D:/projects/q/myQ")
        self.input_dir = self.base_dir / "output" / "period_news"
        self.output_dir = self.base_dir / "output" / "analyzed_news"
        self.output_dir.mkdir(parents=True, exist_ok=True)

        print(f"âœ… åˆå§‹åŒ–å®Œæˆ")
        print(f"   è¾“å…¥ç›®å½•: {self.input_dir}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")

    def load_news_csv(self, input_filename):
        """åŠ è½½æ–°é—»CSVæ–‡ä»¶"""
        input_path = self.input_dir / input_filename
        print(f"\nğŸ“ è¯»å–æ–°é—»æ–‡ä»¶: {input_path}")

        if not input_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
            return pd.DataFrame()

        try:
            df = pd.read_csv(input_path, encoding='utf-8-sig')
            print(f"âœ… æˆåŠŸè¯»å– {len(df)} æ¡æ–°é—»")
            return df
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return pd.DataFrame()

    def _load_existing_results(self, output_filename):
        """åŠ è½½å·²å­˜åœ¨çš„åˆ†æç»“æœ"""
        output_path = self.output_dir / output_filename

        if output_path.exists():
            try:
                existing_df = pd.read_csv(output_path, encoding='utf-8-sig')
                print(f"ğŸ“ å‘ç°å·²æœ‰åˆ†æç»“æœ: {len(existing_df)} æ¡è®°å½•")
                return existing_df
            except Exception as e:
                print(f"âš ï¸ è¯»å–å·²æœ‰ç»“æœå¤±è´¥: {e}")
                return pd.DataFrame()
        else:
            print(f"ğŸ“ æœªå‘ç°å·²æœ‰åˆ†æç»“æœï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
            return pd.DataFrame()

    def _detect_duplicates(self, news_df, existing_df):
        """æ£€æµ‹å¹¶è¿‡æ»¤é‡å¤æ–°é—»ï¼ˆåŸºäºå†…å®¹æŒ‡çº¹ï¼‰"""
        if existing_df.empty:
            print(f"âœ“ æ— å†å²æ•°æ®ï¼Œæ‰€æœ‰ {len(news_df)} æ¡æ–°é—»éƒ½å°†è¿›è¡Œåˆ†æ")
            return news_df

        print(f"ğŸ” å¼€å§‹æ£€æµ‹é‡å¤æ–°é—»...")

        # åˆ›å»ºå†…å®¹æŒ‡çº¹
        def create_content_fingerprint(df):
            if 'content' in df.columns:
                content_col = 'content'
            elif 'title' in df.columns:
                content_col = 'title'
            else:
                return pd.Series([''] * len(df))

            cleaned = df[content_col].fillna('').astype(str).str.strip()
            cleaned = cleaned.str.replace('\n', ' ', regex=False).str.replace('\r', '', regex=False).str.replace('\t', ' ', regex=False)
            cleaned = cleaned.str.replace(r'\d{4}-\d{2}-\d{2}', '', regex=True)
            cleaned = cleaned.str.replace(r'\d{1,2}:\d{2}(:\d{2})?', '', regex=True)
            cleaned = cleaned.str.replace(r'\s+', ' ', regex=True)
            fingerprint = cleaned.str[:150]
            return fingerprint

        # ä¸ºæ–°æ•°æ®å’Œå·²æœ‰æ•°æ®åˆ›å»ºæŒ‡çº¹
        news_df['fingerprint'] = create_content_fingerprint(news_df)
        existing_df['fingerprint'] = create_content_fingerprint(existing_df)

        # åˆ›å»ºæ£€æµ‹é”®ï¼ˆå†…å®¹æŒ‡çº¹ + è‚¡ç¥¨ä»£ç ï¼‰
        news_df['check_key'] = news_df['fingerprint'] + '|' + news_df['stock_code'].astype(str)
        existing_df['check_key'] = existing_df['fingerprint'] + '|' + existing_df['stock_code'].astype(str)

        # æ£€æµ‹é‡å¤
        existing_keys = set(existing_df['check_key'].tolist())
        news_df['is_duplicate'] = news_df['check_key'].isin(existing_keys)

        duplicate_count = news_df['is_duplicate'].sum()
        new_count = (~news_df['is_duplicate']).sum()

        print(f"ğŸ“Š é‡å¤æ£€æµ‹ç»“æœ:")
        print(f"   - æ€»æ–°é—»æ•°: {len(news_df)}")
        print(f"   - å·²åˆ†æè¿‡: {duplicate_count} æ¡ (è·³è¿‡)")
        print(f"   - å¾…åˆ†æ: {new_count} æ¡")

        if duplicate_count > 0:
            print(f"ğŸ’° é¢„è®¡èŠ‚çœ: {duplicate_count} æ¬¡LLMè°ƒç”¨")

        # è¿”å›æœªåˆ†æçš„æ–°é—»
        new_df = news_df[~news_df['is_duplicate']].copy()
        new_df = new_df.drop(['fingerprint', 'check_key', 'is_duplicate'], axis=1)

        return new_df

    def analyze_news(self, input_filename, output_filename, batch_size=4):
        """åˆ†ææ–°é—»æƒ…æ„Ÿ"""
        print(f"\nğŸš€ å¼€å§‹æ–°é—»æƒ…æ„Ÿåˆ†æ")

        # åŠ è½½æ–°é—»æ•°æ®
        news_df = self.load_news_csv(input_filename)
        if news_df.empty:
            print("âŒ æ²¡æœ‰å¯åˆ†æçš„æ–°é—»æ•°æ®")
            return

        # åŠ è½½å·²æœ‰åˆ†æç»“æœ
        existing_df = self._load_existing_results(output_filename)

        # æ£€æµ‹é‡å¤
        new_df = self._detect_duplicates(news_df, existing_df)

        if new_df.empty:
            print(f"\nğŸ‰ æ‰€æœ‰æ–°é—»éƒ½å·²åˆ†æè¿‡ï¼Œæ— éœ€é‡å¤APIè°ƒç”¨ï¼")
            return

        print(f"\nğŸ¤– å¼€å§‹æ‰¹é‡LLMåˆ†æ...")
        print(f"   - å¾…åˆ†æ: {len(new_df)} æ¡")
        print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")

        output_path = self.output_dir / output_filename
        total_batches = (len(new_df) + batch_size - 1) // batch_size
        print(f"   - æ€»æ‰¹æ¬¡æ•°: {total_batches}\n")

        # åˆ¤æ–­æ˜¯å¦ä¸ºç¬¬ä¸€æ¬¡å†™å…¥
        first_write = not output_path.exists() or existing_df.empty

        # é€æ‰¹æ¬¡å¤„ç†
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(new_df))
            batch_df = new_df.iloc[start_idx:end_idx]

            print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} (æ–°é—» {start_idx + 1}-{end_idx})")

            # è°ƒç”¨LLMåˆ†æ
            batch_results = self._analyze_batch(batch_df)

            if batch_results:
                results_df = pd.DataFrame(batch_results)

                # ç«‹å³å†™å…¥æ–‡ä»¶
                if first_write:
                    results_df.to_csv(output_path, mode='w', index=False, encoding='utf-8-sig')
                    first_write = False
                    print(f"   âœ… å·²å†™å…¥ {len(batch_results)} æ¡ç»“æœï¼ˆåˆ›å»ºæ–‡ä»¶ï¼‰")
                else:
                    results_df.to_csv(output_path, mode='a', header=False, index=False, encoding='utf-8-sig')
                    print(f"   âœ… å·²è¿½åŠ  {len(batch_results)} æ¡ç»“æœ")
            else:
                print(f"   âŒ æ‰¹æ¬¡åˆ†æå¤±è´¥")

            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if batch_idx < total_batches - 1:
                wait_time = 2 + random.uniform(0, 1)
                print(f"   â° ç­‰å¾… {wait_time:.1f} ç§’...\n")
                time.sleep(wait_time)

        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"ğŸ’¾ ç»“æœæ–‡ä»¶: {output_path}")

        # ç»Ÿè®¡æœ€ç»ˆç»“æœ
        try:
            final_df = pd.read_csv(output_path, encoding='utf-8-sig')
            print(f"ğŸ“Š æ€»åˆ†ææ•°: {len(final_df)}")
            print(f"ğŸ“Š æ¶‰åŠè‚¡ç¥¨: {final_df['stock_code'].nunique()}")
        except:
            pass

    def _analyze_batch(self, batch_df):
        """æ‰¹é‡åˆ†æä¸€æ‰¹æ–°é—»"""
        prompt = self._build_batch_prompt(batch_df)
        response = self._call_llm_api_with_retry(prompt)

        if response:
            return self._parse_batch_response(response, batch_df)
        else:
            return []

    def _build_batch_prompt(self, batch_df):
        """æ„å»ºæ‰¹é‡åˆ†æçš„prompt"""
        prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹å¤šæ¡æ–°é—»å¯¹å¯¹åº”ä¸ªè‚¡çš„å½±å“ã€‚

è¯·å¯¹æ¯æ¡æ–°é—»ç‹¬ç«‹åˆ†æï¼Œä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼š

1. ç›´æ¥å½±å“è¯„ä¼°ï¼ˆ-5åˆ°+5åˆ†ï¼‰
   - æ–°é—»æ˜¯å¦ç›´æ¥æåŠè¯¥å…¬å¸æˆ–å…¶æ ¸å¿ƒä¸šåŠ¡ï¼Ÿ
   - å¯¹å…¬å¸æ”¶å…¥/æˆæœ¬/åˆ©æ¶¦çš„ç›´æ¥å½±å“ï¼Ÿ

2. é—´æ¥å½±å“è¯„ä¼°ï¼ˆ-5åˆ°+5åˆ†ï¼‰
   - å¯¹è¡Œä¸šæ•´ä½“çš„å½±å“ï¼Ÿ
   - å¯¹äº§ä¸šé“¾ä¸Šä¸‹æ¸¸çš„å½±å“ï¼Ÿ
   - å¯¹ç«äº‰æ ¼å±€çš„å½±å“ï¼Ÿ

3. ç¡®å®šæ€§è¯„ä¼°ï¼ˆ0-1ï¼‰
   - å½±å“å‘ç”Ÿçš„å¯èƒ½æ€§æœ‰å¤šå¤§ï¼Ÿ

4. å½±å“æ—¶é—´çª—å£
   - ç«‹å³ã€1å‘¨å†…ã€1ä¸ªæœˆå†…ã€3ä¸ªæœˆå†…ã€6ä¸ªæœˆä»¥ä¸Š

5. ç»¼åˆè¯„åˆ†ï¼ˆ-10åˆ°+10ï¼‰
   - ç»¼åˆè€ƒè™‘ç›´æ¥å½±å“ã€é—´æ¥å½±å“ã€ç¡®å®šæ€§å¾—å‡ºæ€»åˆ†

æ–°é—»åˆ—è¡¨ï¼š
"""

        for idx, (_, row) in enumerate(batch_df.iterrows()):
            content = str(row.get('content', ''))[:600]
            prompt += f"""
ã€æ–°é—»{idx+1}ã€‘
è‚¡ç¥¨ï¼š{row['stock_name']}({row['stock_code']})
è¡Œä¸šï¼š{row.get('industry', 'N/A')}
æ ‡é¢˜ï¼š{str(row.get('title', ''))}
å†…å®¹ï¼š{content}
æ—¶é—´ï¼š{row.get('datetime', '')}
"""

        prompt += """
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ•°ç»„æ ¼å¼è¿”å›ï¼Œæ•°ç»„ä¸­æ¯ä¸ªå¯¹è±¡å¯¹åº”ä¸€æ¡æ–°é—»ï¼š
[
  {
    "news_index": 1,
    "sentiment": "å¼ºçƒˆæ­£é¢/æ­£é¢/ä¸­æ€§åæ­£/ä¸­æ€§/ä¸­æ€§åè´Ÿ/è´Ÿé¢/å¼ºçƒˆè´Ÿé¢",
    "direct_impact_score": åˆ†æ•°,
    "direct_impact_desc": "æè¿°",
    "indirect_impact_score": åˆ†æ•°,
    "indirect_impact_desc": "æè¿°",
    "certainty": 0.xx,
    "time_to_effect": "æ—¶é—´çª—å£",
    "overall_score": ç»¼åˆåˆ†æ•°,
    "risk_factors": "ä¸»è¦é£é™©å› ç´ ",
    "action_suggestion": "å»ºè®®æ“ä½œ"
  },
  ...
]
"""

        return prompt

    def _call_llm_api_with_retry(self, prompt, max_retries=3):
        """è°ƒç”¨LLM APIï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        base_delay = 20

        for attempt in range(max_retries):
            try:
                print(f"   ğŸ”§ è°ƒç”¨LLM API (ç¬¬{attempt+1}æ¬¡)...", end=" ")

                payload = {
                    "model": "deepseek/deepseek-chat-v3.1:free",
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 4000
                }

                response = requests.post(
                    self.api_url,
                    headers=self.headers,
                    json=payload,
                    timeout=60
                )

                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    print(f"âœ“")
                    return content

                elif response.status_code == 429:
                    wait_time = base_delay * (2 ** attempt) + random.uniform(0, 1)
                    print(f"âš ï¸ é€Ÿç‡é™åˆ¶(429)")
                    print(f"   ç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•...")
                    if attempt < max_retries - 1:
                        time.sleep(wait_time)
                    continue

                else:
                    wait_time = base_delay * (1.5 ** attempt)
                    print(f"âš ï¸ å¤±è´¥(çŠ¶æ€:{response.status_code})")
                    print(f"   ç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•...")
                    if attempt < max_retries - 1:
                        time.sleep(wait_time)
                    continue

            except Exception as e:
                wait_time = base_delay * (2 ** attempt)
                print(f"âŒ å¼‚å¸¸: {str(e)[:50]}")
                print(f"   ç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•...")
                if attempt < max_retries - 1:
                    time.sleep(wait_time)
                continue

        print(f"   âŒ APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
        return None

    def _parse_batch_response(self, content, batch_df):
        """è§£ææ‰¹é‡å“åº”"""
        results = []

        try:
            # æ¸…ç†å†…å®¹
            content = content.strip()
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()

            # æŸ¥æ‰¾JSONæ•°ç»„
            start = content.find('[')
            end = content.rfind(']') + 1

            if start != -1 and end > start:
                json_str = content[start:end]
                parsed_array = json.loads(json_str)

                if isinstance(parsed_array, list):
                    for i, analysis_result in enumerate(parsed_array):
                        if i < len(batch_df):
                            row = batch_df.iloc[i]
                            result = row.to_dict()
                            result['analysis_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                            # éªŒè¯å¿…è¦å­—æ®µ
                            required_fields = ['sentiment', 'overall_score', 'certainty']
                            for field in required_fields:
                                if field not in analysis_result:
                                    analysis_result[field] = 0 if field in ['overall_score', 'certainty'] else "ä¸­æ€§"

                            result.update(analysis_result)
                            results.append(result)

                    print(f"   ğŸ“ è§£ææˆåŠŸ: {len(results)} æ¡")
                    return results
            else:
                print(f"   âŒ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ•°ç»„")
                return []

        except json.JSONDecodeError as e:
            print(f"   âŒ JSONè§£æé”™è¯¯: {e}")
            return []
        except Exception as e:
            print(f"   âŒ è§£æå‡ºé”™: {e}")
            return []


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰¹é‡æ–°é—»æƒ…æ„Ÿåˆ†æå·¥å…·')
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥çš„æ–°é—»CSVæ–‡ä»¶å')
    parser.add_argument('--batch_size', type=int, default=4, help='æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤4')

    args = parser.parse_args()

    # å¯¼å…¥é…ç½®
    import sys
    sys.path.append(str(Path(__file__).parent.parent / "config"))
    from api_config import OPENROUTER_API_KEY

    print(f"ğŸ”‘ é…ç½®æ£€æŸ¥:")
    print(f"   - OpenRouter Key: {'å·²é…ç½®' if OPENROUTER_API_KEY != 'your_openrouter_api_key_here' else 'æœªé…ç½®'}")

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = BatchSentimentAnalyzer(OPENROUTER_API_KEY)

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    input_name = args.input.replace('.csv', '')
    output_filename = f"{input_name}_analyzed.csv"

    # æ‰§è¡Œåˆ†æ
    analyzer.analyze_news(args.input, output_filename, args.batch_size)


if __name__ == "__main__":
    main()
