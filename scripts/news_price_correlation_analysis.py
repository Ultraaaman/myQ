#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»è¯„åˆ†ä¸è‚¡ä»·ç›¸å…³æ€§åˆ†æ
==============================

æœ¬è„šæœ¬åˆ†ææ–°é—»æƒ…æ„Ÿè¯„åˆ†ä¸è‚¡ä»·å˜åŒ–çš„ç›¸å…³æ€§ï¼ŒåŒ…æ‹¬ï¼š
1. è¯»å–æ–°é—»è¯„åˆ†æ•°æ®
2. è·å–å¯¹åº”æ—¶é—´çš„è‚¡ä»·æ•°æ®
3. æ•°æ®å¯¹é½å’Œèšåˆ
4. å¯è§†åŒ–åˆ†æ
5. ç›¸å…³æ€§ç»Ÿè®¡åˆ†æ

ä½œè€…: Claude Code
æ—¥æœŸ: 2025-09-22
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import sys
import os
from scipy.stats import pearsonr, spearmanr

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(r'D:\projects\q\myQ')

# å¯¼å…¥ market_data æ¨¡å—
try:
    from quantlib.market_data import get_stock_data, MarketDataManager
    MARKET_DATA_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥ quantlib.market_data æ¨¡å—: {e}")
    print("å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
    MARKET_DATA_AVAILABLE = False

# è®¾ç½®å›¾è¡¨æ ·å¼
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8')

def load_news_data(file_path):
    """
    è¯»å–æ–°é—»è¯„åˆ†æ•°æ®

    Args:
        file_path (str): æ–°é—»è¯„åˆ†CSVæ–‡ä»¶è·¯å¾„

    Returns:
        pd.DataFrame: å¤„ç†åçš„æ–°é—»æ•°æ®
    """
    print("=" * 50)
    print("æ­¥éª¤ 1: è¯»å–æ–°é—»è¯„åˆ†æ•°æ®")
    print("=" * 50)

    try:
        news_df = pd.read_csv(file_path, encoding='utf-8-sig')
        print(f"âœ“ æˆåŠŸè¯»å–æ•°æ®ï¼Œå½¢çŠ¶: {news_df.shape}")

        # è½¬æ¢æ—¶é—´æ ¼å¼
        news_df['date'] = pd.to_datetime(news_df['original_date'])
        news_df = news_df.sort_values('date')

        print(f"âœ“ æ—¶é—´èŒƒå›´: {news_df['date'].min().date()} åˆ° {news_df['date'].max().date()}")

        # æ˜¾ç¤ºæ›´è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
        print(f"âœ“ æ•°æ®è´¨é‡æ£€æŸ¥:")
        print(f"  - overall_score èŒƒå›´: {news_df['overall_score'].min()} åˆ° {news_df['overall_score'].max()}")
        print(f"  - overall_score å‡å€¼: {news_df['overall_score'].mean():.2f}")

        if 'sentiment' in news_df.columns:
            print(f"  - æƒ…ç»ªåˆ†å¸ƒ: {dict(news_df['sentiment'].value_counts().head(3))}")

        if 'certainty' in news_df.columns:
            print(f"  - ç¡®å®šæ€§å‡å€¼: {news_df['certainty'].mean():.2f}")

        if 'action_suggestion' in news_df.columns:
            print(f"  - è¡ŒåŠ¨å»ºè®®åˆ†å¸ƒ: {dict(news_df['action_suggestion'].value_counts().head(3))}")

        return news_df

    except Exception as e:
        print(f"âœ— è¯»å–æ–°é—»æ•°æ®å¤±è´¥: {e}")
        return None

def aggregate_daily_scores(news_df):
    """
    æŒ‰æ—¥æœŸèšåˆæ–°é—»è¯„åˆ†ï¼ˆå¢å¼ºç‰ˆï¼‰

    Args:
        news_df (pd.DataFrame): åŸå§‹æ–°é—»æ•°æ®

    Returns:
        pd.DataFrame: æ—¥åº¦èšåˆçš„è¯„åˆ†æ•°æ®
    """
    print("\n" + "=" * 50)
    print("æ­¥éª¤ 2: æŒ‰æ—¥æœŸèšåˆæ–°é—»è¯„åˆ†")
    print("=" * 50)

    # æ„å»ºèšåˆå­—å…¸
    agg_dict = {
        'overall_score': ['mean', 'sum', 'count', 'std', 'min', 'max'],
        'sentiment': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'neutral'
    }

    # æ·»åŠ å…¶ä»–å¯ç”¨åˆ—çš„èšåˆ
    if 'direct_impact_score' in news_df.columns:
        agg_dict['direct_impact_score'] = ['mean', 'std']

    if 'indirect_impact_score' in news_df.columns:
        agg_dict['indirect_impact_score'] = ['mean', 'std']

    if 'certainty' in news_df.columns:
        agg_dict['certainty'] = ['mean', 'std']

    if 'action_suggestion' in news_df.columns:
        agg_dict['action_suggestion'] = lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'neutral'

    # æŒ‰æ—¥æœŸèšåˆ
    daily_scores = news_df.groupby(news_df['date'].dt.date).agg(agg_dict).round(3)

    # å±•å¹³åˆ—å
    new_columns = []
    for col in daily_scores.columns:
        if isinstance(col, tuple):
            if col[1] == '<lambda>':
                new_columns.append(f"dominant_{col[0]}")
            else:
                new_columns.append(f"{col[0]}_{col[1]}")
        else:
            new_columns.append(col)

    daily_scores.columns = new_columns
    daily_scores.reset_index(inplace=True)
    daily_scores['date'] = pd.to_datetime(daily_scores['date'])

    # å¡«å……ç¼ºå¤±å€¼
    numeric_cols = daily_scores.select_dtypes(include=[np.number]).columns
    daily_scores[numeric_cols] = daily_scores[numeric_cols].fillna(0)

    # è®¡ç®—ç»„åˆæŒ‡æ ‡
    if 'direct_impact_score_mean' in daily_scores.columns and 'indirect_impact_score_mean' in daily_scores.columns:
        daily_scores['combined_impact'] = (daily_scores['direct_impact_score_mean'] + daily_scores['indirect_impact_score_mean']) / 2

    if 'certainty_mean' in daily_scores.columns:
        # ç¡®å®šæ€§åŠ æƒçš„ç»¼åˆè¯„åˆ†
        daily_scores['weighted_score'] = daily_scores['overall_score_mean'] * daily_scores['certainty_mean']

    print(f"âœ“ èšåˆå®Œæˆï¼Œå…± {len(daily_scores)} ä¸ªäº¤æ˜“æ—¥")
    print(f"âœ“ èšåˆç»´åº¦: {len(daily_scores.columns)} ä¸ªæŒ‡æ ‡")
    print(f"âœ“ ä¸»è¦æŒ‡æ ‡:")

    key_metrics = ['overall_score_mean', 'overall_score_std', 'news_count', 'dominant_sentiment']
    available_metrics = [col for col in key_metrics if col in daily_scores.columns]

    if available_metrics:
        print(daily_scores[['date'] + available_metrics].head())

    return daily_scores

def get_stock_price_data(daily_scores):
    """
    è·å–è‚¡ä»·æ•°æ®

    Args:
        daily_scores (pd.DataFrame): æ—¥åº¦æ–°é—»è¯„åˆ†æ•°æ®

    Returns:
        pd.DataFrame: è‚¡ä»·æ•°æ®
    """
    print("\n" + "=" * 50)
    print("æ­¥éª¤ 3: è·å–è‚¡ä»·æ•°æ®")
    print("=" * 50)

    if MARKET_DATA_AVAILABLE:
        try:
            from quantlib.market_data import DataProviderFactory

            # æ£€æŸ¥æ”¯æŒçš„å¸‚åœº
            supported_markets = DataProviderFactory.get_supported_markets()
            print(f"æ”¯æŒçš„å¸‚åœº: {supported_markets}")

            # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
            data_manager = MarketDataManager()

            # æ‰©å±•æ—¶é—´èŒƒå›´
            start_date = daily_scores['date'].min() - timedelta(days=5)
            end_date = daily_scores['date'].max() + timedelta(days=5)

            print(f"è·å–è‚¡ä»·æ•°æ®æ—¶é—´èŒƒå›´: {start_date.date()} åˆ° {end_date.date()}")

            # å°è¯•ä¸åŒçš„å¸‚åœºæ ‡è¯†ç¬¦å’Œæ—¶é—´å‘¨æœŸè·å–ç´«é‡‘çŸ¿ä¸š(601899)è‚¡ä»·æ•°æ®
            market_options = ['CN', 'Aè‚¡', 'CHINA']
            period_options = ['3mo', '1mo', '2mo', '90d', '60d']
            stock_data = None

            for market in market_options:
                if market in supported_markets:
                    print(f"å°è¯•ä½¿ç”¨å¸‚åœºæ ‡è¯†ç¬¦: {market}")
                    for period in period_options:
                        try:
                            print(f"  - å°è¯•æ—¶é—´å‘¨æœŸ: {period}")
                            stock_data = data_manager.get_stock_data('601899', market=market, period=period, interval='1d')
                            if stock_data is not None and len(stock_data) > 0:
                                print(f"âœ“ ä½¿ç”¨ {market}, period={period} æˆåŠŸè·å–è‚¡ä»·æ•°æ®ï¼Œå½¢çŠ¶: {stock_data.shape}")
                                break
                        except Exception as e:
                            print(f"    Ã— period={period} å¤±è´¥: {e}")
                            continue

                    if stock_data is not None and len(stock_data) > 0:
                        break
                else:
                    print(f"Ã— ä¸æ”¯æŒå¸‚åœºæ ‡è¯†ç¬¦: {market}")

            # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ä¾¿æ·å‡½æ•°
            if stock_data is None:
                try:
                    from quantlib.market_data import get_stock_data
                    print("å°è¯•ä½¿ç”¨ä¾¿æ·å‡½æ•° get_stock_data...")
                    stock_data = get_stock_data('601899', market='CN', period='1mo')
                    if stock_data is not None:
                        print(f"âœ“ ä¾¿æ·å‡½æ•°æˆåŠŸè·å–æ•°æ®ï¼Œå½¢çŠ¶: {stock_data.shape}")
                except Exception as e:
                    print(f"Ã— ä¾¿æ·å‡½æ•°ä¹Ÿå¤±è´¥: {e}")

            if stock_data is not None and len(stock_data) > 0:
                # æ ¹æ®akshareæµ‹è¯•ç»“æœï¼Œå¤„ç†æ•°æ®æ ¼å¼
                stock_data_clean = stock_data.copy()

                # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰dateåˆ—
                if 'date' not in stock_data_clean.columns:
                    # å¦‚æœæ²¡æœ‰dateåˆ—ï¼Œæ£€æŸ¥ç´¢å¼•æˆ–å…¶ä»–æ—¥æœŸåˆ—
                    if hasattr(stock_data_clean.index, 'date'):
                        stock_data_clean['date'] = pd.to_datetime(stock_data_clean.index.date)
                    elif 'æ—¥æœŸ' in stock_data_clean.columns:
                        stock_data_clean['date'] = pd.to_datetime(stock_data_clean['æ—¥æœŸ'])
                    else:
                        # å‡è®¾ç´¢å¼•å°±æ˜¯æ—¥æœŸ
                        stock_data_clean['date'] = pd.to_datetime(stock_data_clean.index)
                else:
                    # ç¡®ä¿dateåˆ—æ˜¯datetimeæ ¼å¼
                    stock_data_clean['date'] = pd.to_datetime(stock_data_clean['date'])

                # ç¡®ä¿æœ‰å¿…è¦çš„OHLCVåˆ—ï¼ˆå¤„ç†ä¸­è‹±æ–‡åˆ—åï¼‰
                column_mapping = {
                    'å¼€ç›˜': 'Open',
                    'æœ€é«˜': 'High',
                    'æœ€ä½': 'Low',
                    'æ”¶ç›˜': 'Close',
                    'æˆäº¤é‡': 'Volume',
                    'open': 'Open',
                    'high': 'High',
                    'low': 'Low',
                    'close': 'Close',
                    'volume': 'Volume'
                }

                for old_col, new_col in column_mapping.items():
                    if old_col in stock_data_clean.columns and new_col not in stock_data_clean.columns:
                        stock_data_clean[new_col] = stock_data_clean[old_col]

                print(f"âœ“ æ•°æ®å¤„ç†å®Œæˆï¼Œæ—¶é—´èŒƒå›´: {stock_data_clean['date'].min().date()} åˆ° {stock_data_clean['date'].max().date()}")
                print(f"âœ“ æ•°æ®åˆ—: {[col for col in stock_data_clean.columns if col in ['date', 'Open', 'High', 'Low', 'Close', 'Volume']]}")
                return stock_data_clean
            else:
                print("âœ— æ‰€æœ‰å¸‚åœºæ ‡è¯†ç¬¦éƒ½è·å–æ•°æ®å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return create_simulated_stock_data(daily_scores)

        except Exception as e:
            print(f"âœ— è·å–è‚¡ä»·æ•°æ®æ—¶å‡ºé”™: {e}")
            print("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
            return create_simulated_stock_data(daily_scores)
    else:
        print("quantlib.market_data æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
        return create_simulated_stock_data(daily_scores)

def create_simulated_stock_data(daily_scores):
    """
    åˆ›å»ºæ¨¡æ‹Ÿè‚¡ä»·æ•°æ®ç”¨äºæ¼”ç¤º

    Args:
        daily_scores (pd.DataFrame): æ—¥åº¦æ–°é—»è¯„åˆ†æ•°æ®

    Returns:
        pd.DataFrame: æ¨¡æ‹Ÿè‚¡ä»·æ•°æ®
    """
    print("åˆ›å»ºæ¨¡æ‹Ÿè‚¡ä»·æ•°æ®...")

    np.random.seed(42)
    dates = pd.date_range(
        start=daily_scores['date'].min() - timedelta(days=2),
        end=daily_scores['date'].max() + timedelta(days=2),
        freq='D'
    )

    simulated_prices = []
    base_price = 25.0  # ç´«é‡‘çŸ¿ä¸šå¤§æ¦‚ä»·æ ¼

    for i, date in enumerate(dates):
        if i == 0:
            price = base_price
        else:
            # éšæœºæ¸¸èµ° + ä¸€äº›è¶‹åŠ¿
            change = np.random.normal(0, 0.015)  # 1.5%çš„æ—¥æ³¢åŠ¨
            price = max(simulated_prices[-1] * (1 + change), 1.0)  # ç¡®ä¿ä»·æ ¼å¤§äº0
        simulated_prices.append(price)

    # åˆ›å»ºå®Œæ•´çš„OHLCVæ•°æ®
    stock_data_sim = pd.DataFrame({
        'date': dates,
        'Open': [p * (1 + np.random.normal(0, 0.005)) for p in simulated_prices],
        'High': [p * (1 + abs(np.random.normal(0, 0.01))) for p in simulated_prices],
        'Low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in simulated_prices],
        'Close': simulated_prices,
        'Volume': np.random.randint(1000000, 10000000, len(dates))
    })

    # ç¡®ä¿OHLCå…³ç³»æ­£ç¡®
    for i in range(len(stock_data_sim)):
        high = max(stock_data_sim.loc[i, 'Open'], stock_data_sim.loc[i, 'Close'], stock_data_sim.loc[i, 'High'])
        low = min(stock_data_sim.loc[i, 'Open'], stock_data_sim.loc[i, 'Close'], stock_data_sim.loc[i, 'Low'])
        stock_data_sim.loc[i, 'High'] = high
        stock_data_sim.loc[i, 'Low'] = low

    print(f"âœ“ æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºå®Œæˆï¼Œå½¢çŠ¶: {stock_data_sim.shape}")
    return stock_data_sim

def merge_data(daily_scores, stock_data):
    """
    åˆå¹¶æ–°é—»è¯„åˆ†å’Œè‚¡ä»·æ•°æ®

    Args:
        daily_scores (pd.DataFrame): æ—¥åº¦æ–°é—»è¯„åˆ†æ•°æ®
        stock_data (pd.DataFrame): è‚¡ä»·æ•°æ®

    Returns:
        pd.DataFrame: åˆå¹¶åçš„æ•°æ®
    """
    print("\n" + "=" * 50)
    print("æ­¥éª¤ 4: æ•°æ®å¯¹é½å’Œåˆå¹¶")
    print("=" * 50)

    # ç¡®å®šè¦åˆå¹¶çš„è‚¡ä»·åˆ—
    stock_columns = ['date']
    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
        if col in stock_data.columns:
            stock_columns.append(col)

    print(f"âœ“ å‡†å¤‡åˆå¹¶çš„è‚¡ä»·åˆ—: {stock_columns}")

    # åˆå¹¶æ•°æ®
    merged_data = pd.merge(
        daily_scores,
        stock_data[stock_columns],
        on='date',
        how='inner'
    )

    print(f"âœ“ åˆå¹¶åæ•°æ®å½¢çŠ¶: {merged_data.shape}")

    if len(merged_data) > 0:
        # è®¡ç®—ä»·æ ¼å˜åŒ–ç‡
        merged_data = merged_data.sort_values('date').reset_index(drop=True)
        merged_data['price_change'] = merged_data['Close'].pct_change() * 100
        merged_data['price_change_next'] = (merged_data['Close'].shift(-1) / merged_data['Close'] - 1) * 100
        merged_data['volume_change'] = merged_data['Volume'].pct_change() * 100

        print(f"âœ“ æ—¶é—´èŒƒå›´: {merged_data['date'].min().date()} åˆ° {merged_data['date'].max().date()}")
        print("\nåˆå¹¶æ•°æ®é¢„è§ˆ:")

        # åŠ¨æ€ç¡®å®šè¯„åˆ†åˆ—å
        score_col = 'overall_score_mean' if 'overall_score_mean' in merged_data.columns else 'score_mean'
        preview_cols = ['date', score_col, 'Close', 'price_change']
        available_cols = [col for col in preview_cols if col in merged_data.columns]

        if len(available_cols) >= 3:
            print(merged_data[available_cols].head())
        else:
            print("åˆ—åä¸åŒ¹é…ï¼Œæ˜¾ç¤ºæ‰€æœ‰å¯ç”¨åˆ—:")
            print(f"å¯ç”¨åˆ—: {list(merged_data.columns)}")
            print(merged_data[['date', 'Close']].head())

    return merged_data

def plot_time_series(merged_data):
    """
    ç»˜åˆ¶å¢å¼ºçš„æ—¶é—´åºåˆ—å›¾

    Args:
        merged_data (pd.DataFrame): åˆå¹¶åçš„æ•°æ®
    """
    print("\n" + "=" * 50)
    print("æ­¥éª¤ 5: ç»˜åˆ¶æ—¶é—´åºåˆ—åˆ†æå›¾")
    print("=" * 50)

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # å›¾1: ç»¼åˆè¯„åˆ†æ—¶é—´åºåˆ—
    ax1 = axes[0, 0]
    ax1.plot(merged_data['date'], merged_data['overall_score_mean'],
             'b-o', linewidth=2.5, markersize=6, alpha=0.8, label='Overall Score')

    # æ·»åŠ æ ‡å‡†å·®å¡«å……
    if 'overall_score_std' in merged_data.columns:
        ax1.fill_between(merged_data['date'],
                         merged_data['overall_score_mean'] - merged_data['overall_score_std'],
                         merged_data['overall_score_mean'] + merged_data['overall_score_std'],
                         alpha=0.2, color='blue', label='Â±1 Std Dev')

    ax1.set_title('Overall Score Time Series', fontsize=14, fontweight='bold')
    ax1.set_ylabel('Overall Score', fontsize=12)
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    ax1.tick_params(axis='x', rotation=45)

    # å›¾2: ç›´æ¥å½±å“ vs é—´æ¥å½±å“
    ax2 = axes[0, 1]
    if 'direct_impact_score_mean' in merged_data.columns and 'indirect_impact_score_mean' in merged_data.columns:
        ax2.plot(merged_data['date'], merged_data['direct_impact_score_mean'],
                 'g-o', linewidth=2, markersize=5, alpha=0.8, label='Direct Impact')
        ax2.plot(merged_data['date'], merged_data['indirect_impact_score_mean'],
                 'orange', linewidth=2, markersize=5, alpha=0.8, label='Indirect Impact')
        ax2.set_title('Direct vs Indirect Impact', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Impact Score', fontsize=12)
        ax2.legend()
    else:
        ax2.text(0.5, 0.5, 'Direct/Indirect Impact\nData Not Available',
                 ha='center', va='center', transform=ax2.transAxes, fontsize=12)
        ax2.set_title('Direct vs Indirect Impact', fontsize=14, fontweight='bold')

    ax2.grid(True, alpha=0.3)
    ax2.tick_params(axis='x', rotation=45)

    # å›¾3: è‚¡ä»·ä¸è¯„åˆ†å¯¹æ¯”
    ax3 = axes[1, 0]
    ax3_twin = ax3.twinx()

    # å·¦è½´ï¼šæ–°é—»è¯„åˆ†
    line1 = ax3.plot(merged_data['date'], merged_data['overall_score_mean'],
                     'b-o', linewidth=2, markersize=5, label='News Score', alpha=0.8)
    ax3.set_ylabel('News Score', color='blue', fontsize=12)
    ax3.tick_params(axis='y', labelcolor='blue')
    ax3.grid(True, alpha=0.3)

    # å³è½´ï¼šè‚¡ä»·
    line2 = ax3_twin.plot(merged_data['date'], merged_data['Close'],
                          'r-s', linewidth=2, markersize=5, label='Close Price', alpha=0.8)
    ax3_twin.set_ylabel('Stock Price (CNY)', color='red', fontsize=12)
    ax3_twin.tick_params(axis='y', labelcolor='red')

    ax3.set_title('News Score vs Stock Price', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Date', fontsize=12)
    ax3.tick_params(axis='x', rotation=45)

    # åˆå¹¶å›¾ä¾‹
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax3.legend(lines, labels, loc='upper left')

    # å›¾4: ç¡®å®šæ€§åŠ æƒè¯„åˆ†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    ax4 = axes[1, 1]
    if 'weighted_score' in merged_data.columns:
        ax4.plot(merged_data['date'], merged_data['weighted_score'],
                 'purple', linewidth=2.5, markersize=6, alpha=0.8, label='Weighted Score')
        ax4.plot(merged_data['date'], merged_data['overall_score_mean'],
                 'b--', linewidth=1.5, alpha=0.6, label='Original Score')
        ax4.set_title('Certainty-Weighted Score', fontsize=14, fontweight='bold')
        ax4.set_ylabel('Weighted Score', fontsize=12)
        ax4.legend()
    else:
        # æ˜¾ç¤ºæ–°é—»æ•°é‡åˆ†å¸ƒ
        # æŸ¥æ‰¾æ–°é—»æ•°é‡åˆ—
        count_col = 'overall_score_count' if 'overall_score_count' in merged_data.columns else 'news_count'
        if count_col in merged_data.columns:
            ax4.bar(merged_data['date'], merged_data[count_col],
                    alpha=0.6, color='green', width=0.8)
        else:
            ax4.text(0.5, 0.5, 'No News Count Data', ha='center', va='center', transform=ax4.transAxes)
        ax4.set_title('Daily News Count', fontsize=14, fontweight='bold')
        ax4.set_ylabel('News Count', fontsize=12)

    ax4.grid(True, alpha=0.3)
    ax4.set_xlabel('Date', fontsize=12)
    ax4.tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

def plot_correlation_analysis(merged_data):
    """
    ç»˜åˆ¶å¢å¼ºçš„ç›¸å…³æ€§åˆ†æå›¾

    Args:
        merged_data (pd.DataFrame): åˆå¹¶åçš„æ•°æ®
    """
    print("\n" + "=" * 50)
    print("æ­¥éª¤ 6: ç»˜åˆ¶ç›¸å…³æ€§åˆ†æå›¾")
    print("=" * 50)

    fig = plt.figure(figsize=(18, 14))

    # å›¾1: ç»¼åˆè¯„åˆ† vs å½“æ—¥è‚¡ä»·å˜åŒ–
    ax1 = plt.subplot(3, 3, 1)
    score_col = 'overall_score_mean' if 'overall_score_mean' in merged_data.columns else 'score_mean'
    valid_data = merged_data.dropna(subset=[score_col, 'price_change'])
    if len(valid_data) > 1:
        ax1.scatter(valid_data[score_col], valid_data['price_change'],
                   alpha=0.7, s=60, c='blue', edgecolors='navy', linewidth=0.5)
        if len(valid_data) > 2:
            z = np.polyfit(valid_data[score_col], valid_data['price_change'], 1)
            p = np.poly1d(z)
            ax1.plot(valid_data[score_col], p(valid_data[score_col]),
                    "r--", alpha=0.8, linewidth=2)
        ax1.set_xlabel('Overall Score', fontsize=10)
        ax1.set_ylabel('Same Day Î”%', fontsize=10)
        ax1.set_title('Overall Score vs Same Day Price', fontsize=12, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        ax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)
        ax1.axvline(x=0, color='k', linestyle='-', alpha=0.3)

    # å›¾2: ç»¼åˆè¯„åˆ† vs æ¬¡æ—¥è‚¡ä»·å˜åŒ–
    ax2 = plt.subplot(3, 3, 2)
    valid_data_next = merged_data.dropna(subset=[score_col, 'price_change_next'])
    if len(valid_data_next) > 1:
        ax2.scatter(valid_data_next[score_col], valid_data_next['price_change_next'],
                   alpha=0.7, s=60, c='green', edgecolors='forestgreen', linewidth=0.5)
        if len(valid_data_next) > 2:
            z = np.polyfit(valid_data_next[score_col], valid_data_next['price_change_next'], 1)
            p = np.poly1d(z)
            ax2.plot(valid_data_next[score_col], p(valid_data_next[score_col]),
                    "r--", alpha=0.8, linewidth=2)
        ax2.set_xlabel('Overall Score', fontsize=10)
        ax2.set_ylabel('Next Day Î”%', fontsize=10)
        ax2.set_title('Overall Score vs Next Day Price', fontsize=12, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)
        ax2.axvline(x=0, color='k', linestyle='-', alpha=0.3)

    # å›¾3: ç›´æ¥å½±å“ vs è‚¡ä»·å˜åŒ–
    ax3 = plt.subplot(3, 3, 3)
    if 'direct_impact_score_mean' in merged_data.columns:
        valid_direct = merged_data.dropna(subset=['direct_impact_score_mean', 'price_change'])
        if len(valid_direct) > 1:
            ax3.scatter(valid_direct['direct_impact_score_mean'], valid_direct['price_change'],
                       alpha=0.7, s=60, c='orange', edgecolors='orangered', linewidth=0.5)
            if len(valid_direct) > 2:
                z = np.polyfit(valid_direct['direct_impact_score_mean'], valid_direct['price_change'], 1)
                p = np.poly1d(z)
                ax3.plot(valid_direct['direct_impact_score_mean'], p(valid_direct['direct_impact_score_mean']),
                        "r--", alpha=0.8, linewidth=2)
            ax3.set_xlabel('Direct Impact', fontsize=10)
            ax3.set_ylabel('Same Day Î”%', fontsize=10)
            ax3.set_title('Direct Impact vs Price Change', fontsize=12, fontweight='bold')
        else:
            ax3.text(0.5, 0.5, 'No Direct Impact Data', ha='center', va='center', transform=ax3.transAxes)
    else:
        ax3.text(0.5, 0.5, 'No Direct Impact Data', ha='center', va='center', transform=ax3.transAxes)
    ax3.grid(True, alpha=0.3)

    # å›¾4: é—´æ¥å½±å“ vs è‚¡ä»·å˜åŒ–
    ax4 = plt.subplot(3, 3, 4)
    if 'indirect_impact_score_mean' in merged_data.columns:
        valid_indirect = merged_data.dropna(subset=['indirect_impact_score_mean', 'price_change'])
        if len(valid_indirect) > 1:
            ax4.scatter(valid_indirect['indirect_impact_score_mean'], valid_indirect['price_change'],
                       alpha=0.7, s=60, c='purple', edgecolors='indigo', linewidth=0.5)
            if len(valid_indirect) > 2:
                z = np.polyfit(valid_indirect['indirect_impact_score_mean'], valid_indirect['price_change'], 1)
                p = np.poly1d(z)
                ax4.plot(valid_indirect['indirect_impact_score_mean'], p(valid_indirect['indirect_impact_score_mean']),
                        "r--", alpha=0.8, linewidth=2)
            ax4.set_xlabel('Indirect Impact', fontsize=10)
            ax4.set_ylabel('Same Day Î”%', fontsize=10)
            ax4.set_title('Indirect Impact vs Price Change', fontsize=12, fontweight='bold')
        else:
            ax4.text(0.5, 0.5, 'No Indirect Impact Data', ha='center', va='center', transform=ax4.transAxes)
    else:
        ax4.text(0.5, 0.5, 'No Indirect Impact Data', ha='center', va='center', transform=ax4.transAxes)
    ax4.grid(True, alpha=0.3)

    # å›¾5: ç¡®å®šæ€§ vs è‚¡ä»·å˜åŒ–
    ax5 = plt.subplot(3, 3, 5)
    if 'certainty_mean' in merged_data.columns:
        valid_certainty = merged_data.dropna(subset=['certainty_mean', 'price_change'])
        if len(valid_certainty) > 1:
            ax5.scatter(valid_certainty['certainty_mean'], valid_certainty['price_change'],
                       alpha=0.7, s=60, c='red', edgecolors='crimson', linewidth=0.5)
            if len(valid_certainty) > 2:
                z = np.polyfit(valid_certainty['certainty_mean'], valid_certainty['price_change'], 1)
                p = np.poly1d(z)
                ax5.plot(valid_certainty['certainty_mean'], p(valid_certainty['certainty_mean']),
                        "r--", alpha=0.8, linewidth=2)
            ax5.set_xlabel('Certainty', fontsize=10)
            ax5.set_ylabel('Same Day Î”%', fontsize=10)
            ax5.set_title('Certainty vs Price Change', fontsize=12, fontweight='bold')
        else:
            ax5.text(0.5, 0.5, 'No Certainty Data', ha='center', va='center', transform=ax5.transAxes)
    else:
        ax5.text(0.5, 0.5, 'No Certainty Data', ha='center', va='center', transform=ax5.transAxes)
    ax5.grid(True, alpha=0.3)

    # å›¾6: åŠ æƒè¯„åˆ† vs è‚¡ä»·å˜åŒ–
    ax6 = plt.subplot(3, 3, 6)
    if 'weighted_score' in merged_data.columns:
        valid_weighted = merged_data.dropna(subset=['weighted_score', 'price_change'])
        if len(valid_weighted) > 1:
            ax6.scatter(valid_weighted['weighted_score'], valid_weighted['price_change'],
                       alpha=0.7, s=60, c='brown', edgecolors='saddlebrown', linewidth=0.5)
            if len(valid_weighted) > 2:
                z = np.polyfit(valid_weighted['weighted_score'], valid_weighted['price_change'], 1)
                p = np.poly1d(z)
                ax6.plot(valid_weighted['weighted_score'], p(valid_weighted['weighted_score']),
                        "r--", alpha=0.8, linewidth=2)
            ax6.set_xlabel('Weighted Score', fontsize=10)
            ax6.set_ylabel('Same Day Î”%', fontsize=10)
            ax6.set_title('Weighted Score vs Price Change', fontsize=12, fontweight='bold')
        else:
            ax6.text(0.5, 0.5, 'No Weighted Score Data', ha='center', va='center', transform=ax6.transAxes)
    else:
        ax6.text(0.5, 0.5, 'No Weighted Score Data', ha='center', va='center', transform=ax6.transAxes)
    ax6.grid(True, alpha=0.3)

    # å›¾7-9: ç›¸å…³æ€§çƒ­åŠ›å›¾ï¼ˆåˆå¹¶æˆä¸€ä¸ªå¤§å›¾ï¼‰
    ax_heatmap = plt.subplot(3, 3, (7, 9))

    # æ„å»ºç›¸å…³æ€§çŸ©é˜µçš„åˆ—
    correlation_columns = []
    column_mapping = {}

    # åŸºç¡€åˆ—
    if score_col in merged_data.columns:
        correlation_columns.append(score_col)
        column_mapping[score_col] = 'Overall Score'

    if 'direct_impact_score_mean' in merged_data.columns:
        correlation_columns.append('direct_impact_score_mean')
        column_mapping['direct_impact_score_mean'] = 'Direct Impact'

    if 'indirect_impact_score_mean' in merged_data.columns:
        correlation_columns.append('indirect_impact_score_mean')
        column_mapping['indirect_impact_score_mean'] = 'Indirect Impact'

    if 'certainty_mean' in merged_data.columns:
        correlation_columns.append('certainty_mean')
        column_mapping['certainty_mean'] = 'Certainty'

    if 'weighted_score' in merged_data.columns:
        correlation_columns.append('weighted_score')
        column_mapping['weighted_score'] = 'Weighted Score'

    # è‚¡ä»·ç›¸å…³åˆ—
    price_columns = ['Close', 'price_change', 'price_change_next']
    for col in price_columns:
        if col in merged_data.columns:
            correlation_columns.append(col)
            if col == 'Close':
                column_mapping[col] = 'Close Price'
            elif col == 'price_change':
                column_mapping[col] = 'Same Day Î”%'
            elif col == 'price_change_next':
                column_mapping[col] = 'Next Day Î”%'

    if len(correlation_columns) > 2:
        correlation_data = merged_data[correlation_columns].corr()
        correlation_data.rename(columns=column_mapping, index=column_mapping, inplace=True)

        mask = np.triu(np.ones_like(correlation_data, dtype=bool))
        sns.heatmap(correlation_data, mask=mask, annot=True, cmap='RdBu_r', center=0,
                    square=True, ax=ax_heatmap, cbar_kws={'shrink': 0.8},
                    fmt='.3f', linewidths=0.5)
        ax_heatmap.set_title('Enhanced Correlation Matrix', fontsize=14, fontweight='bold')
    else:
        ax_heatmap.text(0.5, 0.5, 'Insufficient Data for Correlation Matrix',
                       ha='center', va='center', transform=ax_heatmap.transAxes)

    plt.tight_layout()
    plt.show()

def calculate_correlation_statistics(merged_data):
    """
    è®¡ç®—å¢å¼ºçš„ç›¸å…³æ€§ç»Ÿè®¡

    Args:
        merged_data (pd.DataFrame): åˆå¹¶åçš„æ•°æ®

    Returns:
        dict: ç›¸å…³æ€§åˆ†æç»“æœ
    """
    print("\n" + "=" * 50)
    print("æ­¥éª¤ 7: å¢å¼ºçš„ç›¸å…³æ€§ç»Ÿè®¡åˆ†æ")
    print("=" * 50)

    results = {}

    # ç¡®å®šä¸»è¦è¯„åˆ†åˆ—
    score_col = 'overall_score_mean' if 'overall_score_mean' in merged_data.columns else 'score_mean'

    # ç§»é™¤ç¼ºå¤±å€¼
    analysis_data = merged_data.dropna(subset=[score_col, 'price_change', 'price_change_next'])

    if len(analysis_data) >= 3:  # è‡³å°‘éœ€è¦3ä¸ªæ•°æ®ç‚¹è¿›è¡Œç›¸å…³æ€§åˆ†æ

        print(f"1. ç»¼åˆè¯„åˆ†ç›¸å…³æ€§åˆ†æ:")
        try:
            # 1. ç»¼åˆè¯„åˆ†ç›¸å…³æ€§
            corr_current, p_value_current = pearsonr(analysis_data[score_col], analysis_data['price_change'])
            corr_next, p_value_next = pearsonr(analysis_data[score_col], analysis_data['price_change_next'])

            results['overall_score'] = {
                'current_day': {'correlation': corr_current, 'p_value': p_value_current},
                'next_day': {'correlation': corr_next, 'p_value': p_value_next}
            }

            print(f"   ç»¼åˆè¯„åˆ† vs å½“æ—¥è‚¡ä»·å˜åŒ–: r = {corr_current:.4f}, p-value = {p_value_current:.4f}")
            print(f"   ç»¼åˆè¯„åˆ† vs æ¬¡æ—¥è‚¡ä»·å˜åŒ–: r = {corr_next:.4f}, p-value = {p_value_next:.4f}")

        except Exception as e:
            print(f"   ç»¼åˆè¯„åˆ†ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")

        # 2. ç›´æ¥å½±å“ç›¸å…³æ€§
        print(f"\n2. ç›´æ¥å½±å“ç›¸å…³æ€§åˆ†æ:")
        if 'direct_impact_score_mean' in merged_data.columns:
            try:
                direct_data = analysis_data.dropna(subset=['direct_impact_score_mean'])
                if len(direct_data) >= 3:
                    corr_direct_current, p_direct_current = pearsonr(direct_data['direct_impact_score_mean'], direct_data['price_change'])
                    corr_direct_next, p_direct_next = pearsonr(direct_data['direct_impact_score_mean'], direct_data['price_change_next'])

                    results['direct_impact'] = {
                        'current_day': {'correlation': corr_direct_current, 'p_value': p_direct_current},
                        'next_day': {'correlation': corr_direct_next, 'p_value': p_direct_next}
                    }

                    print(f"   ç›´æ¥å½±å“ vs å½“æ—¥è‚¡ä»·å˜åŒ–: r = {corr_direct_current:.4f}, p-value = {p_direct_current:.4f}")
                    print(f"   ç›´æ¥å½±å“ vs æ¬¡æ—¥è‚¡ä»·å˜åŒ–: r = {corr_direct_next:.4f}, p-value = {p_direct_next:.4f}")
                else:
                    print("   ç›´æ¥å½±å“æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§")
            except Exception as e:
                print(f"   ç›´æ¥å½±å“ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
        else:
            print("   æ— ç›´æ¥å½±å“æ•°æ®")

        # 3. é—´æ¥å½±å“ç›¸å…³æ€§
        print(f"\n3. é—´æ¥å½±å“ç›¸å…³æ€§åˆ†æ:")
        if 'indirect_impact_score_mean' in merged_data.columns:
            try:
                indirect_data = analysis_data.dropna(subset=['indirect_impact_score_mean'])
                if len(indirect_data) >= 3:
                    corr_indirect_current, p_indirect_current = pearsonr(indirect_data['indirect_impact_score_mean'], indirect_data['price_change'])
                    corr_indirect_next, p_indirect_next = pearsonr(indirect_data['indirect_impact_score_mean'], indirect_data['price_change_next'])

                    results['indirect_impact'] = {
                        'current_day': {'correlation': corr_indirect_current, 'p_value': p_indirect_current},
                        'next_day': {'correlation': corr_indirect_next, 'p_value': p_indirect_next}
                    }

                    print(f"   é—´æ¥å½±å“ vs å½“æ—¥è‚¡ä»·å˜åŒ–: r = {corr_indirect_current:.4f}, p-value = {p_indirect_current:.4f}")
                    print(f"   é—´æ¥å½±å“ vs æ¬¡æ—¥è‚¡ä»·å˜åŒ–: r = {corr_indirect_next:.4f}, p-value = {p_indirect_next:.4f}")
                else:
                    print("   é—´æ¥å½±å“æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§")
            except Exception as e:
                print(f"   é—´æ¥å½±å“ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
        else:
            print("   æ— é—´æ¥å½±å“æ•°æ®")

        # 4. ç¡®å®šæ€§ç›¸å…³æ€§
        print(f"\n4. ç¡®å®šæ€§ç›¸å…³æ€§åˆ†æ:")
        if 'certainty_mean' in merged_data.columns:
            try:
                certainty_data = analysis_data.dropna(subset=['certainty_mean'])
                if len(certainty_data) >= 3:
                    corr_certainty_current, p_certainty_current = pearsonr(certainty_data['certainty_mean'], certainty_data['price_change'])
                    corr_certainty_next, p_certainty_next = pearsonr(certainty_data['certainty_mean'], certainty_data['price_change_next'])

                    results['certainty'] = {
                        'current_day': {'correlation': corr_certainty_current, 'p_value': p_certainty_current},
                        'next_day': {'correlation': corr_certainty_next, 'p_value': p_certainty_next}
                    }

                    print(f"   ç¡®å®šæ€§ vs å½“æ—¥è‚¡ä»·å˜åŒ–: r = {corr_certainty_current:.4f}, p-value = {p_certainty_current:.4f}")
                    print(f"   ç¡®å®šæ€§ vs æ¬¡æ—¥è‚¡ä»·å˜åŒ–: r = {corr_certainty_next:.4f}, p-value = {p_certainty_next:.4f}")
                else:
                    print("   ç¡®å®šæ€§æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§")
            except Exception as e:
                print(f"   ç¡®å®šæ€§ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
        else:
            print("   æ— ç¡®å®šæ€§æ•°æ®")

        # 5. åŠ æƒè¯„åˆ†ç›¸å…³æ€§
        print(f"\n5. ç¡®å®šæ€§åŠ æƒè¯„åˆ†ç›¸å…³æ€§åˆ†æ:")
        if 'weighted_score' in merged_data.columns:
            try:
                weighted_data = analysis_data.dropna(subset=['weighted_score'])
                if len(weighted_data) >= 3:
                    corr_weighted_current, p_weighted_current = pearsonr(weighted_data['weighted_score'], weighted_data['price_change'])
                    corr_weighted_next, p_weighted_next = pearsonr(weighted_data['weighted_score'], weighted_data['price_change_next'])

                    results['weighted_score'] = {
                        'current_day': {'correlation': corr_weighted_current, 'p_value': p_weighted_current},
                        'next_day': {'correlation': corr_weighted_next, 'p_value': p_weighted_next}
                    }

                    print(f"   åŠ æƒè¯„åˆ† vs å½“æ—¥è‚¡ä»·å˜åŒ–: r = {corr_weighted_current:.4f}, p-value = {p_weighted_current:.4f}")
                    print(f"   åŠ æƒè¯„åˆ† vs æ¬¡æ—¥è‚¡ä»·å˜åŒ–: r = {corr_weighted_next:.4f}, p-value = {p_weighted_next:.4f}")
                else:
                    print("   åŠ æƒè¯„åˆ†æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§")
            except Exception as e:
                print(f"   åŠ æƒè¯„åˆ†ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
        else:
            print("   æ— åŠ æƒè¯„åˆ†æ•°æ®")

        # 6. ç›¸å…³æ€§å¼ºåº¦è§£é‡Šå’Œæ¯”è¾ƒ
        def interpret_correlation(r):
            abs_r = abs(r)
            if abs_r < 0.1:
                return "å‡ ä¹æ— ç›¸å…³"
            elif abs_r < 0.3:
                return "å¼±ç›¸å…³"
            elif abs_r < 0.5:
                return "ä¸­ç­‰ç›¸å…³"
            elif abs_r < 0.7:
                return "å¼ºç›¸å…³"
            else:
                return "å¾ˆå¼ºç›¸å…³"

        print(f"\n6. ç›¸å…³æ€§å¼ºåº¦æ¯”è¾ƒ:")
        correlations = []

        for category, data in results.items():
            if isinstance(data, dict) and 'current_day' in data:
                current_r = data['current_day']['correlation']
                next_r = data['next_day']['correlation']
                correlations.append((category, 'current_day', current_r))
                correlations.append((category, 'next_day', next_r))

        # æŒ‰ç»å¯¹å€¼æ’åº
        correlations.sort(key=lambda x: abs(x[2]), reverse=True)

        for i, (category, timing, corr) in enumerate(correlations[:5]):  # æ˜¾ç¤ºå‰5ä¸ªæœ€å¼ºç›¸å…³æ€§
            direction = 'æ­£' if corr > 0 else 'è´Ÿ'
            strength = interpret_correlation(corr)
            print(f"   {i+1}. {category}_{timing}: r={corr:.4f} ({strength}, {direction}ç›¸å…³)")

        # 7. æŠ•èµ„å»ºè®®
        print(f"\n7. åŸºäºå¢å¼ºåˆ†æçš„æŠ•èµ„å»ºè®®:")

        best_correlation = max(correlations, key=lambda x: abs(x[2])) if correlations else None

        if best_correlation and abs(best_correlation[2]) > 0.3:
            category, timing, corr = best_correlation
            print(f"   â€¢ æœ€å¼ºé¢„æµ‹å› å­: {category} ({'å½“æ—¥' if timing == 'current_day' else 'æ¬¡æ—¥'})")
            if timing == 'next_day' and abs(corr) > 0.3:
                direction = 'æ­£é¢' if corr > 0 else 'è´Ÿé¢'
                print(f"   â€¢ {direction}æ–°é—»è¯„åˆ†å¯èƒ½é¢„ç¤ºæ¬¡æ—¥è‚¡ä»·{'ä¸Šæ¶¨' if corr > 0 else 'ä¸‹è·Œ'}")
            else:
                print("   â€¢ ä¸»è¦å½±å“ä½“ç°åœ¨å½“æ—¥è‚¡ä»·ååº”")
        else:
            print("   â€¢ æ–°é—»è¯„åˆ†ä¸è‚¡ä»·å˜åŒ–çš„å…³ç³»ç›¸å¯¹è¾ƒå¼±")
            print("   â€¢ å»ºè®®ç»“åˆå…¶ä»–æŠ€æœ¯æŒ‡æ ‡è¿›è¡Œç»¼åˆåˆ†æ")

        if 'weighted_score' in results and 'overall_score' in results:
            weighted_corr = abs(results['weighted_score']['next_day']['correlation'])
            overall_corr = abs(results['overall_score']['next_day']['correlation'])
            if weighted_corr > overall_corr * 1.1:
                print("   â€¢ ç¡®å®šæ€§åŠ æƒè¯„åˆ†é¢„æµ‹æ•ˆæœæ›´å¥½ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨é«˜ç¡®å®šæ€§æ–°é—»")

        results['sample_size'] = len(analysis_data)

    else:
        print(f"âš ï¸  æ•°æ®æ ·æœ¬è¿‡å°‘ (ä»…{len(analysis_data)}ä¸ªæ ·æœ¬)ï¼Œæ— æ³•è¿›è¡Œå¯é çš„ç›¸å…³æ€§åˆ†æ")
        print("å»ºè®®æ”¶é›†æ›´å¤šæ—¶é—´æ®µçš„æ•°æ®ä»¥è·å¾—æ›´å‡†ç¡®çš„åˆ†æç»“æœ")
        results['error'] = "æ ·æœ¬é‡ä¸è¶³"

    return results

def save_results(merged_data, output_path):
    """
    ä¿å­˜åˆ†æç»“æœ

    Args:
        merged_data (pd.DataFrame): åˆ†æç»“æœæ•°æ®
        output_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    print("\n" + "=" * 50)
    print("æ­¥éª¤ 8: ä¿å­˜åˆ†æç»“æœ")
    print("=" * 50)

    try:
        merged_data.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"âœ“ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")

        # æ˜¾ç¤ºæœ€ç»ˆæ•°æ®æ‘˜è¦
        print(f"\n=== æœ€ç»ˆæ•°æ®æ‘˜è¦ ===")
        print(f"æ€»å…±åˆ†æäº† {len(merged_data)} ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®")
        print(f"æ—¶é—´èŒƒå›´: {merged_data['date'].min().date()} åˆ° {merged_data['date'].max().date()}")
        # åŠ¨æ€ç¡®å®šè¯„åˆ†åˆ—å
        score_col = 'overall_score_mean' if 'overall_score_mean' in merged_data.columns else 'score_mean'
        if score_col in merged_data.columns:
            print(f"å¹³å‡æ–°é—»è¯„åˆ†: {merged_data[score_col].mean():.2f}")
        else:
            print(f"æ— æ³•æ‰¾åˆ°è¯„åˆ†åˆ—ï¼Œå¯ç”¨åˆ—: {list(merged_data.columns)}")
        print(f"å¹³å‡è‚¡ä»·: {merged_data['Close'].mean():.2f} å…ƒ")

        if 'price_change' in merged_data.columns:
            valid_changes = merged_data['price_change'].dropna()
            if len(valid_changes) > 0:
                print(f"å¹³å‡æ—¥æ”¶ç›Šç‡: {valid_changes.mean():.2f}%")
                print(f"æ”¶ç›Šç‡æ³¢åŠ¨ç‡: {valid_changes.std():.2f}%")

    except Exception as e:
        print(f"âœ— ä¿å­˜å¤±è´¥: {e}")

def main():
    """
    ä¸»å‡½æ•° - æ‰§è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
    """
    print("ğŸ” æ–°é—»è¯„åˆ†ä¸è‚¡ä»·ç›¸å…³æ€§åˆ†æ")
    print("=" * 60)
    print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # æ–‡ä»¶è·¯å¾„é…ç½®
    news_data_path = r'D:\projects\q\myQ\scripts\news_scores_result.csv'
    output_path = r'D:\projects\q\myQ\scripts\news_price_analysis_result.csv'

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(news_data_path):
        print(f"âœ— é”™è¯¯: æ‰¾ä¸åˆ°æ–°é—»æ•°æ®æ–‡ä»¶ {news_data_path}")
        return

    try:
        # æ­¥éª¤1: è¯»å–æ–°é—»æ•°æ®
        news_df = load_news_data(news_data_path)
        if news_df is None:
            return

        # æ­¥éª¤2: èšåˆæ—¥åº¦è¯„åˆ†
        daily_scores = aggregate_daily_scores(news_df)

        # æ­¥éª¤3: è·å–è‚¡ä»·æ•°æ®
        stock_data = get_stock_price_data(daily_scores)

        # æ­¥éª¤4: åˆå¹¶æ•°æ®
        merged_data = merge_data(daily_scores, stock_data)

        if len(merged_data) == 0:
            print("âœ— é”™è¯¯: åˆå¹¶åæ²¡æœ‰æ•°æ®ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            return

        # æ­¥éª¤5: æ—¶é—´åºåˆ—å¯è§†åŒ–
        plot_time_series(merged_data)

        # æ­¥éª¤6: ç›¸å…³æ€§å¯è§†åŒ–
        plot_correlation_analysis(merged_data)

        # æ­¥éª¤7: ç»Ÿè®¡åˆ†æ
        correlation_results = calculate_correlation_statistics(merged_data)

        # æ­¥éª¤8: ä¿å­˜ç»“æœ
        save_results(merged_data, output_path)

        print("\n" + "=" * 60)
        print("ğŸ‰ åˆ†æå®Œæˆï¼")
        print("=" * 60)
        print("ä¸»è¦è¾“å‡º:")
        print("1. æ—¶é—´åºåˆ—å›¾è¡¨")
        print("2. ç›¸å…³æ€§åˆ†æå›¾è¡¨")
        print("3. ç»Ÿè®¡åˆ†æç»“æœ")
        print(f"4. æ•°æ®æ–‡ä»¶: {output_path}")

    except Exception as e:
        print(f"âœ— åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        print(traceback.format_exc())

if __name__ == "__main__":
    main()