{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9b0d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "openrouter_key='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'\n",
    "response = requests.post(\n",
    "  url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "  headers={\n",
    "    \"Authorization\": f\"Bearer {openrouter_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "  },\n",
    "  data=json.dumps({\n",
    "    \"model\": \"deepseek/deepseek-chat-v3.1:free\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"解释一下多因子策略在投资组合中的作用?用中文回答\"\n",
    "      }\n",
    "    ],\n",
    "    \n",
    "  })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20438999",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'choices'"
     ]
    }
   ],
   "source": [
    "response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "051846a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\": {\n",
      "    \"message\": \"User not found.\",\n",
      "    \"code\": 401\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "openrouter_key='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'\n",
    "response = requests.get(\n",
    "  url=\"https://openrouter.ai/api/v1/key\",\n",
    "  headers={\n",
    "    \"Authorization\": f\"Bearer {openrouter_key}\"\n",
    "  }\n",
    ")\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e7ffd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df=pd.read_parquet(r'D:\\projects\\q\\data\\minute_data\\000002\\2025-09.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb3734a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-03 13:53:00</td>\n",
       "      <td>6.60</td>\n",
       "      <td>6.60</td>\n",
       "      <td>6.59</td>\n",
       "      <td>6.60</td>\n",
       "      <td>224300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-03 13:54:00</td>\n",
       "      <td>6.60</td>\n",
       "      <td>6.60</td>\n",
       "      <td>6.59</td>\n",
       "      <td>6.60</td>\n",
       "      <td>695600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-03 13:55:00</td>\n",
       "      <td>6.60</td>\n",
       "      <td>6.61</td>\n",
       "      <td>6.60</td>\n",
       "      <td>6.61</td>\n",
       "      <td>339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-03 13:56:00</td>\n",
       "      <td>6.61</td>\n",
       "      <td>6.61</td>\n",
       "      <td>6.60</td>\n",
       "      <td>6.61</td>\n",
       "      <td>406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-03 13:57:00</td>\n",
       "      <td>6.61</td>\n",
       "      <td>6.61</td>\n",
       "      <td>6.59</td>\n",
       "      <td>6.60</td>\n",
       "      <td>446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>2025-09-15 14:54:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>649200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>2025-09-15 14:55:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1186100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>2025-09-15 14:56:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2255700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>2025-09-15 14:57:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2261800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>2025-09-15 15:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2333599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1970 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date  open  high   low  close   volume\n",
       "0    2025-09-03 13:53:00  6.60  6.60  6.59   6.60   224300\n",
       "1    2025-09-03 13:54:00  6.60  6.60  6.59   6.60   695600\n",
       "2    2025-09-03 13:55:00  6.60  6.61  6.60   6.61   339600\n",
       "3    2025-09-03 13:56:00  6.61  6.61  6.60   6.61   406800\n",
       "4    2025-09-03 13:57:00  6.61  6.61  6.59   6.60   446900\n",
       "...                  ...   ...   ...   ...    ...      ...\n",
       "1965 2025-09-15 14:54:00   NaN   NaN   NaN    NaN   649200\n",
       "1966 2025-09-15 14:55:00   NaN   NaN   NaN    NaN  1186100\n",
       "1967 2025-09-15 14:56:00   NaN   NaN   NaN    NaN  2255700\n",
       "1968 2025-09-15 14:57:00   NaN   NaN   NaN    NaN  2261800\n",
       "1969 2025-09-15 15:00:00   NaN   NaN   NaN    NaN  2333599\n",
       "\n",
       "[1970 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a15a6a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索关键词: 紫金矿业\n",
      "获取页数: 2\n",
      "第1页获取到 20 条新闻\n",
      "新闻字段: ['id', 'code', 'title', 'postId', 'nickname', 'content', 'date', 'articleType', 'portrait', 'listImage', 'url', 'authorUrl', 'uid', 'likeNum', 'commentNum']\n",
      "date: 2023-12-28 08:30:00\n",
      "示例标题: 后浪森林·商业史 | <em>紫金矿业</em>三十二年，人人都是大富翁（四）...\n",
      "\n",
      "正在处理第1条新闻:\n",
      "  标题: 后浪森林·商业史 | <em>紫金矿业</em>三十二年，人人都是大富翁（四）...\n",
      "  日期: 2023-12-28 08:30:00\n",
      "  来源: 成长企业常识\n",
      "  URL: http://caifuhao.eastmoney.com/news/20231228004008763123340\n",
      "  ✗ 内容获取失败\n",
      "\n",
      "正在处理第2条新闻:\n",
      "  标题: <em>紫金矿业</em>，爆了！...\n",
      "  日期: 2025-02-13 16:28:05\n",
      "  来源: 看财经\n",
      "  URL: http://caifuhao.eastmoney.com/news/20250213162805586513200\n",
      "  ✗ 内容获取失败\n",
      "第2页获取到 20 条新闻\n",
      "新闻字段: ['id', 'code', 'title', 'postId', 'nickname', 'content', 'date', 'articleType', 'portrait', 'listImage', 'url', 'authorUrl', 'uid', 'likeNum', 'commentNum']\n",
      "date: 2024-11-14 19:39:05\n",
      "示例标题: <em>紫金矿业</em>遭遇“黄金大劫案”？公司回应...\n",
      "\n",
      "正在处理第1条新闻:\n",
      "  标题: <em>紫金矿业</em>遭遇“黄金大劫案”？公司回应...\n",
      "  日期: 2024-11-14 19:39:05\n",
      "  来源: 财经读数\n",
      "  URL: http://caifuhao.eastmoney.com/news/20241114193905492120760\n",
      "  ✗ 内容获取失败\n",
      "\n",
      "正在处理第2条新闻:\n",
      "  标题: 刘肥肥：<em>紫金矿业</em>4月17日午评...\n",
      "  日期: 2023-04-17 15:54:02\n",
      "  来源: 刘肥肥\n",
      "  URL: http://caifuhao.eastmoney.com/news/20230417155402734025900\n",
      "  ✗ 内容获取失败\n",
      "未获取到任何新闻内容\n",
      "未获取到新闻数据\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class EastmoneyNewsCrawler:\n",
    "    def __init__(self):\n",
    "        self.api_url = \"https://search-api-web.eastmoney.com/search/jsonp\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Referer': 'https://so.eastmoney.com/',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "        }\n",
    "    \n",
    "    def get_news_content(self, keyword, start_date=None, end_date=None, max_pages=2):\n",
    "        \"\"\"\n",
    "        获取新闻内容 - 简化版，先测试基本功能\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"搜索关键词: {keyword}\")\n",
    "        print(f\"获取页数: {max_pages}\")\n",
    "        \n",
    "        all_news = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            # 获取新闻列表\n",
    "            news_list = self._get_news_list(keyword, page)\n",
    "            if not news_list:\n",
    "                break\n",
    "                \n",
    "            # 获取每页前2条新闻的内容进行测试\n",
    "            for i, news in enumerate(news_list[:2]):\n",
    "                news_date = self._parse_date(news.get('raw_date', ''))\n",
    "                \n",
    "                print(f\"\\n正在处理第{i+1}条新闻:\")\n",
    "                print(f\"  标题: {news['title'][:60]}...\")\n",
    "                print(f\"  日期: {news_date}\")\n",
    "                print(f\"  来源: {news['source']}\")\n",
    "                print(f\"  URL: {news['url']}\")\n",
    "                \n",
    "                content = self._fetch_news_content(news['url'])\n",
    "                if content:\n",
    "                    all_news.append({\n",
    "                        'title': news['title'].replace('<em>', '').replace('</em>', ''),  # 清理高亮标签\n",
    "                        'date': news_date,\n",
    "                        'url': news['url'],\n",
    "                        'source': news['source'],\n",
    "                        'content': content\n",
    "                    })\n",
    "                    print(f\"  ✓ 内容获取成功 ({len(content)} 字符)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ 内容获取失败\")\n",
    "                    \n",
    "                time.sleep(1)\n",
    "            \n",
    "            time.sleep(2)\n",
    "        \n",
    "        # 转换为DataFrame并按时间排序\n",
    "        if all_news:\n",
    "            df = pd.DataFrame(all_news)\n",
    "            try:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df = df.sort_values('date', ascending=False).reset_index(drop=True)\n",
    "            except:\n",
    "                pass  # 如果日期解析失败，不排序\n",
    "            \n",
    "            print(f\"\\n总共获取 {len(df)} 条新闻内容\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"未获取到任何新闻内容\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _get_news_list(self, keyword, page_index):\n",
    "        \"\"\"获取新闻列表\"\"\"\n",
    "        param_data = {\n",
    "            \"uid\": \"\",\n",
    "            \"keyword\": keyword,\n",
    "            \"type\": [\"article\"],\n",
    "            \"client\": \"web\",\n",
    "            \"clientType\": \"web\",\n",
    "            \"clientVersion\": \"curr\",\n",
    "            \"param\": {\n",
    "                \"article\": {\n",
    "                    \"searchScope\": \"\",\n",
    "                    \"sort\": \"time\",  # 修改为按时间排序\n",
    "                    \"pageIndex\": page_index,\n",
    "                    \"pageSize\": 20,\n",
    "                    \"preTag\": \"\",\n",
    "                    \"postTag\": \"\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        params = {\n",
    "            'cb': f'jQuery{int(time.time() * 1000)}_{int(time.time() * 1000) + 1}',\n",
    "            'param': json.dumps(param_data)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.api_url, headers=self.headers, params=params, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                response_text = response.text\n",
    "                \n",
    "                if response_text.startswith('jQuery'):\n",
    "                    start = response_text.find('(') + 1\n",
    "                    end = response_text.rfind(')')\n",
    "                    json_str = response_text[start:end]\n",
    "                else:\n",
    "                    json_str = response_text\n",
    "                \n",
    "                data = json.loads(json_str)\n",
    "                \n",
    "                if 'result' in data and data['result'] and 'article' in data['result']:\n",
    "                    articles = data['result']['article']\n",
    "                    print(f\"第{page_index}页获取到 {len(articles)} 条新闻\")\n",
    "                    \n",
    "                    # 调试第一条新闻的字段\n",
    "                    if articles:\n",
    "                        first_article = articles[0]\n",
    "                        print(f\"新闻字段: {list(first_article.keys())}\")\n",
    "                        print(f\"date: {first_article.get('date')}\")\n",
    "                        print(f\"示例标题: {first_article.get('title')[:50]}...\")\n",
    "                    \n",
    "                    news_list = []\n",
    "                    for article in articles:\n",
    "                        # 使用正确的日期字段\n",
    "                        article_date = article.get('date', '')\n",
    "                        formatted_date = self._parse_date(article_date)\n",
    "                        \n",
    "                        news_list.append({\n",
    "                            'title': article.get('title', '').strip(),\n",
    "                            'url': article.get('url', ''),\n",
    "                            'date': formatted_date,\n",
    "                            'source': article.get('nickname', ''),  # 使用nickname作为来源\n",
    "                            'raw_date': article_date  # 保留原始日期用于调试\n",
    "                        })\n",
    "                    \n",
    "                    return news_list\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"获取第{page_index}页失败: {e}\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _fetch_news_content(self, url):\n",
    "        \"\"\"获取新闻正文内容\"\"\"\n",
    "        if not url:\n",
    "            return \"\"\n",
    "            \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # 尝试多种可能的正文选择器\n",
    "                content_selectors = [\n",
    "                    '.ArticleBody',\n",
    "                    '.content',\n",
    "                    '.article-content',\n",
    "                    '#ContentBody',\n",
    "                    '.main-content',\n",
    "                    'article',\n",
    "                    '.post-content'\n",
    "                ]\n",
    "                \n",
    "                for selector in content_selectors:\n",
    "                    content_div = soup.select_one(selector)\n",
    "                    if content_div:\n",
    "                        text = content_div.get_text().strip()\n",
    "                        text = re.sub(r'\\s+', ' ', text)\n",
    "                        if len(text) > 100:\n",
    "                            return text\n",
    "                \n",
    "                # 如果没找到，尝试获取所有p标签内容\n",
    "                paragraphs = soup.find_all('p')\n",
    "                if paragraphs:\n",
    "                    content = ' '.join([p.get_text().strip() for p in paragraphs])\n",
    "                    content = re.sub(r'\\s+', ' ', content)\n",
    "                    if len(content) > 100:\n",
    "                        return content\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"获取内容失败 {url}: {e}\")\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def _parse_date(self, date_str):\n",
    "        \"\"\"解析日期字符串\"\"\"\n",
    "        if not date_str:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            # 处理ISO格式\n",
    "            if 'T' in str(date_str):\n",
    "                dt = datetime.fromisoformat(str(date_str).replace('Z', ''))\n",
    "                return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # 处理其他常见格式\n",
    "            date_str = str(date_str).strip()\n",
    "            \n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', date_str):\n",
    "                return date_str\n",
    "            \n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}', date_str):\n",
    "                return date_str + ':00'\n",
    "            \n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_str):\n",
    "                return date_str + ' 00:00:00'\n",
    "                \n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        return date_str\n",
    "    \n",
    "    def _is_in_date_range(self, news_date, start_date, end_date):\n",
    "        \"\"\"检查日期是否在指定范围内\"\"\"\n",
    "        try:\n",
    "            if not news_date:\n",
    "                return True\n",
    "                \n",
    "            news_dt = datetime.strptime(news_date[:10], '%Y-%m-%d')\n",
    "            start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "            end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "            \n",
    "            return start_dt <= news_dt <= end_dt\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = EastmoneyNewsCrawler()\n",
    "    \n",
    "    # 获取紫金矿业最近一周的新闻内容\n",
    "    df = crawler.get_news_content(\"紫金矿业\", max_pages=2)\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(f\"\\n获取到 {len(df)} 条新闻\")\n",
    "        print(\"\\n最新3条新闻:\")\n",
    "        for i, row in df.head(3).iterrows():\n",
    "            print(f\"\\n{i+1}. {row['title']}\")\n",
    "            print(f\"时间: {row['date']}\")\n",
    "            print(f\"来源: {row['source']}\")\n",
    "            print(f\"内容: {row['content'][:200]}...\")\n",
    "        \n",
    "        # 保存到CSV\n",
    "        df.to_csv('news_content.csv', index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n数据已保存到 news_content.csv\")\n",
    "    else:\n",
    "        print(\"未获取到新闻数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5dfd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索关键词: 紫金矿业\n",
      "获取页数: 2\n",
      "第1页获取到 20 条新闻\n",
      "新闻字段: ['id', 'code', 'title', 'postId', 'nickname', 'content', 'date', 'articleType', 'portrait', 'listImage', 'url', 'authorUrl', 'uid', 'likeNum', 'commentNum']\n",
      "date: 2023-12-28 08:30:00\n",
      "示例标题: 后浪森林·商业史 | <em>紫金矿业</em>三十二年，人人都是大富翁（四）...\n",
      "\n",
      "正在处理第1条新闻:\n",
      "  标题: 后浪森林·商业史 | <em>紫金矿业</em>三十二年，人人都是大富翁（四）...\n",
      "  日期: 2023-12-28 08:30:00\n",
      "  来源: 成长企业常识\n",
      "  URL: http://caifuhao.eastmoney.com/news/20231228004008763123340\n",
      "  ✓ 内容获取成功 (4089 字符)\n",
      "\n",
      "正在处理第2条新闻:\n",
      "  标题: <em>紫金矿业</em>，爆了！...\n",
      "  日期: 2025-02-13 16:28:05\n",
      "  来源: 看财经\n",
      "  URL: http://caifuhao.eastmoney.com/news/20250213162805586513200\n",
      "  ✓ 内容获取成功 (1527 字符)\n",
      "第2页获取到 20 条新闻\n",
      "新闻字段: ['id', 'code', 'title', 'postId', 'nickname', 'content', 'date', 'articleType', 'portrait', 'listImage', 'url', 'authorUrl', 'uid', 'likeNum', 'commentNum']\n",
      "date: 2024-11-14 19:39:05\n",
      "示例标题: <em>紫金矿业</em>遭遇“黄金大劫案”？公司回应...\n",
      "\n",
      "正在处理第1条新闻:\n",
      "  标题: <em>紫金矿业</em>遭遇“黄金大劫案”？公司回应...\n",
      "  日期: 2024-11-14 19:39:05\n",
      "  来源: 财经读数\n",
      "  URL: http://caifuhao.eastmoney.com/news/20241114193905492120760\n",
      "  ✓ 内容获取成功 (2962 字符)\n",
      "\n",
      "正在处理第2条新闻:\n",
      "  标题: 刘肥肥：<em>紫金矿业</em>4月17日午评...\n",
      "  日期: 2023-04-17 15:54:02\n",
      "  来源: 刘肥肥\n",
      "  URL: http://caifuhao.eastmoney.com/news/20230417155402734025900\n",
      "  ✓ 内容获取成功 (356 字符)\n",
      "\n",
      "总共获取 4 条新闻内容\n",
      "\n",
      "获取到 4 条新闻\n",
      "\n",
      "最新3条新闻:\n",
      "\n",
      "1. 紫金矿业，爆了！\n",
      "时间: 2025-02-13 16:28:05\n",
      "来源: 看财经\n",
      "内容: 过去六年时间，$紫金矿业(SH601899)$紫金矿业（601899.SH）大涨近10倍；尽管最近股价有所回调，但市值依然高达4500亿元。 30年前的小作坊、启动资金仅1万的紫金矿业，如今已经成为参天大树，真正的全球矿业巨头。 这背后，归功于出色的盈利能力。 根据业绩预告显示，紫金矿业预计2024年净利润约320亿元，同比增长51.5%，创历史新高，并且连续9年增长。 紫金矿业的盈利能力究竟有多...\n",
      "\n",
      "2. 紫金矿业遭遇“黄金大劫案”？公司回应\n",
      "时间: 2024-11-14 19:39:05\n",
      "来源: 财经读数\n",
      "内容: 中国企业投资的海外矿山存在被盗采的情况，整体来看不多见，但在一些高风险国家，类似事件比较多 文｜张建锋 康国亮 编辑｜杨秀红 11月14日，有外媒报道称,紫金矿业（601899.SH）的哥伦比亚金矿遭到贩毒集团掠夺了价值2亿美元(约合人民币15亿元)的3.2吨黄金。 对此，紫金矿业董秘办回应，盗采确实存在，在2020年紫金矿业进入该矿山前就已经存在，公司也不清楚非法开采的实际数量。“不知道网上流传...\n",
      "\n",
      "3. 后浪森林·商业史 | 紫金矿业三十二年，人人都是大富翁（四）\n",
      "时间: 2023-12-28 08:30:00\n",
      "来源: 成长企业常识\n",
      "内容: 后浪森林研究室|三文 编辑|罗周 统筹、编辑助理|许佳维 紫金矿业改变了太多人的命运。 从陈景河、陈发树、柯希平，到同康村民、紫金职工，再到基金经理邓晓峰等等。最新数据是紫金矿业持股机构1195家。 紫金矿业的财富游戏，人人都是大富翁。 原始股…… 1996年，紫金矿山投产，当年利润虽达到1000万，对于矿山大规模的资金投入却杯水车薪，于是上杭县派工作组进驻紫金矿业进行改制。 改制方案分三步：第一...\n",
      "\n",
      "数据已保存到 news_content.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6b2c54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>紫金矿业遭遇“黄金大劫案”？公司回应</td>\n",
       "      <td>2024-11-14 19:39:05</td>\n",
       "      <td>http://caifuhao.eastmoney.com/news/20241114193...</td>\n",
       "      <td>财经读数</td>\n",
       "      <td>中国企业投资的海外矿山存在被盗采的情况，整体来看不多见，但在一些高风险国家，类似事件比较多 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>热衷买矿的紫金矿业，又大幅融资补血了</td>\n",
       "      <td>2024-06-19 23:41:01</td>\n",
       "      <td>http://caifuhao.eastmoney.com/news/20240619234...</td>\n",
       "      <td>雷达财经</td>\n",
       "      <td>雷达财经鸿途出品 文|李亦辉 编|深海 有着“矿茅”之称的紫金矿业，正在大举融资。 6月18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>后浪森林·商业史 | 紫金矿业三十二年，人人都是大富翁（四）</td>\n",
       "      <td>2023-12-28 08:30:00</td>\n",
       "      <td>http://caifuhao.eastmoney.com/news/20231228004...</td>\n",
       "      <td>成长企业常识</td>\n",
       "      <td>后浪森林研究室|三文 编辑|罗周 统筹、编辑助理|许佳维 紫金矿业改变了太多人的命运。 从陈...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>刘肥肥：紫金矿业4月17日午评</td>\n",
       "      <td>2023-04-17 15:54:02</td>\n",
       "      <td>http://caifuhao.eastmoney.com/news/20230417155...</td>\n",
       "      <td>刘肥肥</td>\n",
       "      <td>1，紫金矿业 601899 今日收盘价13.91元人民币， 港股紫金矿业 02899下午三点...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title                date  \\\n",
       "0              紫金矿业遭遇“黄金大劫案”？公司回应 2024-11-14 19:39:05   \n",
       "1              热衷买矿的紫金矿业，又大幅融资补血了 2024-06-19 23:41:01   \n",
       "2  后浪森林·商业史 | 紫金矿业三十二年，人人都是大富翁（四） 2023-12-28 08:30:00   \n",
       "3                 刘肥肥：紫金矿业4月17日午评 2023-04-17 15:54:02   \n",
       "\n",
       "                                                 url  source  \\\n",
       "0  http://caifuhao.eastmoney.com/news/20241114193...    财经读数   \n",
       "1  http://caifuhao.eastmoney.com/news/20240619234...    雷达财经   \n",
       "2  http://caifuhao.eastmoney.com/news/20231228004...  成长企业常识   \n",
       "3  http://caifuhao.eastmoney.com/news/20230417155...     刘肥肥   \n",
       "\n",
       "                                             content  \n",
       "0  中国企业投资的海外矿山存在被盗采的情况，整体来看不多见，但在一些高风险国家，类似事件比较多 ...  \n",
       "1  雷达财经鸿途出品 文|李亦辉 编|深海 有着“矿茅”之称的紫金矿业，正在大举融资。 6月18...  \n",
       "2  后浪森林研究室|三文 编辑|罗周 统筹、编辑助理|许佳维 紫金矿业改变了太多人的命运。 从陈...  \n",
       "3  1，紫金矿业 601899 今日收盘价13.91元人民币， 港股紫金矿业 02899下午三点...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "880d9392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索关键词: 紫金矿业\n",
      "获取页数: 2\n",
      "第1页获取到 20 条新闻\n",
      "新闻字段: ['id', 'code', 'title', 'postId', 'nickname', 'content', 'date', 'articleType', 'portrait', 'listImage', 'url', 'authorUrl', 'uid', 'likeNum', 'commentNum']\n",
      "date: 2023-12-28 08:30:00\n",
      "示例标题: 后浪森林·商业史 | <em>紫金矿业</em>三十二年，人人都是大富翁（四）...\n",
      "\n",
      "正在处理第1条新闻:\n",
      "  标题: 后浪森林·商业史 | <em>紫金矿业</em>三十二年，人人都是大富翁（四）...\n",
      "  日期: 2023-12-28 08:30:00\n",
      "  来源: 成长企业常识\n",
      "  URL: http://caifuhao.eastmoney.com/news/20231228004008763123340\n",
      "  ✓ 内容获取成功 (4089 字符)\n",
      "\n",
      "正在处理第2条新闻:\n",
      "  标题: 热衷买矿的<em>紫金矿业</em>，又大幅融资补血了...\n",
      "  日期: 2024-06-19 23:41:01\n",
      "  来源: 雷达财经\n",
      "  URL: http://caifuhao.eastmoney.com/news/20240619234101211306570\n",
      "  ✓ 内容获取成功 (3841 字符)\n",
      "第2页获取到 20 条新闻\n",
      "新闻字段: ['id', 'code', 'title', 'postId', 'nickname', 'content', 'date', 'articleType', 'portrait', 'listImage', 'url', 'authorUrl', 'uid', 'likeNum', 'commentNum']\n",
      "date: 2024-11-14 19:39:05\n",
      "示例标题: <em>紫金矿业</em>遭遇“黄金大劫案”？公司回应...\n",
      "\n",
      "正在处理第1条新闻:\n",
      "  标题: <em>紫金矿业</em>遭遇“黄金大劫案”？公司回应...\n",
      "  日期: 2024-11-14 19:39:05\n",
      "  来源: 财经读数\n",
      "  URL: http://caifuhao.eastmoney.com/news/20241114193905492120760\n",
      "  ✓ 内容获取成功 (2962 字符)\n",
      "\n",
      "正在处理第2条新闻:\n",
      "  标题: 刘肥肥：<em>紫金矿业</em>4月17日午评...\n",
      "  日期: 2023-04-17 15:54:02\n",
      "  来源: 刘肥肥\n",
      "  URL: http://caifuhao.eastmoney.com/news/20230417155402734025900\n",
      "  ✓ 内容获取成功 (356 字符)\n",
      "\n",
      "总共获取 4 条新闻内容\n",
      "\n",
      "获取到 4 条新闻\n",
      "\n",
      "最新3条新闻:\n",
      "\n",
      "1. 紫金矿业遭遇“黄金大劫案”？公司回应\n",
      "时间: 2024-11-14 19:39:05\n",
      "来源: 财经读数\n",
      "内容: 中国企业投资的海外矿山存在被盗采的情况，整体来看不多见，但在一些高风险国家，类似事件比较多 文｜张建锋 康国亮 编辑｜杨秀红 11月14日，有外媒报道称,紫金矿业（601899.SH）的哥伦比亚金矿遭到贩毒集团掠夺了价值2亿美元(约合人民币15亿元)的3.2吨黄金。 对此，紫金矿业董秘办回应，盗采确实存在，在2020年紫金矿业进入该矿山前就已经存在，公司也不清楚非法开采的实际数量。“不知道网上流传...\n",
      "\n",
      "2. 热衷买矿的紫金矿业，又大幅融资补血了\n",
      "时间: 2024-06-19 23:41:01\n",
      "来源: 雷达财经\n",
      "内容: 雷达财经鸿途出品 文|李亦辉 编|深海 有着“矿茅”之称的紫金矿业，正在大举融资。 6月18日早间，紫金矿业发布公告，面向海外投资者完成港股市场再融资25亿美元，其中可转债20亿美元，配售5亿美元。按当日汇率计算，此笔融资金额高达181.4亿元人民币。根据Wind统计，这将是紫金矿业史上规模最大的一笔融资。 今年以来，受益于金铜价格的上涨，紫金矿业A、H股价均创下历史新高，其中A股市值目前已突破4...\n",
      "\n",
      "3. 后浪森林·商业史 | 紫金矿业三十二年，人人都是大富翁（四）\n",
      "时间: 2023-12-28 08:30:00\n",
      "来源: 成长企业常识\n",
      "内容: 后浪森林研究室|三文 编辑|罗周 统筹、编辑助理|许佳维 紫金矿业改变了太多人的命运。 从陈景河、陈发树、柯希平，到同康村民、紫金职工，再到基金经理邓晓峰等等。最新数据是紫金矿业持股机构1195家。 紫金矿业的财富游戏，人人都是大富翁。 原始股…… 1996年，紫金矿山投产，当年利润虽达到1000万，对于矿山大规模的资金投入却杯水车薪，于是上杭县派工作组进驻紫金矿业进行改制。 改制方案分三步：第一...\n",
      "\n",
      "数据已保存到 news_content.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class EastmoneyNewsCrawler:\n",
    "    def __init__(self):\n",
    "        self.api_url = \"https://search-api-web.eastmoney.com/search/jsonp\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Referer': 'https://so.eastmoney.com/',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "        }\n",
    "    \n",
    "    def get_news_content(self, keyword, start_date=None, end_date=None, max_pages=2):\n",
    "        \"\"\"\n",
    "        获取新闻内容 - 简化版，先测试基本功能\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"搜索关键词: {keyword}\")\n",
    "        print(f\"获取页数: {max_pages}\")\n",
    "        \n",
    "        all_news = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            # 获取新闻列表\n",
    "            news_list = self._get_news_list(keyword, page)\n",
    "            if not news_list:\n",
    "                break\n",
    "                \n",
    "            # 获取每页前2条新闻的内容进行测试\n",
    "            for i, news in enumerate(news_list[:2]):\n",
    "                news_date = self._parse_date(news.get('raw_date', ''))\n",
    "                \n",
    "                print(f\"\\n正在处理第{i+1}条新闻:\")\n",
    "                print(f\"  标题: {news['title'][:60]}...\")\n",
    "                print(f\"  日期: {news_date}\")\n",
    "                print(f\"  来源: {news['source']}\")\n",
    "                print(f\"  URL: {news['url']}\")\n",
    "                \n",
    "                content = self._fetch_news_content(news['url'])\n",
    "                if content:\n",
    "                    all_news.append({\n",
    "                        'title': news['title'].replace('<em>', '').replace('</em>', ''),  # 清理高亮标签\n",
    "                        'date': news_date,\n",
    "                        'url': news['url'],\n",
    "                        'source': news['source'],\n",
    "                        'content': content\n",
    "                    })\n",
    "                    print(f\"  ✓ 内容获取成功 ({len(content)} 字符)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ 内容获取失败\")\n",
    "                    \n",
    "                time.sleep(1)\n",
    "            \n",
    "            time.sleep(2)\n",
    "        \n",
    "        # 转换为DataFrame并按时间排序\n",
    "        if all_news:\n",
    "            df = pd.DataFrame(all_news)\n",
    "            try:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df = df.sort_values('date', ascending=False).reset_index(drop=True)\n",
    "            except:\n",
    "                pass  # 如果日期解析失败，不排序\n",
    "            \n",
    "            print(f\"\\n总共获取 {len(df)} 条新闻内容\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"未获取到任何新闻内容\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def test_sorting(self, keyword):\n",
    "        \"\"\"测试不同排序参数的效果\"\"\"\n",
    "        \n",
    "        sort_values = [\"default\", \"relate\", \"time\", \"\", \"1\", \"2\"]\n",
    "        \n",
    "        for sort_val in sort_values:\n",
    "            print(f\"\\n=== 测试排序参数: '{sort_val}' ===\")\n",
    "            \n",
    "            param_data = {\n",
    "                \"uid\": \"\",\n",
    "                \"keyword\": keyword,\n",
    "                \"type\": [\"article\"],\n",
    "                \"client\": \"web\",\n",
    "                \"clientType\": \"web\",\n",
    "                \"clientVersion\": \"curr\",\n",
    "                \"param\": {\n",
    "                    \"article\": {\n",
    "                        \"searchScope\": \"\",\n",
    "                        \"sort\": sort_val,\n",
    "                        \"pageIndex\": 1,\n",
    "                        \"pageSize\": 5,\n",
    "                        \"preTag\": \"\",\n",
    "                        \"postTag\": \"\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            params = {\n",
    "                'cb': f'jQuery{int(time.time() * 1000)}_{int(time.time() * 1000) + 1}',\n",
    "                'param': json.dumps(param_data)\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(self.api_url, headers=self.headers, params=params, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    response_text = response.text\n",
    "                    \n",
    "                    if response_text.startswith('jQuery'):\n",
    "                        start = response_text.find('(') + 1\n",
    "                        end = response_text.rfind(')')\n",
    "                        json_str = response_text[start:end]\n",
    "                    else:\n",
    "                        json_str = response_text\n",
    "                    \n",
    "                    data = json.loads(json_str)\n",
    "                    \n",
    "                    if 'result' in data and data['result'] and 'article' in data['result']:\n",
    "                        articles = data['result']['article']\n",
    "                        print(f\"获取到 {len(articles)} 条新闻\")\n",
    "                        \n",
    "                        # 显示前5条新闻的日期，看排序效果\n",
    "                        dates = []\n",
    "                        for i, article in enumerate(articles):\n",
    "                            date = article.get('date', '')\n",
    "                            title = article.get('title', '').replace('<em>', '').replace('</em>', '')[:40]\n",
    "                            print(f\"  {i+1}. {date} - {title}...\")\n",
    "                            dates.append(date)\n",
    "                        \n",
    "                        # 检查是否按时间排序（最新在前）\n",
    "                        if len(dates) > 1:\n",
    "                            is_desc_sorted = all(dates[i] >= dates[i+1] for i in range(len(dates)-1) if dates[i] and dates[i+1])\n",
    "                            print(f\"是否按时间倒序排列: {is_desc_sorted}\")\n",
    "                    else:\n",
    "                        print(\"没有获取到新闻\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"测试失败: {e}\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \"\"\"测试不同的排序参数\"\"\"\n",
    "        \n",
    "        sort_options = [\n",
    "            \"\",              # 空字符串（默认）\n",
    "            \"default\",       # 默认\n",
    "            \"time\",          # 时间\n",
    "            \"relate\",        # 相关度\n",
    "            \"score\",         # 分数\n",
    "            \"1\",             # 数字值\n",
    "            \"2\", \n",
    "            \"3\",\n",
    "            \"createTime\",    # 创建时间\n",
    "            \"publishTime\",   # 发布时间\n",
    "            \"desc\",          # 降序\n",
    "            \"asc\",           # 升序\n",
    "        ]\n",
    "        \n",
    "        for sort_param in sort_options:\n",
    "            print(f\"\\n=== 测试排序参数: '{sort_param}' ===\")\n",
    "            \n",
    "            param_data = {\n",
    "                \"uid\": \"\",\n",
    "                \"keyword\": keyword,\n",
    "                \"type\": [\"article\"],\n",
    "                \"client\": \"web\",\n",
    "                \"clientType\": \"web\",\n",
    "                \"clientVersion\": \"curr\",\n",
    "                \"param\": {\n",
    "                    \"article\": {\n",
    "                        \"searchScope\": \"\",\n",
    "                        \"sort\": sort_param,\n",
    "                        \"pageIndex\": 1,\n",
    "                        \"pageSize\": 5,\n",
    "                        \"preTag\": \"\",\n",
    "                        \"postTag\": \"\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            params = {\n",
    "                'cb': f'jQuery{int(time.time() * 1000)}_{int(time.time() * 1000) + 1}',\n",
    "                'param': json.dumps(param_data)\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(self.api_url, headers=self.headers, params=params, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    response_text = response.text\n",
    "                    \n",
    "                    if response_text.startswith('jQuery'):\n",
    "                        start = response_text.find('(') + 1\n",
    "                        end = response_text.rfind(')')\n",
    "                        json_str = response_text[start:end]\n",
    "                    else:\n",
    "                        json_str = response_text\n",
    "                    \n",
    "                    data = json.loads(json_str)\n",
    "                    \n",
    "                    if 'result' in data and data['result'] and 'article' in data['result']:\n",
    "                        articles = data['result']['article']\n",
    "                        print(f\"获取到 {len(articles)} 条新闻\")\n",
    "                        \n",
    "                        # 显示前3条新闻的日期\n",
    "                        for i, article in enumerate(articles[:3]):\n",
    "                            date = article.get('date', '')\n",
    "                            title = article.get('title', '')[:50]\n",
    "                            print(f\"  {i+1}. {date} - {title}...\")\n",
    "                    else:\n",
    "                        print(\"没有获取到新闻\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"测试失败: {e}\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \"\"\"\n",
    "        获取新闻内容 - 简化版，先测试基本功能\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"搜索关键词: {keyword}\")\n",
    "        print(f\"获取页数: {max_pages}\")\n",
    "        \n",
    "        all_news = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            # 获取新闻列表\n",
    "            news_list = self._get_news_list(keyword, page)\n",
    "            if not news_list:\n",
    "                break\n",
    "                \n",
    "            # 获取每页前2条新闻的内容进行测试\n",
    "            for i, news in enumerate(news_list[:2]):\n",
    "                news_date = self._parse_date(news.get('raw_date', ''))\n",
    "                \n",
    "                print(f\"\\n正在处理第{i+1}条新闻:\")\n",
    "                print(f\"  标题: {news['title'][:60]}...\")\n",
    "                print(f\"  日期: {news_date}\")\n",
    "                print(f\"  来源: {news['source']}\")\n",
    "                print(f\"  URL: {news['url']}\")\n",
    "                \n",
    "                content = self._fetch_news_content(news['url'])\n",
    "                if content:\n",
    "                    all_news.append({\n",
    "                        'title': news['title'].replace('<em>', '').replace('</em>', ''),  # 清理高亮标签\n",
    "                        'date': news_date,\n",
    "                        'url': news['url'],\n",
    "                        'source': news['source'],\n",
    "                        'content': content\n",
    "                    })\n",
    "                    print(f\"  ✓ 内容获取成功 ({len(content)} 字符)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ 内容获取失败\")\n",
    "                    \n",
    "                time.sleep(1)\n",
    "            \n",
    "            time.sleep(2)\n",
    "        \n",
    "        # 转换为DataFrame并按时间排序\n",
    "        if all_news:\n",
    "            df = pd.DataFrame(all_news)\n",
    "            try:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df = df.sort_values('date', ascending=False).reset_index(drop=True)\n",
    "            except:\n",
    "                pass  # 如果日期解析失败，不排序\n",
    "            \n",
    "            print(f\"\\n总共获取 {len(df)} 条新闻内容\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"未获取到任何新闻内容\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _get_news_list(self, keyword, page_index):\n",
    "        \"\"\"获取新闻列表\"\"\"\n",
    "        param_data = {\n",
    "            \"uid\": \"\",\n",
    "            \"keyword\": keyword,\n",
    "            \"type\": [\"article\"],\n",
    "            \"client\": \"web\",\n",
    "            \"clientType\": \"web\",\n",
    "            \"clientVersion\": \"curr\",\n",
    "            \"param\": {\n",
    "                \"article\": {\n",
    "                    \"searchScope\": \"\",\n",
    "                    \"sort\": \"time\",  # 使用正确的时间排序参数\n",
    "                    \"pageIndex\": page_index,\n",
    "                    \"pageSize\": 20,\n",
    "                    \"preTag\": \"\",\n",
    "                    \"postTag\": \"\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        params = {\n",
    "            'cb': f'jQuery{int(time.time() * 1000)}_{int(time.time() * 1000) + 1}',\n",
    "            'param': json.dumps(param_data)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.api_url, headers=self.headers, params=params, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                response_text = response.text\n",
    "                \n",
    "                if response_text.startswith('jQuery'):\n",
    "                    start = response_text.find('(') + 1\n",
    "                    end = response_text.rfind(')')\n",
    "                    json_str = response_text[start:end]\n",
    "                else:\n",
    "                    json_str = response_text\n",
    "                \n",
    "                data = json.loads(json_str)\n",
    "                \n",
    "                if 'result' in data and data['result'] and 'article' in data['result']:\n",
    "                    articles = data['result']['article']\n",
    "                    print(f\"第{page_index}页获取到 {len(articles)} 条新闻\")\n",
    "                    \n",
    "                    # 调试第一条新闻的字段\n",
    "                    if articles:\n",
    "                        first_article = articles[0]\n",
    "                        print(f\"新闻字段: {list(first_article.keys())}\")\n",
    "                        print(f\"date: {first_article.get('date')}\")\n",
    "                        print(f\"示例标题: {first_article.get('title')[:50]}...\")\n",
    "                    \n",
    "                    news_list = []\n",
    "                    for article in articles:\n",
    "                        # 使用正确的日期字段\n",
    "                        article_date = article.get('date', '')\n",
    "                        formatted_date = self._parse_date(article_date)\n",
    "                        \n",
    "                        news_list.append({\n",
    "                            'title': article.get('title', '').strip(),\n",
    "                            'url': article.get('url', ''),\n",
    "                            'date': formatted_date,\n",
    "                            'source': article.get('nickname', ''),  # 使用nickname作为来源\n",
    "                            'raw_date': article_date  # 保留原始日期用于调试\n",
    "                        })\n",
    "                    \n",
    "                    return news_list\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"获取第{page_index}页失败: {e}\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _fetch_news_content(self, url):\n",
    "        \"\"\"获取新闻正文内容\"\"\"\n",
    "        if not url:\n",
    "            return \"\"\n",
    "            \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # 尝试多种可能的正文选择器\n",
    "                content_selectors = [\n",
    "                    '.ArticleBody',\n",
    "                    '.content',\n",
    "                    '.article-content',\n",
    "                    '#ContentBody',\n",
    "                    '.main-content',\n",
    "                    'article',\n",
    "                    '.post-content'\n",
    "                ]\n",
    "                \n",
    "                for selector in content_selectors:\n",
    "                    content_div = soup.select_one(selector)\n",
    "                    if content_div:\n",
    "                        text = content_div.get_text().strip()\n",
    "                        text = re.sub(r'\\s+', ' ', text)\n",
    "                        if len(text) > 100:\n",
    "                            return text\n",
    "                \n",
    "                # 如果没找到，尝试获取所有p标签内容\n",
    "                paragraphs = soup.find_all('p')\n",
    "                if paragraphs:\n",
    "                    content = ' '.join([p.get_text().strip() for p in paragraphs])\n",
    "                    content = re.sub(r'\\s+', ' ', content)\n",
    "                    if len(content) > 100:\n",
    "                        return content\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"获取内容失败 {url}: {e}\")\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def _parse_date(self, date_str):\n",
    "        \"\"\"解析日期字符串\"\"\"\n",
    "        if not date_str:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            # 处理ISO格式\n",
    "            if 'T' in str(date_str):\n",
    "                dt = datetime.fromisoformat(str(date_str).replace('Z', ''))\n",
    "                return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # 处理其他常见格式\n",
    "            date_str = str(date_str).strip()\n",
    "            \n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', date_str):\n",
    "                return date_str\n",
    "            \n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}', date_str):\n",
    "                return date_str + ':00'\n",
    "            \n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_str):\n",
    "                return date_str + ' 00:00:00'\n",
    "                \n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        return date_str\n",
    "    \n",
    "    def _is_in_date_range(self, news_date, start_date, end_date):\n",
    "        \"\"\"检查日期是否在指定范围内\"\"\"\n",
    "        try:\n",
    "            if not news_date:\n",
    "                return True\n",
    "                \n",
    "            news_dt = datetime.strptime(news_date[:10], '%Y-%m-%d')\n",
    "            start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "            end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "            \n",
    "            return start_dt <= news_dt <= end_dt\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = EastmoneyNewsCrawler()\n",
    "    \n",
    "    # 获取紫金矿业最近一周的新闻内容\n",
    "    df = crawler.get_news_content(\"紫金矿业\", max_pages=2)\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(f\"\\n获取到 {len(df)} 条新闻\")\n",
    "        print(\"\\n最新3条新闻:\")\n",
    "        for i, row in df.head(3).iterrows():\n",
    "            print(f\"\\n{i+1}. {row['title']}\")\n",
    "            print(f\"时间: {row['date']}\")\n",
    "            print(f\"来源: {row['source']}\")\n",
    "            print(f\"内容: {row['content'][:200]}...\")\n",
    "        \n",
    "        # 保存到CSV\n",
    "        df.to_csv('news_content.csv', index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n数据已保存到 news_content.csv\")\n",
    "    else:\n",
    "        print(\"未获取到新闻数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee10f1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索关键词: 紫金矿业\n",
      "最大获取 10 条新闻\n",
      "第1页获取到 20 条新闻\n",
      "新闻字段: ['id', 'code', 'title', 'postId', 'nickname', 'content', 'date', 'articleType', 'portrait', 'listImage', 'url', 'authorUrl', 'uid', 'likeNum', 'commentNum']\n",
      "date: 2023-12-28 08:30:00\n",
      "示例标题: 后浪森林·商业史 | <em>紫金矿业</em>三十二年，人人都是大富翁（四）...\n",
      "\n",
      "正在处理: 后浪森林·商业史 | <em>紫金矿业</em>三十二年，人人都是大富翁（四）...\n",
      "  日期: 2023-12-28 08:30:00\n",
      "  ✓ 成功获取内容 (4089 字符)\n",
      "\n",
      "正在处理: <em>紫金矿业</em>海外扩张频频触雷...\n",
      "  日期: 2023-12-15 09:05:39\n",
      "  ✓ 成功获取内容 (1227 字符)\n",
      "\n",
      "正在处理: 财大气粗的<em>紫金矿业</em>差钱了...\n",
      "  日期: 2024-06-21 18:33:52\n",
      "  ✓ 成功获取内容 (1078 字符)\n",
      "\n",
      "正在处理: <em>紫金矿业</em>，爆了！...\n",
      "  日期: 2025-02-13 16:28:05\n",
      "  ✓ 成功获取内容 (1527 字符)\n",
      "\n",
      "正在处理: <em>紫金矿业</em>：拟收购藏格矿业股份 可能涉及控制权变更...\n",
      "  日期: 2025-01-11 09:00:00\n",
      "  ✓ 成功获取内容 (504 字符)\n",
      "\n",
      "正在处理: <em>紫金矿业</em>，海外金矿被非法开采...\n",
      "  日期: 2024-11-21 17:03:02\n",
      "  ✓ 成功获取内容 (2167 字符)\n",
      "\n",
      "正在处理: 后浪森林·商业史 | <em>紫金矿业</em>并购史，及其反周期逻辑（三）...\n",
      "  日期: 2023-12-27 08:30:00\n",
      "  ✓ 成功获取内容 (2565 字符)\n",
      "\n",
      "正在处理: <em>紫金矿业</em>，彻底爆了！...\n",
      "  日期: 2025-02-06 17:11:03\n",
      "  ✓ 成功获取内容 (1800 字符)\n",
      "\n",
      "正在处理: 热衷买矿的<em>紫金矿业</em>，又大幅融资补血了...\n",
      "  日期: 2024-06-19 23:41:01\n",
      "  ✓ 成功获取内容 (3841 字符)\n",
      "\n",
      "正在处理: 让“中国第一矿企”日进8亿的男人...\n",
      "  日期: 2023-10-20 22:01:37\n",
      "  ✓ 成功获取内容 (1363 字符)\n",
      "时间排序失败: DataFrame.sort_values() got an unexpected keyword argument 'na_last'\n",
      "\n",
      "获取到 10 条新闻\n",
      "\n",
      "最新3条新闻:\n",
      "\n",
      "1. 后浪森林·商业史 | 紫金矿业三十二年，人人都是大富翁（四）\n",
      "时间: 2023-12-28 08:30:00\n",
      "来源: 成长企业常识\n",
      "内容: 后浪森林研究室|三文 编辑|罗周 统筹、编辑助理|许佳维 紫金矿业改变了太多人的命运。 从陈景河、陈发树、柯希平，到同康村民、紫金职工，再到基金经理邓晓峰等等。最新数据是紫金矿业持股机构1195家。 紫金矿业的财富游戏，人人都是大富翁。 原始股…… 1996年，紫金矿山投产，当年利润虽达到1000万，对于矿山大规模的资金投入却杯水车薪，于是上杭县派工作组进驻紫金矿业进行改制。 改制方案分三步：第一...\n",
      "\n",
      "2. 紫金矿业海外扩张频频触雷\n",
      "时间: 2023-12-15 09:05:39\n",
      "来源: 经济道理\n",
      "内容:  问道者 杜一用 阿根廷选特朗普的忠实粉丝米莱当总统，大洋这一头的中国矿业巨头紫金矿业可能因此受伤。米莱竞选期间可是不只一次扬言要断绝与中国的政治经济合作，而在去年初，紫金矿业刚斥资10余亿美元在阿根廷收购了一座盐湖锂矿建了一座碳酸锂工厂。米莱要真兑现了竞选承诺，紫金矿业会损失惨重。 近年来，紫金矿业把锂作为新的增长点，建立起了“两湖一矿”格局，阿根廷的盐湖锂矿就是其中最重要的海外锂矿资产。米莱的...\n",
      "\n",
      "3. 财大气粗的紫金矿业差钱了\n",
      "时间: 2024-06-21 18:33:52\n",
      "来源: 经济道理\n",
      "内容:  问道者 李 百 过去几年一直买买买的紫金矿业开始差钱了，最近通过发行可转债和配售新H股，准备向海外投资者融资25亿美元。这是紫金矿业上市以来最大规模的一次融资，如果加上5月份刚刚发行的20亿元公司债券，紫金矿业今年来的融资额折合人民币已经超过200亿元。紫金矿业大规模融资背后，是现金流增量四年来第一次出现了减少。 现在的紫金矿业确实财大气粗，市值超过4500亿，去年营收将近3000亿，净利润21...\n",
      "\n",
      "数据已保存到 news_content.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class EastmoneyNewsCrawler:\n",
    "    def __init__(self):\n",
    "        self.api_url = \"https://search-api-web.eastmoney.com/search/jsonp\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Referer': 'https://so.eastmoney.com/',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "        }\n",
    "    \n",
    "    def get_news_content(self, keyword, max_pages=2, max_news=10):\n",
    "        \"\"\"\n",
    "        获取新闻内容（简化版）\n",
    "        \n",
    "        Args:\n",
    "            keyword: 搜索关键词\n",
    "            max_pages: 最大页数\n",
    "            max_news: 最大新闻数量\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: 包含新闻标题、时间、内容的数据框，按时间排序\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"搜索关键词: {keyword}\")\n",
    "        print(f\"最大获取 {max_news} 条新闻\")\n",
    "        \n",
    "        all_news = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            if len(all_news) >= max_news:\n",
    "                break\n",
    "                \n",
    "            # 获取新闻列表\n",
    "            news_list = self._get_news_list(keyword, page)\n",
    "            if not news_list:\n",
    "                break\n",
    "                \n",
    "            # 处理新闻\n",
    "            for news in news_list:\n",
    "                if len(all_news) >= max_news:\n",
    "                    break\n",
    "                    \n",
    "                news_date = self._parse_date(news.get('raw_date', ''))\n",
    "                \n",
    "                print(f\"\\n正在处理: {news['title'][:50]}...\")\n",
    "                print(f\"  日期: {news_date}\")\n",
    "                \n",
    "                content = self._fetch_news_content(news['url'])\n",
    "                if content and len(content) > 100:  # 确保内容有意义\n",
    "                    all_news.append({\n",
    "                        'title': news['title'].replace('<em>', '').replace('</em>', ''),\n",
    "                        'date': news_date,\n",
    "                        'url': news['url'],\n",
    "                        'source': news['source'],\n",
    "                        'content': content\n",
    "                    })\n",
    "                    print(f\"  ✓ 成功获取内容 ({len(content)} 字符)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ 内容获取失败或内容太短\")\n",
    "                    \n",
    "                time.sleep(1)\n",
    "            \n",
    "            time.sleep(2)\n",
    "        \n",
    "        # 转换为DataFrame并手动按时间排序\n",
    "        if all_news:\n",
    "            df = pd.DataFrame(all_news)\n",
    "            \n",
    "            # 手动按时间排序（最新在前）\n",
    "            try:\n",
    "                df['date_parsed'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "                df = df.sort_values('date_parsed', ascending=False, na_last=True)\n",
    "                df = df.drop('date_parsed', axis=1).reset_index(drop=True)\n",
    "                print(f\"\\n成功获取 {len(df)} 条新闻，已按时间排序\")\n",
    "            except Exception as e:\n",
    "                print(f\"时间排序失败: {e}\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            print(\"未获取到任何新闻内容\")\n",
    "            return pd.DataFrame()\n",
    "        \"\"\"\n",
    "        获取新闻内容 - 简化版，先测试基本功能\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"搜索关键词: {keyword}\")\n",
    "        print(f\"获取页数: {max_pages}\")\n",
    "        \n",
    "        all_news = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            # 获取新闻列表\n",
    "            news_list = self._get_news_list(keyword, page)\n",
    "            if not news_list:\n",
    "                break\n",
    "                \n",
    "            # 获取每页前2条新闻的内容进行测试\n",
    "            for i, news in enumerate(news_list[:2]):\n",
    "                news_date = self._parse_date(news.get('raw_date', ''))\n",
    "                \n",
    "                print(f\"\\n正在处理第{i+1}条新闻:\")\n",
    "                print(f\"  标题: {news['title'][:60]}...\")\n",
    "                print(f\"  日期: {news_date}\")\n",
    "                print(f\"  来源: {news['source']}\")\n",
    "                print(f\"  URL: {news['url']}\")\n",
    "                \n",
    "                content = self._fetch_news_content(news['url'])\n",
    "                if content:\n",
    "                    all_news.append({\n",
    "                        'title': news['title'].replace('<em>', '').replace('</em>', ''),  # 清理高亮标签\n",
    "                        'date': news_date,\n",
    "                        'url': news['url'],\n",
    "                        'source': news['source'],\n",
    "                        'content': content\n",
    "                    })\n",
    "                    print(f\"  ✓ 内容获取成功 ({len(content)} 字符)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ 内容获取失败\")\n",
    "                    \n",
    "                time.sleep(1)\n",
    "            \n",
    "            time.sleep(2)\n",
    "        \n",
    "        # 转换为DataFrame并按时间排序\n",
    "        if all_news:\n",
    "            df = pd.DataFrame(all_news)\n",
    "            try:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df = df.sort_values('date', ascending=False).reset_index(drop=True)\n",
    "            except:\n",
    "                pass  # 如果日期解析失败，不排序\n",
    "            \n",
    "            print(f\"\\n总共获取 {len(df)} 条新闻内容\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"未获取到任何新闻内容\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def test_sorting(self, keyword):\n",
    "        \"\"\"测试不同排序参数的效果\"\"\"\n",
    "        \n",
    "        sort_values = [\"default\", \"relate\", \"time\", \"\", \"1\", \"2\"]\n",
    "        \n",
    "        for sort_val in sort_values:\n",
    "            print(f\"\\n=== 测试排序参数: '{sort_val}' ===\")\n",
    "            \n",
    "            param_data = {\n",
    "                \"uid\": \"\",\n",
    "                \"keyword\": keyword,\n",
    "                \"type\": [\"article\"],\n",
    "                \"client\": \"web\",\n",
    "                \"clientType\": \"web\",\n",
    "                \"clientVersion\": \"curr\",\n",
    "                \"param\": {\n",
    "                    \"article\": {\n",
    "                        \"searchScope\": \"\",\n",
    "                        \"sort\": sort_val,\n",
    "                        \"pageIndex\": 1,\n",
    "                        \"pageSize\": 5,\n",
    "                        \"preTag\": \"\",\n",
    "                        \"postTag\": \"\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            params = {\n",
    "                'cb': f'jQuery{int(time.time() * 1000)}_{int(time.time() * 1000) + 1}',\n",
    "                'param': json.dumps(param_data)\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(self.api_url, headers=self.headers, params=params, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    response_text = response.text\n",
    "                    \n",
    "                    if response_text.startswith('jQuery'):\n",
    "                        start = response_text.find('(') + 1\n",
    "                        end = response_text.rfind(')')\n",
    "                        json_str = response_text[start:end]\n",
    "                    else:\n",
    "                        json_str = response_text\n",
    "                    \n",
    "                    data = json.loads(json_str)\n",
    "                    \n",
    "                    if 'result' in data and data['result'] and 'article' in data['result']:\n",
    "                        articles = data['result']['article']\n",
    "                        print(f\"获取到 {len(articles)} 条新闻\")\n",
    "                        \n",
    "                        # 显示前5条新闻的日期，看排序效果\n",
    "                        dates = []\n",
    "                        for i, article in enumerate(articles):\n",
    "                            date = article.get('date', '')\n",
    "                            title = article.get('title', '').replace('<em>', '').replace('</em>', '')[:40]\n",
    "                            print(f\"  {i+1}. {date} - {title}...\")\n",
    "                            dates.append(date)\n",
    "                        \n",
    "                        # 检查是否按时间排序（最新在前）\n",
    "                        if len(dates) > 1:\n",
    "                            is_desc_sorted = all(dates[i] >= dates[i+1] for i in range(len(dates)-1) if dates[i] and dates[i+1])\n",
    "                            print(f\"是否按时间倒序排列: {is_desc_sorted}\")\n",
    "                    else:\n",
    "                        print(\"没有获取到新闻\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"测试失败: {e}\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \"\"\"测试不同的排序参数\"\"\"\n",
    "        \n",
    "        sort_options = [\n",
    "            \"\",              # 空字符串（默认）\n",
    "            \"default\",       # 默认\n",
    "            \"time\",          # 时间\n",
    "            \"relate\",        # 相关度\n",
    "            \"score\",         # 分数\n",
    "            \"1\",             # 数字值\n",
    "            \"2\", \n",
    "            \"3\",\n",
    "            \"createTime\",    # 创建时间\n",
    "            \"publishTime\",   # 发布时间\n",
    "            \"desc\",          # 降序\n",
    "            \"asc\",           # 升序\n",
    "        ]\n",
    "        \n",
    "        for sort_param in sort_options:\n",
    "            print(f\"\\n=== 测试排序参数: '{sort_param}' ===\")\n",
    "            \n",
    "            param_data = {\n",
    "                \"uid\": \"\",\n",
    "                \"keyword\": keyword,\n",
    "                \"type\": [\"article\"],\n",
    "                \"client\": \"web\",\n",
    "                \"clientType\": \"web\",\n",
    "                \"clientVersion\": \"curr\",\n",
    "                \"param\": {\n",
    "                    \"article\": {\n",
    "                        \"searchScope\": \"\",\n",
    "                        \"sort\": sort_param,\n",
    "                        \"pageIndex\": 1,\n",
    "                        \"pageSize\": 5,\n",
    "                        \"preTag\": \"\",\n",
    "                        \"postTag\": \"\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            params = {\n",
    "                'cb': f'jQuery{int(time.time() * 1000)}_{int(time.time() * 1000) + 1}',\n",
    "                'param': json.dumps(param_data)\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(self.api_url, headers=self.headers, params=params, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    response_text = response.text\n",
    "                    \n",
    "                    if response_text.startswith('jQuery'):\n",
    "                        start = response_text.find('(') + 1\n",
    "                        end = response_text.rfind(')')\n",
    "                        json_str = response_text[start:end]\n",
    "                    else:\n",
    "                        json_str = response_text\n",
    "                    \n",
    "                    data = json.loads(json_str)\n",
    "                    \n",
    "                    if 'result' in data and data['result'] and 'article' in data['result']:\n",
    "                        articles = data['result']['article']\n",
    "                        print(f\"获取到 {len(articles)} 条新闻\")\n",
    "                        \n",
    "                        # 显示前3条新闻的日期\n",
    "                        for i, article in enumerate(articles[:3]):\n",
    "                            date = article.get('date', '')\n",
    "                            title = article.get('title', '')[:50]\n",
    "                            print(f\"  {i+1}. {date} - {title}...\")\n",
    "                    else:\n",
    "                        print(\"没有获取到新闻\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"测试失败: {e}\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \"\"\"\n",
    "        获取新闻内容 - 简化版，先测试基本功能\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"搜索关键词: {keyword}\")\n",
    "        print(f\"获取页数: {max_pages}\")\n",
    "        \n",
    "        all_news = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            # 获取新闻列表\n",
    "            news_list = self._get_news_list(keyword, page)\n",
    "            if not news_list:\n",
    "                break\n",
    "                \n",
    "            # 获取每页前2条新闻的内容进行测试\n",
    "            for i, news in enumerate(news_list[:2]):\n",
    "                news_date = self._parse_date(news.get('raw_date', ''))\n",
    "                \n",
    "                print(f\"\\n正在处理第{i+1}条新闻:\")\n",
    "                print(f\"  标题: {news['title'][:60]}...\")\n",
    "                print(f\"  日期: {news_date}\")\n",
    "                print(f\"  来源: {news['source']}\")\n",
    "                print(f\"  URL: {news['url']}\")\n",
    "                \n",
    "                content = self._fetch_news_content(news['url'])\n",
    "                if content:\n",
    "                    all_news.append({\n",
    "                        'title': news['title'].replace('<em>', '').replace('</em>', ''),  # 清理高亮标签\n",
    "                        'date': news_date,\n",
    "                        'url': news['url'],\n",
    "                        'source': news['source'],\n",
    "                        'content': content\n",
    "                    })\n",
    "                    print(f\"  ✓ 内容获取成功 ({len(content)} 字符)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ 内容获取失败\")\n",
    "                    \n",
    "                time.sleep(1)\n",
    "            \n",
    "            time.sleep(2)\n",
    "        \n",
    "        # 转换为DataFrame并按时间排序\n",
    "        if all_news:\n",
    "            df = pd.DataFrame(all_news)\n",
    "            try:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df = df.sort_values('date', ascending=False).reset_index(drop=True)\n",
    "            except:\n",
    "                pass  # 如果日期解析失败，不排序\n",
    "            \n",
    "            print(f\"\\n总共获取 {len(df)} 条新闻内容\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"未获取到任何新闻内容\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _get_news_list(self, keyword, page_index):\n",
    "        \"\"\"获取新闻列表\"\"\"\n",
    "        param_data = {\n",
    "            \"uid\": \"\",\n",
    "            \"keyword\": keyword,\n",
    "            \"type\": [\"article\"],\n",
    "            \"client\": \"web\",\n",
    "            \"clientType\": \"web\",\n",
    "            \"clientVersion\": \"curr\",\n",
    "            \"param\": {\n",
    "                \"article\": {\n",
    "                    \"searchScope\": \"\",\n",
    "                    \"sort\": \"time\",  # 使用正确的时间排序参数\n",
    "                    \"pageIndex\": page_index,\n",
    "                    \"pageSize\": 20,\n",
    "                    \"preTag\": \"\",\n",
    "                    \"postTag\": \"\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        params = {\n",
    "            'cb': f'jQuery{int(time.time() * 1000)}_{int(time.time() * 1000) + 1}',\n",
    "            'param': json.dumps(param_data)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.api_url, headers=self.headers, params=params, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                response_text = response.text\n",
    "                \n",
    "                if response_text.startswith('jQuery'):\n",
    "                    start = response_text.find('(') + 1\n",
    "                    end = response_text.rfind(')')\n",
    "                    json_str = response_text[start:end]\n",
    "                else:\n",
    "                    json_str = response_text\n",
    "                \n",
    "                data = json.loads(json_str)\n",
    "                \n",
    "                if 'result' in data and data['result'] and 'article' in data['result']:\n",
    "                    articles = data['result']['article']\n",
    "                    print(f\"第{page_index}页获取到 {len(articles)} 条新闻\")\n",
    "                    \n",
    "                    # 调试第一条新闻的字段\n",
    "                    if articles:\n",
    "                        first_article = articles[0]\n",
    "                        print(f\"新闻字段: {list(first_article.keys())}\")\n",
    "                        print(f\"date: {first_article.get('date')}\")\n",
    "                        print(f\"示例标题: {first_article.get('title')[:50]}...\")\n",
    "                    \n",
    "                    news_list = []\n",
    "                    for article in articles:\n",
    "                        # 使用正确的日期字段\n",
    "                        article_date = article.get('date', '')\n",
    "                        formatted_date = self._parse_date(article_date)\n",
    "                        \n",
    "                        news_list.append({\n",
    "                            'title': article.get('title', '').strip(),\n",
    "                            'url': article.get('url', ''),\n",
    "                            'date': formatted_date,\n",
    "                            'source': article.get('nickname', ''),  # 使用nickname作为来源\n",
    "                            'raw_date': article_date  # 保留原始日期用于调试\n",
    "                        })\n",
    "                    \n",
    "                    return news_list\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"获取第{page_index}页失败: {e}\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _fetch_news_content(self, url):\n",
    "        \"\"\"获取新闻正文内容\"\"\"\n",
    "        if not url:\n",
    "            return \"\"\n",
    "            \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # 尝试多种可能的正文选择器\n",
    "                content_selectors = [\n",
    "                    '.ArticleBody',\n",
    "                    '.content',\n",
    "                    '.article-content',\n",
    "                    '#ContentBody',\n",
    "                    '.main-content',\n",
    "                    'article',\n",
    "                    '.post-content'\n",
    "                ]\n",
    "                \n",
    "                for selector in content_selectors:\n",
    "                    content_div = soup.select_one(selector)\n",
    "                    if content_div:\n",
    "                        text = content_div.get_text().strip()\n",
    "                        text = re.sub(r'\\s+', ' ', text)\n",
    "                        if len(text) > 100:\n",
    "                            return text\n",
    "                \n",
    "                # 如果没找到，尝试获取所有p标签内容\n",
    "                paragraphs = soup.find_all('p')\n",
    "                if paragraphs:\n",
    "                    content = ' '.join([p.get_text().strip() for p in paragraphs])\n",
    "                    content = re.sub(r'\\s+', ' ', content)\n",
    "                    if len(content) > 100:\n",
    "                        return content\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"获取内容失败 {url}: {e}\")\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def _parse_date(self, date_str):\n",
    "        \"\"\"解析日期字符串\"\"\"\n",
    "        if not date_str:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            # 处理ISO格式\n",
    "            if 'T' in str(date_str):\n",
    "                dt = datetime.fromisoformat(str(date_str).replace('Z', ''))\n",
    "                return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # 处理其他常见格式\n",
    "            date_str = str(date_str).strip()\n",
    "            \n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', date_str):\n",
    "                return date_str\n",
    "            \n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}', date_str):\n",
    "                return date_str + ':00'\n",
    "            \n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_str):\n",
    "                return date_str + ' 00:00:00'\n",
    "                \n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        return date_str\n",
    "    \n",
    "    def _is_in_date_range(self, news_date, start_date, end_date):\n",
    "        \"\"\"检查日期是否在指定范围内\"\"\"\n",
    "        try:\n",
    "            if not news_date:\n",
    "                return True\n",
    "                \n",
    "            news_dt = datetime.strptime(news_date[:10], '%Y-%m-%d')\n",
    "            start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "            end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "            \n",
    "            return start_dt <= news_dt <= end_dt\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = EastmoneyNewsCrawler()\n",
    "    \n",
    "    # 获取紫金矿业最近一周的新闻内容\n",
    "    df = crawler.get_news_content(\"紫金矿业\", max_pages=2)\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(f\"\\n获取到 {len(df)} 条新闻\")\n",
    "        print(\"\\n最新3条新闻:\")\n",
    "        for i, row in df.head(3).iterrows():\n",
    "            print(f\"\\n{i+1}. {row['title']}\")\n",
    "            print(f\"时间: {row['date']}\")\n",
    "            print(f\"来源: {row['source']}\")\n",
    "            print(f\"内容: {row['content'][:200]}...\")\n",
    "        \n",
    "        # 保存到CSV\n",
    "        df.to_csv('news_content.csv', index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n数据已保存到 news_content.csv\")\n",
    "    else:\n",
    "        print(\"未获取到新闻数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a425c80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索关键词: 紫金矿业\n",
      "最大获取 10 条新闻\n",
      "未获取到任何新闻内容\n",
      "未获取到新闻数据\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class EastmoneyNewsCrawler:\n",
    "    def __init__(self):\n",
    "        # 尝试old_search API\n",
    "        self.api_url = \"https://searchapi.eastmoney.com/bussiness/Web/GetSearchList\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Referer': 'https://so.eastmoney.com/',\n",
    "            'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "        }\n",
    "    \n",
    "    def test_old_search_api(self, keyword):\n",
    "        \"\"\"测试old_search API\"\"\"\n",
    "        \n",
    "        # 根据之前看到的searchReport等函数，尝试这种参数格式\n",
    "        params = {\n",
    "            'cb': f'jQuery{int(time.time() * 1000)}_{int(time.time() * 1000) + 1}',\n",
    "            'keyword': keyword,\n",
    "            'type': 1,  # 新闻类型\n",
    "            'pageindex': 1,\n",
    "            'pagesize': 10,\n",
    "            'name': 'normal'\n",
    "        }\n",
    "        \n",
    "        print(f\"测试 old_search API: {self.api_url}\")\n",
    "        print(f\"参数: {params}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.api_url, headers=self.headers, params=params, timeout=15)\n",
    "            print(f\"状态码: {response.status_code}\")\n",
    "            print(f\"响应长度: {len(response.text)}\")\n",
    "            print(f\"响应前500字符: {response.text[:500]}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                response_text = response.text\n",
    "                \n",
    "                # 处理JSONP\n",
    "                if response_text.startswith('jQuery'):\n",
    "                    start = response_text.find('(') + 1\n",
    "                    end = response_text.rfind(')')\n",
    "                    json_str = response_text[start:end]\n",
    "                else:\n",
    "                    json_str = response_text\n",
    "                \n",
    "                try:\n",
    "                    data = json.loads(json_str)\n",
    "                    print(f\"JSON解析成功\")\n",
    "                    print(f\"数据结构: {type(data)}\")\n",
    "                    if isinstance(data, dict):\n",
    "                        print(f\"顶层字段: {list(data.keys())}\")\n",
    "                        \n",
    "                        # 查找新闻数据\n",
    "                        for key, value in data.items():\n",
    "                            if isinstance(value, list) and value:\n",
    "                                print(f\"找到列表字段 '{key}': {len(value)} 个项目\")\n",
    "                                if value:\n",
    "                                    print(f\"第一个项目字段: {list(value[0].keys()) if isinstance(value[0], dict) else type(value[0])}\")\n",
    "                                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"JSON解析失败: {e}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"请求失败: {e}\")\n",
    "\n",
    "    def test_browser_params(self, keyword):\n",
    "        \"\"\"测试浏览器观察到的确切参数\"\"\"\n",
    "        \n",
    "        # 完全按照你看到的参数\n",
    "        param_data = {\n",
    "            \"uid\": \"\",\n",
    "            \"keyword\": keyword,\n",
    "            \"type\": [\"cmsArticleWebOld\"],\n",
    "            \"client\": \"web\",\n",
    "            \"clientType\": \"web\",\n",
    "            \"clientVersion\": \"curr\",\n",
    "            \"param\": {\n",
    "                \"cmsArticleWebOld\": {\n",
    "                    \"searchScope\": \"default\",\n",
    "                    \"sort\": \"time\",\n",
    "                    \"pageIndex\": 1,\n",
    "                    \"pageSize\": 10,\n",
    "                    \"preTag\": \"<em>\",\n",
    "                    \"postTag\": \"</em>\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        params = {\n",
    "            'cb': f'jQuery{int(time.time() * 1000)}_{int(time.time() * 1000) + 1}',\n",
    "            'param': json.dumps(param_data)\n",
    "        }\n",
    "        \n",
    "        print(\"测试浏览器观察到的参数:\")\n",
    "        print(f\"API URL: {self.api_url}\")\n",
    "        print(f\"参数: {json.dumps(param_data, ensure_ascii=False, indent=2)}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.api_url, headers=self.headers, params=params, timeout=15)\n",
    "            print(f\"状态码: {response.status_code}\")\n",
    "            print(f\"响应长度: {len(response.text)}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                response_text = response.text\n",
    "                print(f\"响应前300字符: {response_text[:300]}\")\n",
    "                \n",
    "                if response_text.startswith('jQuery'):\n",
    "                    start = response_text.find('(') + 1\n",
    "                    end = response_text.rfind(')')\n",
    "                    json_str = response_text[start:end]\n",
    "                else:\n",
    "                    json_str = response_text\n",
    "                \n",
    "                try:\n",
    "                    data = json.loads(json_str)\n",
    "                    print(f\"JSON解析成功\")\n",
    "                    print(f\"顶层字段: {list(data.keys())}\")\n",
    "                    \n",
    "                    if 'result' in data:\n",
    "                        result = data['result']\n",
    "                        print(f\"result字段: {list(result.keys()) if isinstance(result, dict) else type(result)}\")\n",
    "                        \n",
    "                        if 'cmsArticleWebOld' in result:\n",
    "                            articles = result['cmsArticleWebOld']\n",
    "                            print(f\"找到 {len(articles)} 条新闻\")\n",
    "                            \n",
    "                            if articles:\n",
    "                                print(\"前5条新闻:\")\n",
    "                                for i, article in enumerate(articles[:5]):\n",
    "                                    print(f\"  {i+1}. 字段: {list(article.keys())}\")\n",
    "                                    print(f\"     标题: {article.get('title', '')[:50]}...\")\n",
    "                                    print(f\"     showTime: {article.get('showTime')}\")\n",
    "                                    print(f\"     date: {article.get('date')}\")\n",
    "                                    print()\n",
    "                        else:\n",
    "                            print(\"result中没有cmsArticleWebOld字段\")\n",
    "                    else:\n",
    "                        print(\"响应中没有result字段\")\n",
    "                        \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"JSON解析失败: {e}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"请求失败: {e}\")\n",
    "\n",
    "    def get_news_content(self, keyword, max_pages=2, max_news=10):\n",
    "        \"\"\"\n",
    "        获取新闻内容（简化版）\n",
    "        \n",
    "        Args:\n",
    "            keyword: 搜索关键词\n",
    "            max_pages: 最大页数\n",
    "            max_news: 最大新闻数量\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: 包含新闻标题、时间、内容的数据框，按时间排序\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"搜索关键词: {keyword}\")\n",
    "        print(f\"最大获取 {max_news} 条新闻\")\n",
    "        \n",
    "        all_news = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            if len(all_news) >= max_news:\n",
    "                break\n",
    "                \n",
    "            # 获取新闻列表\n",
    "            news_list = self._get_news_list(keyword, page)\n",
    "            if not news_list:\n",
    "                break\n",
    "                \n",
    "            # 处理新闻\n",
    "            for news in news_list:\n",
    "                if len(all_news) >= max_news:\n",
    "                    break\n",
    "                    \n",
    "                news_date = self._parse_date(news.get('raw_date', ''))\n",
    "                \n",
    "                print(f\"\\n正在处理: {news['title'][:50]}...\")\n",
    "                print(f\"  日期: {news_date}\")\n",
    "                \n",
    "                content = self._fetch_news_content(news['url'])\n",
    "                if content and len(content) > 100:  # 确保内容有意义\n",
    "                    all_news.append({\n",
    "                        'title': news['title'].replace('<em>', '').replace('</em>', ''),\n",
    "                        'date': news_date,\n",
    "                        'url': news['url'],\n",
    "                        'source': news['source'],\n",
    "                        'content': content\n",
    "                    })\n",
    "                    print(f\"  ✓ 成功获取内容 ({len(content)} 字符)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ 内容获取失败或内容太短\")\n",
    "                    \n",
    "                time.sleep(1)\n",
    "            \n",
    "            time.sleep(2)\n",
    "        \n",
    "        # 转换为DataFrame并手动按时间排序\n",
    "        if all_news:\n",
    "            df = pd.DataFrame(all_news)\n",
    "            \n",
    "            # 手动按时间排序（最新在前）\n",
    "            try:\n",
    "                df['date_parsed'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "                df = df.sort_values('date_parsed', ascending=False, na_last=True)\n",
    "                df = df.drop('date_parsed', axis=1).reset_index(drop=True)\n",
    "                print(f\"\\n成功获取 {len(df)} 条新闻，已按时间排序\")\n",
    "            except Exception as e:\n",
    "                print(f\"时间排序失败: {e}\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            print(\"未获取到任何新闻内容\")\n",
    "            return pd.DataFrame()\n",
    "        \"\"\"\n",
    "        获取新闻内容 - 简化版，先测试基本功能\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"搜索关键词: {keyword}\")\n",
    "        print(f\"获取页数: {max_pages}\")\n",
    "        \n",
    "        all_news = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            # 获取新闻列表\n",
    "            news_list = self._get_news_list(keyword, page)\n",
    "            if not news_list:\n",
    "                break\n",
    "                \n",
    "            # 获取每页前2条新闻的内容进行测试\n",
    "            for i, news in enumerate(news_list[:2]):\n",
    "                news_date = self._parse_date(news.get('raw_date', ''))\n",
    "                \n",
    "                print(f\"\\n正在处理第{i+1}条新闻:\")\n",
    "                print(f\"  标题: {news['title'][:60]}...\")\n",
    "                print(f\"  日期: {news_date}\")\n",
    "                print(f\"  来源: {news['source']}\")\n",
    "                print(f\"  URL: {news['url']}\")\n",
    "                \n",
    "                content = self._fetch_news_content(news['url'])\n",
    "                if content:\n",
    "                    all_news.append({\n",
    "                        'title': news['title'].replace('<em>', '').replace('</em>', ''),  # 清理高亮标签\n",
    "                        'date': news_date,\n",
    "                        'url': news['url'],\n",
    "                        'source': news['source'],\n",
    "                        'content': content\n",
    "                    })\n",
    "                    print(f\"  ✓ 内容获取成功 ({len(content)} 字符)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ 内容获取失败\")\n",
    "                    \n",
    "                time.sleep(1)\n",
    "            \n",
    "            time.sleep(2)\n",
    "        \n",
    "        # 转换为DataFrame并按时间排序\n",
    "        if all_news:\n",
    "            df = pd.DataFrame(all_news)\n",
    "            try:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df = df.sort_values('date', ascending=False).reset_index(drop=True)\n",
    "            except:\n",
    "                pass  # 如果日期解析失败，不排序\n",
    "            \n",
    "            print(f\"\\n总共获取 {len(df)} 条新闻内容\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"未获取到任何新闻内容\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def test_sorting(self, keyword):\n",
    "        \"\"\"测试不同排序参数的效果\"\"\"\n",
    "        \n",
    "        sort_values = [\"default\", \"relate\", \"time\", \"\", \"1\", \"2\"]\n",
    "        \n",
    "        for sort_val in sort_values:\n",
    "            print(f\"\\n=== 测试排序参数: '{sort_val}' ===\")\n",
    "            \n",
    "            param_data = {\n",
    "                \"uid\": \"\",\n",
    "                \"keyword\": keyword,\n",
    "                \"type\": [\"article\"],\n",
    "                \"client\": \"web\",\n",
    "                \"clientType\": \"web\",\n",
    "                \"clientVersion\": \"curr\",\n",
    "                \"param\": {\n",
    "                    \"article\": {\n",
    "                        \"searchScope\": \"\",\n",
    "                        \"sort\": sort_val,\n",
    "                        \"pageIndex\": 1,\n",
    "                        \"pageSize\": 5,\n",
    "                        \"preTag\": \"\",\n",
    "                        \"postTag\": \"\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            params = {\n",
    "                'cb': f'jQuery{int(time.time() * 1000)}_{int(time.time() * 1000) + 1}',\n",
    "                'param': json.dumps(param_data)\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(self.api_url, headers=self.headers, params=params, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    response_text = response.text\n",
    "                    \n",
    "                    if response_text.startswith('jQuery'):\n",
    "                        start = response_text.find('(') + 1\n",
    "                        end = response_text.rfind(')')\n",
    "                        json_str = response_text[start:end]\n",
    "                    else:\n",
    "                        json_str = response_text\n",
    "                    \n",
    "                    data = json.loads(json_str)\n",
    "                    \n",
    "                    if 'result' in data and data['result'] and 'article' in data['result']:\n",
    "                        articles = data['result']['article']\n",
    "                        print(f\"获取到 {len(articles)} 条新闻\")\n",
    "                        \n",
    "                        # 显示前5条新闻的日期，看排序效果\n",
    "                        dates = []\n",
    "                        for i, article in enumerate(articles):\n",
    "                            date = article.get('date', '')\n",
    "                            title = article.get('title', '').replace('<em>', '').replace('</em>', '')[:40]\n",
    "                            print(f\"  {i+1}. {date} - {title}...\")\n",
    "                            dates.append(date)\n",
    "                        \n",
    "                        # 检查是否按时间排序（最新在前）\n",
    "                        if len(dates) > 1:\n",
    "                            is_desc_sorted = all(dates[i] >= dates[i+1] for i in range(len(dates)-1) if dates[i] and dates[i+1])\n",
    "                            print(f\"是否按时间倒序排列: {is_desc_sorted}\")\n",
    "                    else:\n",
    "                        print(\"没有获取到新闻\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"测试失败: {e}\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \"\"\"测试不同的排序参数\"\"\"\n",
    "        \n",
    "        sort_options = [\n",
    "            \"\",              # 空字符串（默认）\n",
    "            \"default\",       # 默认\n",
    "            \"time\",          # 时间\n",
    "            \"relate\",        # 相关度\n",
    "            \"score\",         # 分数\n",
    "            \"1\",             # 数字值\n",
    "            \"2\", \n",
    "            \"3\",\n",
    "            \"createTime\",    # 创建时间\n",
    "            \"publishTime\",   # 发布时间\n",
    "            \"desc\",          # 降序\n",
    "            \"asc\",           # 升序\n",
    "        ]\n",
    "        \n",
    "        for sort_param in sort_options:\n",
    "            print(f\"\\n=== 测试排序参数: '{sort_param}' ===\")\n",
    "            \n",
    "            param_data = {\n",
    "                \"uid\": \"\",\n",
    "                \"keyword\": keyword,\n",
    "                \"type\": [\"article\"],\n",
    "                \"client\": \"web\",\n",
    "                \"clientType\": \"web\",\n",
    "                \"clientVersion\": \"curr\",\n",
    "                \"param\": {\n",
    "                    \"article\": {\n",
    "                        \"searchScope\": \"\",\n",
    "                        \"sort\": sort_param,\n",
    "                        \"pageIndex\": 1,\n",
    "                        \"pageSize\": 5,\n",
    "                        \"preTag\": \"\",\n",
    "                        \"postTag\": \"\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            params = {\n",
    "                'cb': f'jQuery{int(time.time() * 1000)}_{int(time.time() * 1000) + 1}',\n",
    "                'param': json.dumps(param_data)\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(self.api_url, headers=self.headers, params=params, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    response_text = response.text\n",
    "                    \n",
    "                    if response_text.startswith('jQuery'):\n",
    "                        start = response_text.find('(') + 1\n",
    "                        end = response_text.rfind(')')\n",
    "                        json_str = response_text[start:end]\n",
    "                    else:\n",
    "                        json_str = response_text\n",
    "                    \n",
    "                    data = json.loads(json_str)\n",
    "                    \n",
    "                    if 'result' in data and data['result'] and 'article' in data['result']:\n",
    "                        articles = data['result']['article']\n",
    "                        print(f\"获取到 {len(articles)} 条新闻\")\n",
    "                        \n",
    "                        # 显示前3条新闻的日期\n",
    "                        for i, article in enumerate(articles[:3]):\n",
    "                            date = article.get('date', '')\n",
    "                            title = article.get('title', '')[:50]\n",
    "                            print(f\"  {i+1}. {date} - {title}...\")\n",
    "                    else:\n",
    "                        print(\"没有获取到新闻\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"测试失败: {e}\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \"\"\"\n",
    "        获取新闻内容 - 简化版，先测试基本功能\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"搜索关键词: {keyword}\")\n",
    "        print(f\"获取页数: {max_pages}\")\n",
    "        \n",
    "        all_news = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            # 获取新闻列表\n",
    "            news_list = self._get_news_list(keyword, page)\n",
    "            if not news_list:\n",
    "                break\n",
    "                \n",
    "            # 获取每页前2条新闻的内容进行测试\n",
    "            for i, news in enumerate(news_list[:2]):\n",
    "                news_date = self._parse_date(news.get('raw_date', ''))\n",
    "                \n",
    "                print(f\"\\n正在处理第{i+1}条新闻:\")\n",
    "                print(f\"  标题: {news['title'][:60]}...\")\n",
    "                print(f\"  日期: {news_date}\")\n",
    "                print(f\"  来源: {news['source']}\")\n",
    "                print(f\"  URL: {news['url']}\")\n",
    "                \n",
    "                content = self._fetch_news_content(news['url'])\n",
    "                if content:\n",
    "                    all_news.append({\n",
    "                        'title': news['title'].replace('<em>', '').replace('</em>', ''),  # 清理高亮标签\n",
    "                        'date': news_date,\n",
    "                        'url': news['url'],\n",
    "                        'source': news['source'],\n",
    "                        'content': content\n",
    "                    })\n",
    "                    print(f\"  ✓ 内容获取成功 ({len(content)} 字符)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ 内容获取失败\")\n",
    "                    \n",
    "                time.sleep(1)\n",
    "            \n",
    "            time.sleep(2)\n",
    "        \n",
    "        # 转换为DataFrame并按时间排序\n",
    "        if all_news:\n",
    "            df = pd.DataFrame(all_news)\n",
    "            try:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df = df.sort_values('date', ascending=False).reset_index(drop=True)\n",
    "            except:\n",
    "                pass  # 如果日期解析失败，不排序\n",
    "            \n",
    "            print(f\"\\n总共获取 {len(df)} 条新闻内容\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"未获取到任何新闻内容\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _get_news_list(self, keyword, page_index):\n",
    "        \"\"\"获取新闻列表\"\"\"\n",
    "        param_data = {\n",
    "            \"uid\": \"\",\n",
    "            \"keyword\": keyword,\n",
    "            \"type\": [\"article\"],\n",
    "            \"client\": \"web\",\n",
    "            \"clientType\": \"web\",\n",
    "            \"clientVersion\": \"curr\",\n",
    "            \"param\": {\n",
    "                \"article\": {\n",
    "                    \"searchScope\": \"\",\n",
    "                    \"sort\": \"time\",  # 使用正确的时间排序参数\n",
    "                    \"pageIndex\": page_index,\n",
    "                    \"pageSize\": 20,\n",
    "                    \"preTag\": \"\",\n",
    "                    \"postTag\": \"\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        params = {\n",
    "            'cb': f'jQuery{int(time.time() * 1000)}_{int(time.time() * 1000) + 1}',\n",
    "            'param': json.dumps(param_data)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.api_url, headers=self.headers, params=params, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                response_text = response.text\n",
    "                \n",
    "                if response_text.startswith('jQuery'):\n",
    "                    start = response_text.find('(') + 1\n",
    "                    end = response_text.rfind(')')\n",
    "                    json_str = response_text[start:end]\n",
    "                else:\n",
    "                    json_str = response_text\n",
    "                \n",
    "                data = json.loads(json_str)\n",
    "                \n",
    "                if 'result' in data and data['result'] and 'article' in data['result']:\n",
    "                    articles = data['result']['article']\n",
    "                    print(f\"第{page_index}页获取到 {len(articles)} 条新闻\")\n",
    "                    \n",
    "                    # 调试第一条新闻的字段\n",
    "                    if articles:\n",
    "                        first_article = articles[0]\n",
    "                        print(f\"新闻字段: {list(first_article.keys())}\")\n",
    "                        print(f\"date: {first_article.get('date')}\")\n",
    "                        print(f\"示例标题: {first_article.get('title')[:50]}...\")\n",
    "                    \n",
    "                    news_list = []\n",
    "                    for article in articles:\n",
    "                        # 使用正确的日期字段\n",
    "                        article_date = article.get('date', '')\n",
    "                        formatted_date = self._parse_date(article_date)\n",
    "                        \n",
    "                        news_list.append({\n",
    "                            'title': article.get('title', '').strip(),\n",
    "                            'url': article.get('url', ''),\n",
    "                            'date': formatted_date,\n",
    "                            'source': article.get('nickname', ''),  # 使用nickname作为来源\n",
    "                            'raw_date': article_date  # 保留原始日期用于调试\n",
    "                        })\n",
    "                    \n",
    "                    return news_list\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"获取第{page_index}页失败: {e}\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _fetch_news_content(self, url):\n",
    "        \"\"\"获取新闻正文内容\"\"\"\n",
    "        if not url:\n",
    "            return \"\"\n",
    "            \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # 尝试多种可能的正文选择器\n",
    "                content_selectors = [\n",
    "                    '.ArticleBody',\n",
    "                    '.content',\n",
    "                    '.article-content',\n",
    "                    '#ContentBody',\n",
    "                    '.main-content',\n",
    "                    'article',\n",
    "                    '.post-content'\n",
    "                ]\n",
    "                \n",
    "                for selector in content_selectors:\n",
    "                    content_div = soup.select_one(selector)\n",
    "                    if content_div:\n",
    "                        text = content_div.get_text().strip()\n",
    "                        text = re.sub(r'\\s+', ' ', text)\n",
    "                        if len(text) > 100:\n",
    "                            return text\n",
    "                \n",
    "                # 如果没找到，尝试获取所有p标签内容\n",
    "                paragraphs = soup.find_all('p')\n",
    "                if paragraphs:\n",
    "                    content = ' '.join([p.get_text().strip() for p in paragraphs])\n",
    "                    content = re.sub(r'\\s+', ' ', content)\n",
    "                    if len(content) > 100:\n",
    "                        return content\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"获取内容失败 {url}: {e}\")\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def _parse_date(self, date_str):\n",
    "        \"\"\"解析日期字符串\"\"\"\n",
    "        if not date_str:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            # 处理ISO格式\n",
    "            if 'T' in str(date_str):\n",
    "                dt = datetime.fromisoformat(str(date_str).replace('Z', ''))\n",
    "                return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # 处理其他常见格式\n",
    "            date_str = str(date_str).strip()\n",
    "            \n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', date_str):\n",
    "                return date_str\n",
    "            \n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}', date_str):\n",
    "                return date_str + ':00'\n",
    "            \n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_str):\n",
    "                return date_str + ' 00:00:00'\n",
    "                \n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        return date_str\n",
    "    \n",
    "    def _is_in_date_range(self, news_date, start_date, end_date):\n",
    "        \"\"\"检查日期是否在指定范围内\"\"\"\n",
    "        try:\n",
    "            if not news_date:\n",
    "                return True\n",
    "                \n",
    "            news_dt = datetime.strptime(news_date[:10], '%Y-%m-%d')\n",
    "            start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "            end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "            \n",
    "            return start_dt <= news_dt <= end_dt\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = EastmoneyNewsCrawler()\n",
    "    \n",
    "    # 获取紫金矿业最近一周的新闻内容\n",
    "    df = crawler.get_news_content(\"紫金矿业\", max_pages=2)\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(f\"\\n获取到 {len(df)} 条新闻\")\n",
    "        print(\"\\n最新3条新闻:\")\n",
    "        for i, row in df.head(3).iterrows():\n",
    "            print(f\"\\n{i+1}. {row['title']}\")\n",
    "            print(f\"时间: {row['date']}\")\n",
    "            print(f\"来源: {row['source']}\")\n",
    "            print(f\"内容: {row['content'][:200]}...\")\n",
    "        \n",
    "        # 保存到CSV\n",
    "        df.to_csv('news_content.csv', index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n数据已保存到 news_content.csv\")\n",
    "    else:\n",
    "        print(\"未获取到新闻数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4784378e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索关键词: 紫金矿业\n",
      "获取到 5 条新闻\n",
      "\n",
      "处理第1条新闻:\n",
      "标题: 后浪森林·商业史 | <em>紫金矿业</em>三十二年，人人都是大富翁（四）...\n",
      "日期: 2023-12-28 08:30:00\n",
      "URL: http://caifuhao.eastmoney.com/news/20231228004008763123340\n",
      "✓ 内容获取成功 (4089 字符)\n",
      "\n",
      "处理第2条新闻:\n",
      "标题: <em>紫金矿业</em>，爆了！...\n",
      "日期: 2025-02-13 16:28:05\n",
      "URL: http://caifuhao.eastmoney.com/news/20250213162805586513200\n",
      "✓ 内容获取成功 (1527 字符)\n",
      "\n",
      "处理第3条新闻:\n",
      "标题: <em>紫金矿业</em>，海外金矿被非法开采...\n",
      "日期: 2024-11-21 17:03:02\n",
      "URL: http://caifuhao.eastmoney.com/news/20241121170302175921210\n",
      "✓ 内容获取成功 (2167 字符)\n",
      "\n",
      "处理第4条新闻:\n",
      "标题: 热衷买矿的<em>紫金矿业</em>，又大幅融资补血了...\n",
      "日期: 2024-06-19 23:41:01\n",
      "URL: http://caifuhao.eastmoney.com/news/20240619234101211306570\n",
      "✓ 内容获取成功 (3841 字符)\n",
      "\n",
      "处理第5条新闻:\n",
      "标题: 财大气粗的<em>紫金矿业</em>差钱了...\n",
      "日期: 2024-06-21 18:33:52\n",
      "URL: http://caifuhao.eastmoney.com/news/20240621183352621803200\n",
      "✓ 内容获取成功 (1078 字符)\n",
      "\n",
      "成功获取 5 条完整新闻\n",
      "\n",
      "成功获取 5 条新闻\n",
      "\n",
      "1. 后浪森林·商业史 | 紫金矿业三十二年，人人都是大富翁（四）\n",
      "日期: 2023-12-28 08:30:00\n",
      "内容: 后浪森林研究室|三文 编辑|罗周 统筹、编辑助理|许佳维 紫金矿业改变了太多人的命运。 从陈景河、陈发树、柯希平，到同康村民、紫金职工，再到基金经理邓晓峰等等。最新数据是紫金矿业持股机构1195家。 紫金矿业的财富游戏，人人都是大富翁。 原始股…… 1996年，紫金矿山投产，当年利润虽达到1000万，对于矿山大规模的资金投入却杯水车薪，于是上杭县派工作组进驻紫金矿业进行改制。 改制方案分三步：第一...\n",
      "\n",
      "2. 紫金矿业，爆了！\n",
      "日期: 2025-02-13 16:28:05\n",
      "内容: 过去六年时间，$紫金矿业(SH601899)$紫金矿业（601899.SH）大涨近10倍；尽管最近股价有所回调，但市值依然高达4500亿元。 30年前的小作坊、启动资金仅1万的紫金矿业，如今已经成为参天大树，真正的全球矿业巨头。 这背后，归功于出色的盈利能力。 根据业绩预告显示，紫金矿业预计2024年净利润约320亿元，同比增长51.5%，创历史新高，并且连续9年增长。 紫金矿业的盈利能力究竟有多...\n",
      "\n",
      "3. 紫金矿业，海外金矿被非法开采\n",
      "日期: 2024-11-21 17:03:02\n",
      "内容:  作者/星空下的锅包肉 编辑/菠菜的星空 排版/星空下的香菜 近日，“$紫金矿业(SH601899)$被贩毒集团掠夺3.2吨黄金，价值约2亿美元”的消息，在网络上迅速发酵。据网传，紫金矿业控股的Continental Gold Inc.（下称“大陆黄金”）旗下的武里蒂卡金矿，被一个贩毒集团夺取了大量矿道，正在进行非法开采。 来源：紫金矿业官网 而后，紫金矿业在公司官网上对此事作出了说明。简而言之就...\n",
      "\n",
      "4. 热衷买矿的紫金矿业，又大幅融资补血了\n",
      "日期: 2024-06-19 23:41:01\n",
      "内容: 雷达财经鸿途出品 文|李亦辉 编|深海 有着“矿茅”之称的紫金矿业，正在大举融资。 6月18日早间，紫金矿业发布公告，面向海外投资者完成港股市场再融资25亿美元，其中可转债20亿美元，配售5亿美元。按当日汇率计算，此笔融资金额高达181.4亿元人民币。根据Wind统计，这将是紫金矿业史上规模最大的一笔融资。 今年以来，受益于金铜价格的上涨，紫金矿业A、H股价均创下历史新高，其中A股市值目前已突破4...\n",
      "\n",
      "5. 财大气粗的紫金矿业差钱了\n",
      "日期: 2024-06-21 18:33:52\n",
      "内容:  问道者 李 百 过去几年一直买买买的紫金矿业开始差钱了，最近通过发行可转债和配售新H股，准备向海外投资者融资25亿美元。这是紫金矿业上市以来最大规模的一次融资，如果加上5月份刚刚发行的20亿元公司债券，紫金矿业今年来的融资额折合人民币已经超过200亿元。紫金矿业大规模融资背后，是现金流增量四年来第一次出现了减少。 现在的紫金矿业确实财大气粗，市值超过4500亿，去年营收将近3000亿，净利润21...\n",
      "\n",
      "数据已保存到 news_simple.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class EastmoneyNewsCrawler:\n",
    "    def __init__(self):\n",
    "        # 回到最初能工作的API\n",
    "        self.api_url = \"https://search-api-web.eastmoney.com/search/jsonp\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Referer': 'https://so.eastmoney.com/',\n",
    "            'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "        }\n",
    "    \n",
    "    def get_news_simple(self, keyword, max_news=10):\n",
    "        \"\"\"\n",
    "        简化版：只获取新闻列表和内容，不做复杂的时间排序\n",
    "        \"\"\"\n",
    "        print(f\"搜索关键词: {keyword}\")\n",
    "        \n",
    "        # 使用最初能工作的参数\n",
    "        param_data = {\n",
    "            \"uid\": \"\",\n",
    "            \"keyword\": keyword,\n",
    "            \"type\": [\"article\"],\n",
    "            \"client\": \"web\",\n",
    "            \"clientType\": \"web\",\n",
    "            \"clientVersion\": \"curr\",\n",
    "            \"param\": {\n",
    "                \"article\": {\n",
    "                    \"searchScope\": \"\",\n",
    "                    \"sort\": \"\",\n",
    "                    \"pageIndex\": 1,\n",
    "                    \"pageSize\": max_news,\n",
    "                    \"preTag\": \"\",\n",
    "                    \"postTag\": \"\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        params = {\n",
    "            'cb': f'jQuery{int(time.time() * 1000)}_{int(time.time() * 1000) + 1}',\n",
    "            'param': json.dumps(param_data)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.api_url, headers=self.headers, params=params, timeout=15)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                response_text = response.text\n",
    "                \n",
    "                if response_text.startswith('jQuery'):\n",
    "                    start = response_text.find('(') + 1\n",
    "                    end = response_text.rfind(')')\n",
    "                    json_str = response_text[start:end]\n",
    "                else:\n",
    "                    json_str = response_text\n",
    "                \n",
    "                data = json.loads(json_str)\n",
    "                \n",
    "                if 'result' in data and data['result'] and 'article' in data['result']:\n",
    "                    articles = data['result']['article']\n",
    "                    print(f\"获取到 {len(articles)} 条新闻\")\n",
    "                    \n",
    "                    news_data = []\n",
    "                    for i, article in enumerate(articles[:max_news]):\n",
    "                        print(f\"\\n处理第{i+1}条新闻:\")\n",
    "                        print(f\"标题: {article.get('title', '')[:50]}...\")\n",
    "                        print(f\"日期: {article.get('date', '')}\")\n",
    "                        print(f\"URL: {article.get('url', '')}\")\n",
    "                        \n",
    "                        # 获取新闻内容\n",
    "                        content = self._get_content(article.get('url', ''))\n",
    "                        \n",
    "                        news_item = {\n",
    "                            'title': self._clean_title(article.get('title', '')),\n",
    "                            'date': article.get('date', ''),\n",
    "                            'url': article.get('url', ''),\n",
    "                            'source': article.get('nickname', ''),\n",
    "                            'content': content\n",
    "                        }\n",
    "                        \n",
    "                        if content:\n",
    "                            news_data.append(news_item)\n",
    "                            print(f\"✓ 内容获取成功 ({len(content)} 字符)\")\n",
    "                        else:\n",
    "                            print(\"✗ 内容获取失败\")\n",
    "                        \n",
    "                        time.sleep(1)\n",
    "                    \n",
    "                    if news_data:\n",
    "                        df = pd.DataFrame(news_data)\n",
    "                        print(f\"\\n成功获取 {len(df)} 条完整新闻\")\n",
    "                        return df\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"获取失败: {e}\")\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def _clean_title(self, title):\n",
    "        \"\"\"清理标题\"\"\"\n",
    "        return title.replace('<em>', '').replace('</em>', '').strip()\n",
    "    \n",
    "    def _get_content(self, url):\n",
    "        \"\"\"获取新闻正文\"\"\"\n",
    "        if not url:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # 尝试多种内容选择器\n",
    "                selectors = [\n",
    "                    '.ArticleBody',\n",
    "                    '.content', \n",
    "                    '.article-content',\n",
    "                    '#ContentBody',\n",
    "                    'article',\n",
    "                    '.post-content'\n",
    "                ]\n",
    "                \n",
    "                for selector in selectors:\n",
    "                    content_div = soup.select_one(selector)\n",
    "                    if content_div:\n",
    "                        text = content_div.get_text().strip()\n",
    "                        text = re.sub(r'\\s+', ' ', text)\n",
    "                        if len(text) > 100:\n",
    "                            return text\n",
    "                \n",
    "                # 备用方案：获取所有p标签\n",
    "                paragraphs = soup.find_all('p')\n",
    "                if paragraphs:\n",
    "                    content = ' '.join([p.get_text().strip() for p in paragraphs])\n",
    "                    content = re.sub(r'\\s+', ' ', content)\n",
    "                    if len(content) > 100:\n",
    "                        return content\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"获取内容失败 {url}: {e}\")\n",
    "        \n",
    "        return \"\"\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = EastmoneyNewsCrawler()\n",
    "    \n",
    "    # 获取新闻\n",
    "    df = crawler.get_news_simple(\"紫金矿业\", max_news=5)\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(f\"\\n成功获取 {len(df)} 条新闻\")\n",
    "        \n",
    "        # 显示结果\n",
    "        for i, row in df.iterrows():\n",
    "            print(f\"\\n{i+1}. {row['title']}\")\n",
    "            print(f\"日期: {row['date']}\")\n",
    "            print(f\"内容: {row['content'][:200]}...\")\n",
    "        \n",
    "        # 保存\n",
    "        df.to_csv('news_simple.csv', index=False, encoding='utf-8-sig')\n",
    "        print(\"\\n数据已保存到 news_simple.csv\")\n",
    "    else:\n",
    "        print(\"未获取到新闻\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c56c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
