#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥æ–°é—»åˆ†æè„šæœ¬ï¼ˆæ­£å¼ç‰ˆ - æ‰¹é‡LLMå¤„ç†ï¼‰
"""

import tushare as ts
import pandas as pd
import json
import requests
import os
import time
import random
from datetime import datetime, timedelta
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DailyNewsAnalyzer:
    def __init__(self, tushare_token, openrouter_api_key):
        """åˆå§‹åŒ–æ–°é—»åˆ†æå™¨"""
        print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–åˆ†æå™¨...")

        # åˆå§‹åŒ–API
        print(f"ğŸ”§ åˆå§‹åŒ–Tushare API...")
        self.ts_pro = ts.pro_api(tushare_token)
        print(f"âœ… Tushare APIåˆå§‹åŒ–å®Œæˆ")

        self.api_key = openrouter_api_key
        self.api_url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {openrouter_api_key}",
            "Content-Type": "application/json",
        }

        # è®¾ç½®æ–‡ä»¶è·¯å¾„
        print(f"ğŸ”§ è®¾ç½®æ–‡ä»¶è·¯å¾„...")
        self.base_dir = Path("D:/projects/q/myQ")
        self.output_dir = self.base_dir / "output" / "daily_analysis"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        print(f"âœ… è¾“å‡ºç›®å½•: {self.output_dir}")

        # åŠ è½½è‚¡ç¥¨æ± 
        print(f"ğŸ”§ åŠ è½½è‚¡ç¥¨æ± ...")
        self.stock_pool = self._load_stock_pool()
        print(f"âœ… è‚¡ç¥¨æ± åŠ è½½å®Œæˆ: {len(self.stock_pool)} åªè‚¡ç¥¨")

        print(f"ğŸ”§ æå–å…³é”®è¯...")
        self.stock_keywords = self._extract_keywords()
        print(f"âœ… å…³é”®è¯æå–å®Œæˆ: {len(self.stock_keywords)} ä¸ªè‚¡ç¥¨")

    def _load_stock_pool(self):
        """åŠ è½½è‚¡ç¥¨æ± """
        stock_pool_path = self.base_dir / "config" / "stock_pool.json"

        with open(stock_pool_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # æ”¯æŒä¸åŒçš„JSONæ ¼å¼
        if 'stocks' in data:
            stocks = data['stocks']
        elif 'stock_database' in data:
            stocks = data['stock_database']
        else:
            stocks = data if isinstance(data, list) else []

        logger.info(f"åŠ è½½è‚¡ç¥¨æ± ï¼Œå…± {len(stocks)} åªè‚¡ç¥¨")
        return stocks

    def _extract_keywords(self):
        """æå–å…³é”®è¯"""
        keywords = {}

        for stock in self.stock_pool:
            stock_keywords = []
            stock_keywords.append(stock['stock_name'])
            stock_keywords.append(stock['stock_code'])

            # æš‚æ—¶åªç”¨è‚¡ç¥¨åç§°å’Œä»£ç ä½œä¸ºå…³é”®è¯ï¼ˆç”¨æˆ·å·²æ³¨é‡Šæ‰è¡Œä¸šå’Œä¸šåŠ¡å…³é”®è¯ï¼‰
            keywords[stock['stock_code']] = list(set(stock_keywords))

        logger.info(f"æå–å…³é”®è¯å®Œæˆï¼Œè¦†ç›– {len(keywords)} åªè‚¡ç¥¨")
        return keywords

    def get_daily_news(self, target_date=None):
        """è·å–å½“æ—¥æ–°é—»"""
        if target_date is None:
            start_date = datetime.now().strftime('%Y-%m-%d 09:00:00')
            end_date = datetime.now().strftime('%Y-%m-%d 18:00:00')
        else:
            start_date = f"{target_date} 09:00:00"
            end_date = f"{target_date} 18:00:00"

        logger.info(f"è·å– {target_date or 'ä»Šæ—¥'} çš„æ–°é—»æ•°æ®...")

        news_df = self.ts_pro.news(
            src='sina',
            start_date=start_date,
            end_date=end_date
        )

        if news_df.empty:
            logger.warning("æœªè·å–åˆ°æ–°é—»æ•°æ®")
            return pd.DataFrame()

        logger.info(f"è·å–åˆ° {len(news_df)} æ¡æ–°é—»")
        return news_df

    def match_news_to_stocks(self, news_df):
        """åŒ¹é…æ–°é—»åˆ°è‚¡ç¥¨"""
        if news_df.empty:
            return news_df

        logger.info("å¼€å§‹åŒ¹é…æ–°é—»åˆ°è‚¡ç¥¨...")
        matched_records = []

        for _, news_row in news_df.iterrows():
            # å®‰å…¨è·å–å­—æ®µå€¼
            title = news_row.get('title') or news_row.get('Title') or ''
            content = news_row.get('content') or news_row.get('Content') or ''

            news_title = str(title).lower() if title else ''
            news_content = str(content).lower() if content else ''
            full_text = f"{news_title} {news_content}"

            # åŒ¹é…è‚¡ç¥¨
            for stock in self.stock_pool:
                stock_code = stock['stock_code']
                keywords = self.stock_keywords.get(stock_code, [])

                # æ£€æŸ¥æ˜¯å¦åŒ¹é…
                matched = False
                for keyword in keywords:
                    if len(keyword) >= 2 and keyword.lower() in full_text:
                        matched = True
                        break

                if matched:
                    record = news_row.to_dict()
                    record.update(stock)
                    matched_records.append(record)

        matched_df = pd.DataFrame(matched_records)
        logger.info(f"åŒ¹é…å®Œæˆï¼Œæ‰¾åˆ° {len(matched_df)} æ¡è‚¡ç¥¨ç›¸å…³æ–°é—»")

        return matched_df

    def score_news_with_llm(self, matched_df, batch_size=5):
        """ä½¿ç”¨å¤§æ¨¡å‹æ‰¹é‡å¯¹æ–°é—»è¿›è¡Œè¯„åˆ†"""
        if matched_df.empty:
            logger.warning("åŒ¹é…çš„æ–°é—»æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡LLMè¯„åˆ†")
            return matched_df

        logger.info(f"å¼€å§‹ä½¿ç”¨å¤§æ¨¡å‹æ‰¹é‡è¯„åˆ†ï¼Œå…± {len(matched_df)} æ¡æ–°é—»ï¼Œæ‰¹é‡å¤§å°: {batch_size}")

        scored_results = []
        total_batches = (len(matched_df) + batch_size - 1) // batch_size

        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(matched_df))
            batch_df = matched_df.iloc[start_idx:end_idx]

            logger.info(f"å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹æ¬¡ï¼ŒåŒ…å« {len(batch_df)} æ¡æ–°é—»")

            # æ‰¹é‡è¯„åˆ†
            batch_results = self._score_batch_llm(batch_df)
            scored_results.extend(batch_results)

            # æ§åˆ¶è¯·æ±‚é¢‘ç‡
            if batch_idx < total_batches - 1:  # ä¸æ˜¯æœ€åä¸€æ‰¹
                time.sleep(random.uniform(2.0, 3.0))

        scored_df = pd.DataFrame(scored_results)
        logger.info(f"LLMæ‰¹é‡è¯„åˆ†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(scored_df)} æ¡")

        return scored_df

    def _score_batch_llm(self, batch_df):
        """æ‰¹é‡è¯„åˆ†ä¸€ç»„æ–°é—»"""
        # æ„å»ºæ‰¹é‡promptæ¨¡æ¿
        batch_prompt_template = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹å¤šæ¡æ–°é—»å¯¹å¯¹åº”ä¸ªè‚¡çš„å½±å“ã€‚

è¯·å¯¹ä»¥ä¸‹æ¯æ¡æ–°é—»åˆ†åˆ«è¿›è¡Œåˆ†æï¼Œä¸¥æ ¼æŒ‰ç…§JSONæ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªå¯¹è±¡å¯¹åº”ä¸€æ¡æ–°é—»ï¼š

{news_items}

åˆ†æç»´åº¦ï¼š
1. ç›´æ¥å½±å“è¯„ä¼°ï¼ˆ-5åˆ°+5åˆ†ï¼‰
2. é—´æ¥å½±å“è¯„ä¼°ï¼ˆ-5åˆ°+5åˆ†ï¼‰
3. ç¡®å®šæ€§è¯„ä¼°ï¼ˆ0-1ï¼‰
4. ç»¼åˆè¯„åˆ†ï¼ˆ-10åˆ°+10ï¼‰

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ•°ç»„æ ¼å¼è¿”å›ï¼Œæ•°ç»„ä¸­æ¯ä¸ªå¯¹è±¡å¯¹åº”ä¸Šè¿°ä¸€æ¡æ–°é—»ï¼š
[
  {{
    "news_index": 0,
    "sentiment": "å¼ºçƒˆæ­£é¢/æ­£é¢/ä¸­æ€§åæ­£/ä¸­æ€§/ä¸­æ€§åè´Ÿ/è´Ÿé¢/å¼ºçƒˆè´Ÿé¢",
    "direct_impact_score": åˆ†æ•°,
    "direct_impact_desc": "æè¿°",
    "indirect_impact_score": åˆ†æ•°,
    "indirect_impact_desc": "æè¿°",
    "certainty": 0.xx,
    "time_to_effect": "æ—¶é—´çª—å£",
    "overall_score": ç»¼åˆåˆ†æ•°,
    "risk_factors": "ä¸»è¦é£é™©å› ç´ ",
    "action_suggestion": "å»ºè®®æ“ä½œ"
  }},
  {{
    "news_index": 1,
    ...
  }}
]
"""

        # æ„å»ºæ–°é—»é¡¹ç›®åˆ—è¡¨
        news_items = []
        for idx, (_, row) in enumerate(batch_df.iterrows()):
            news_item = f"""
æ–°é—» {idx}:
- è‚¡ç¥¨ï¼š{row['stock_name']}({row['stock_code']})
- æ‰€å±è¡Œä¸šï¼š{row.get('industry', 'N/A')}
- ä¸»è¥ä¸šåŠ¡ï¼š{row.get('main_business', 'N/A')}
- å½“å‰å¸‚å€¼ï¼š{row.get('market_cap', 'N/A')}
- æ ‡é¢˜ï¼š{str(row.get('title', ''))}
- å†…å®¹ï¼š{str(row.get('content', ''))[:600]}
- å‘å¸ƒæ—¶é—´ï¼š{row.get('datetime', '')}
- æ¶ˆæ¯æ¥æºï¼š{row.get('source', '')}
"""
            news_items.append(news_item)

        # æ„å»ºå®Œæ•´æç¤ºè¯
        prompt = batch_prompt_template.format(
            news_items='\n'.join(news_items)
        )

        # è°ƒç”¨API
        responses = self._call_batch_llm_api(prompt, len(batch_df))

        # å¤„ç†ç»“æœ
        batch_results = []
        for idx, (_, row) in enumerate(batch_df.iterrows()):
            result = row.to_dict()
            result['analysis_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            # åŒ¹é…å¯¹åº”çš„è¯„åˆ†ç»“æœ
            if responses and idx < len(responses):
                response = responses[idx]
                if isinstance(response, dict):
                    result.update(response)
                    logger.info(f"å·²è¯„åˆ†: {row['stock_name']} - ç»¼åˆåˆ†æ•°: {response.get('overall_score', 'N/A')}")
                else:
                    logger.warning(f"è¯„åˆ†ç»“æœæ ¼å¼å¼‚å¸¸: {row['stock_name']}")
            else:
                logger.warning(f"æœªè·å–åˆ°è¯„åˆ†ç»“æœ: {row['stock_name']}")

            batch_results.append(result)

        return batch_results

    def _call_batch_llm_api(self, prompt, expected_count):
        """è°ƒç”¨æ‰¹é‡LLM API"""
        logger.info(f"è°ƒç”¨LLM APIï¼ŒæœŸæœ›è¿”å› {expected_count} æ¡è¯„åˆ†ç»“æœ...")

        payload = {
            "model": "deepseek/deepseek-chat-v3.1:free",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.1,
            "max_tokens": 4000  # å¢åŠ tokené™åˆ¶ä»¥æ”¯æŒæ‰¹é‡å“åº”
        }

        response = requests.post(
            self.api_url,
            headers=self.headers,
            json=payload,
            timeout=60  # å¢åŠ è¶…æ—¶æ—¶é—´
        )

        logger.info(f"APIå“åº”çŠ¶æ€: {response.status_code}")

        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            logger.info(f"è¿”å›å†…å®¹é•¿åº¦: {len(content)}")

            # è§£æJSONæ•°ç»„
            try:
                # æ¸…ç†å†…å®¹
                content = content.strip()

                # æŸ¥æ‰¾JSONæ•°ç»„
                start = content.find('[')
                end = content.rfind(']') + 1

                if start != -1 and end > start:
                    json_str = content[start:end]

                    # æ¸…ç†å¯èƒ½çš„é—®é¢˜å­—ç¬¦
                    json_str = json_str.replace('\n', '').replace('\r', '').replace('\t', '')

                    # å°è¯•è§£æ
                    parsed_json = json.loads(json_str)

                    if isinstance(parsed_json, list):
                        logger.info(f"æˆåŠŸè§£æ {len(parsed_json)} æ¡è¯„åˆ†ç»“æœ")

                        # éªŒè¯æ¯ä¸ªç»“æœçš„å¿…è¦å­—æ®µ
                        for i, item in enumerate(parsed_json):
                            if isinstance(item, dict):
                                required_fields = ['sentiment', 'overall_score', 'certainty']
                                for field in required_fields:
                                    if field not in item:
                                        logger.warning(f"ç¬¬{i}æ¡ç»“æœç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
                                        item[field] = 0 if field in ['overall_score', 'certainty'] else "ä¸­æ€§"

                        return parsed_json
                    else:
                        logger.error("è¿”å›å†…å®¹ä¸æ˜¯JSONæ•°ç»„æ ¼å¼")
                        return []

            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æé”™è¯¯: {e}")
                logger.debug(f"åŸå§‹å“åº”: {content[:500]}...")
                return []

        else:
            logger.error(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
            if response.text:
                logger.error(f"é”™è¯¯ä¿¡æ¯: {response.text[:200]}")

        return []

    def save_results(self, results_df, target_date):
        """ä¿å­˜ç»“æœåˆ°CSVï¼Œæ”¯æŒè¿½åŠ å’Œè¦†ç›–"""
        if results_df.empty:
            logger.warning("æ²¡æœ‰ç»“æœå¯ä¿å­˜")
            return

        filename = self.output_dir / f"news_analysis_{target_date.replace('-', '')}.csv"

        # ç›´æ¥è¦†ç›–æ–‡ä»¶ï¼ˆå®ç°åŒæ—¥è¦†ç›–ï¼‰
        results_df.to_csv(filename, index=False, encoding='utf-8-sig')
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {filename}")

    def generate_factor_report(self, results_df, target_date):
        """ç”Ÿæˆå› å­å¼ºåº¦æŠ¥å‘Š"""
        if results_df.empty:
            logger.warning("æ²¡æœ‰æ•°æ®ç”ŸæˆæŠ¥å‘Š")
            return pd.DataFrame()

        logger.info("å¼€å§‹ç”Ÿæˆå› å­å¼ºåº¦æŠ¥å‘Š...")

        # æŒ‰è‚¡ç¥¨èšåˆ
        stock_scores = results_df.groupby(['stock_code', 'stock_name']).agg({
            'overall_score': ['mean', 'max', 'count'],
            'certainty': 'mean'
        }).round(4)

        # é‡å‘½ååˆ—
        stock_scores.columns = ['avg_score', 'max_score', 'news_count', 'avg_certainty']
        stock_scores = stock_scores.reset_index()

        # è®¡ç®—å› å­å¼ºåº¦
        stock_scores['factor_strength'] = (
            stock_scores['avg_score'] * 0.4 +
            stock_scores['max_score'] * 0.3 +
            stock_scores['avg_certainty'] * 10 * 0.3
        ).round(4)

        # æŒ‰å› å­å¼ºåº¦æ’åº
        stock_scores = stock_scores.sort_values('factor_strength', ascending=False)

        # ä¿å­˜è‚¡ç¥¨è¯„åˆ†ç»“æœ
        scores_filename = self.output_dir / f"stock_scores_{target_date.replace('-', '')}.csv"
        stock_scores.to_csv(scores_filename, index=False, encoding='utf-8-sig')

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_filename = self.output_dir / f"factor_report_{target_date.replace('-', '')}.md"

        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(f"# Daily News Factor Analysis Report - {target_date}\n\n")

            # å¼ºå› å­è‚¡ç¥¨
            strong_stocks = stock_scores[stock_scores['factor_strength'] >= 3.0]

            if not strong_stocks.empty:
                f.write("## Strong Factor Stocks (Factor Strength >= 3.0)\n\n")
                f.write("| Rank | Stock | Code | Factor Strength | Avg Score | Max Score | News Count |\n")
                f.write("|------|-------|------|----------------|-----------|-----------|------------|\n")

                for idx, row in strong_stocks.head(10).iterrows():
                    f.write(f"| {idx+1} | {row['stock_name']} | {row['stock_code']} | {row['factor_strength']:.4f} | {row['avg_score']:.4f} | {row['max_score']:.4f} | {row['news_count']} |\n")
            else:
                f.write("## No Strong Factor Stocks Found\n\n")
                f.write("No stocks with factor strength >= 3.0 found today.\n\n")

            f.write("\n## All Analyzed Stocks\n\n")
            f.write("| Rank | Stock | Code | Factor Strength | Avg Score | Max Score | News Count |\n")
            f.write("|------|-------|------|----------------|-----------|-----------|------------|\n")

            for idx, row in stock_scores.head(20).iterrows():
                f.write(f"| {idx+1} | {row['stock_name']} | {row['stock_code']} | {row['factor_strength']:.4f} | {row['avg_score']:.4f} | {row['max_score']:.4f} | {row['news_count']} |\n")

        logger.info(f"å› å­å¼ºåº¦æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}")

        # è¿”å›å¼ºå› å­è‚¡ç¥¨
        return strong_stocks

def main():
    """æ­£å¼ç‰ˆä¸»å‡½æ•°"""
    # å¯¼å…¥é…ç½®
    import sys
    sys.path.append(str(Path(__file__).parent.parent / "config"))
    from api_config import TUSHARE_TOKEN, OPENROUTER_API_KEY

    target_date = "2024-12-20"
    print(f"ğŸš€ å¼€å§‹æ¯æ—¥æ–°é—»å› å­åˆ†æ - {target_date}")

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = DailyNewsAnalyzer(TUSHARE_TOKEN, OPENROUTER_API_KEY)

    # 1. è·å–å½“æ—¥æ–°é—»
    print("\nğŸ“° æ­¥éª¤1: è·å–æ–°é—»æ•°æ®...")
    news_df = analyzer.get_daily_news(target_date)
    if news_df.empty:
        print("æœªè·å–åˆ°æ–°é—»æ•°æ®ï¼Œç¨‹åºç»“æŸ")
        return

    # 2. åŒ¹é…æ–°é—»åˆ°è‚¡ç¥¨
    print("\nğŸ” æ­¥éª¤2: åŒ¹é…è‚¡ç¥¨ç›¸å…³æ–°é—»...")
    matched_df = analyzer.match_news_to_stocks(news_df)
    if matched_df.empty:
        print("æœªåŒ¹é…åˆ°ç›¸å…³è‚¡ç¥¨æ–°é—»ï¼Œç¨‹åºç»“æŸ")
        return

    # 3. å¤§æ¨¡å‹æ‰¹é‡è¯„åˆ†
    print("\nğŸ¤– æ­¥éª¤3: æ‰¹é‡LLMè¯„åˆ†...")
    scored_df = analyzer.score_news_with_llm(matched_df)
    if scored_df.empty:
        print("LLMè¯„åˆ†å¤±è´¥ï¼Œç¨‹åºç»“æŸ")
        return

    # 4. ä¿å­˜ç»“æœ
    print("\nğŸ’¾ æ­¥éª¤4: ä¿å­˜ç»“æœ...")
    analyzer.save_results(scored_df, target_date)

    # 5. ç”Ÿæˆå› å­å¼ºåº¦æŠ¥å‘Š
    print("\nğŸ“Š æ­¥éª¤5: ç”Ÿæˆå› å­å¼ºåº¦æŠ¥å‘Š...")
    strong_stocks = analyzer.generate_factor_report(scored_df, target_date)

    print(f"\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“Š åˆ†æäº† {len(scored_df)} æ¡è‚¡ç¥¨æ–°é—»")

    if not strong_stocks.empty:
        print(f"ğŸ”¥ å‘ç° {len(strong_stocks)} åªå¼ºå› å­è‚¡ç¥¨ï¼š")
        for idx, row in strong_stocks.head(5).iterrows():
            print(f"   {idx+1}. {row['stock_name']} ({row['stock_code']}): å› å­å¼ºåº¦ {row['factor_strength']:.2f}")
    else:
        print("ğŸ“‰ ä»Šæ—¥æœªå‘ç°å¼ºå› å­è‚¡ç¥¨")

    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {analyzer.output_dir}")

if __name__ == "__main__":
    main()