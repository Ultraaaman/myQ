#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»è¯„åˆ†ä¸å¼€ç›˜å¼ºåº¦æ·±åº¦åˆ†æ
============================

ä¸“é—¨åˆ†ææ–°é—»æƒ…æ„Ÿè¯„åˆ†ä¸å¼€ç›˜å¼ºåº¦æŒ‡æ ‡çš„ç›¸å…³æ€§ï¼ŒåŒ…æ‹¬ï¼š
1. å¼€ç›˜å¼ºåº¦çš„ICç³»æ•°åˆ†æ
2. ä¸åŒæƒ…æ„Ÿè¯„åˆ†é˜ˆå€¼çš„å½±å“
3. æ—¶é—´è¡°å‡æ•ˆåº”åˆ†æ
4. å¼€ç›˜å¼ºåº¦ä¸æˆäº¤é‡ã€æ³¢åŠ¨ç‡çš„ç»„åˆåˆ†æ
5. åŸºäºå¼€ç›˜å¼ºåº¦çš„äº¤æ˜“ç­–ç•¥å›æµ‹

å‘ç°ï¼šå¼€ç›˜å¼ºåº¦æ¯”ç®€å•æ”¶ç›Šç‡æ›´èƒ½åæ˜ æ–°é—»çš„å¸‚åœºå½±å“
åº”ç”¨ï¼šçŸ­æœŸäº¤æ˜“ç­–ç•¥ã€ç›˜å£åˆ†æã€æƒ…ç»ªä¼ å¯¼ç ”ç©¶

ä½œè€…: Claude Code
æ—¥æœŸ: 2025-09-23
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import sys
import os
from scipy.stats import pearsonr, spearmanr
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(r'E:\projects\myQ')

# å¯¼å…¥ market_data æ¨¡å—
try:
    from quantlib.market_data import get_stock_data, MarketDataManager
    MARKET_DATA_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥ quantlib.market_data æ¨¡å—: {e}")
    MARKET_DATA_AVAILABLE = False

# è®¾ç½®å›¾è¡¨æ ·å¼
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8')

# è®¾ç½®è‹±æ–‡æ˜¾ç¤º
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['font.size'] = 10

def load_and_prepare_data(news_file_path, stock_code='601899'):
    """
    åŠ è½½å¹¶å‡†å¤‡åˆ†ææ•°æ®

    Args:
        news_file_path (str): æ–°é—»è¯„åˆ†æ–‡ä»¶è·¯å¾„
        stock_code (str): è‚¡ç¥¨ä»£ç 

    Returns:
        pd.DataFrame: åˆå¹¶åçš„åˆ†ææ•°æ®
    """
    print("=" * 60)
    print("æ­¥éª¤ 1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
    print("=" * 60)

    # è¯»å–æ–°é—»æ•°æ®
    news_df = pd.read_csv(news_file_path, encoding='utf-8-sig')
    news_df['date'] = pd.to_datetime(news_df['original_date']).dt.date

    print(f"âœ“ æ–°é—»æ•°æ®: {len(news_df)} æ¡")
    print(f"âœ“ æ—¶é—´èŒƒå›´: {news_df['date'].min()} åˆ° {news_df['date'].max()}")

    # èšåˆæ—¥åº¦æ–°é—»è¯„åˆ†
    daily_scores = news_df.groupby('date').agg({
        'overall_score': ['mean', 'std', 'count', 'min', 'max'],
        'direct_impact_score': ['mean', 'std'],
        'indirect_impact_score': ['mean', 'std'],
        'certainty': ['mean', 'std'],
        'sentiment': lambda x: x.mode()[0] if not x.empty else 'neutral'
    }).round(4)

    # æ‰å¹³åŒ–åˆ—å
    daily_scores.columns = [f"{col[0]}_{col[1]}" if col[1] else col[0]
                           for col in daily_scores.columns]
    daily_scores = daily_scores.reset_index()

    # è®¡ç®—ç¡®å®šæ€§åŠ æƒè¯„åˆ†
    daily_scores['weighted_score'] = (daily_scores['overall_score_mean'] *
                                     daily_scores['certainty_mean'])

    print(f"âœ“ æ—¥åº¦èšåˆ: {len(daily_scores)} å¤©")

    # è·å–è‚¡ä»·æ•°æ®
    if MARKET_DATA_AVAILABLE:
        try:
            manager = MarketDataManager()

            # è®¡ç®—æ—¶é—´è·¨åº¦ï¼Œé€‰æ‹©åˆé€‚çš„period
            start_date = daily_scores['date'].min() - timedelta(days=10)
            end_date = daily_scores['date'].max() + timedelta(days=10)
            time_span = (end_date - start_date).days

            print(f"âœ“ éœ€è¦çš„è‚¡ä»·æ•°æ®æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date} ({time_span}å¤©)")

            # ç”±äºæ–°é—»æ•°æ®æ˜¯2024å¹´çš„ï¼Œéœ€è¦è·å–æ›´é•¿æœŸé—´çš„æ•°æ®
            # é€‰æ‹©è¶³å¤Ÿé•¿çš„periodæ¥è¦†ç›–2024å¹´çš„æ•°æ®
            current_date = datetime.now().date()
            days_from_start = (current_date - start_date).days

            if days_from_start <= 95:
                period = '6mo'
            elif days_from_start <= 185:
                period = '1y'
            elif days_from_start <= 370:
                period = '2y'
            else:
                period = '5y'

            print(f"âœ“ é€‰æ‹©period: {period} (è·ç¦»å¼€å§‹æ—¥æœŸ{days_from_start}å¤©)")

            stock_data = manager.get_stock_data(
                symbol=stock_code,
                market='CN',  # ä¸­å›½å¸‚åœº
                period=period,
                interval='1d'
            )

            if stock_data is not None and len(stock_data) > 0:
                print(f"âœ“ åŸå§‹è‚¡ä»·æ•°æ®å½¢çŠ¶: {stock_data.shape}")
                print(f"âœ“ åŸå§‹è‚¡ä»·æ•°æ®åˆ—: {list(stock_data.columns)}")

                # é‡ç½®ç´¢å¼•ï¼Œç¡®ä¿dateåˆ—å­˜åœ¨
                stock_data = stock_data.reset_index()

                # é€‰æ‹©éœ€è¦çš„åˆ—ï¼ˆOHLCVï¼‰
                required_columns = ['open', 'high', 'low', 'close', 'volume']
                available_columns = []

                # æŸ¥æ‰¾åŒ¹é…çš„åˆ—ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                for req_col in required_columns:
                    for col in stock_data.columns:
                        if col.lower() == req_col.lower():
                            available_columns.append(col)
                            break

                if len(available_columns) >= 4:  # è‡³å°‘éœ€è¦OHLC
                    # åªé€‰æ‹©éœ€è¦çš„åˆ—
                    stock_data = stock_data[['date'] + available_columns[:5]]  # date + OHLCV

                    # é‡å‘½ååˆ—
                    new_column_names = ['date', 'Open', 'High', 'Low', 'Close']
                    if len(available_columns) == 5:
                        new_column_names.append('Volume')

                    stock_data.columns = new_column_names[:len(stock_data.columns)]

                    # ç¡®ä¿æ—¥æœŸåˆ—æ ¼å¼æ­£ç¡®
                    stock_data['date'] = pd.to_datetime(stock_data['date']).dt.date

                    print(f"âœ“ å¤„ç†åè‚¡ä»·æ•°æ®: {len(stock_data)} å¤©")
                    print(f"âœ“ è‚¡ä»·æ•°æ®æ—¶é—´èŒƒå›´: {stock_data['date'].min()} åˆ° {stock_data['date'].max()}")
                    print(f"âœ“ æœ€ç»ˆåˆ—å: {list(stock_data.columns)}")

                    # ç­›é€‰è‚¡ä»·æ•°æ®åˆ°æ–°é—»æ—¶é—´èŒƒå›´
                    news_start = daily_scores['date'].min()
                    news_end = daily_scores['date'].max()

                    stock_filtered = stock_data[
                        (stock_data['date'] >= news_start) &
                        (stock_data['date'] <= news_end)
                    ].copy()

                    if len(stock_filtered) > 0:
                        stock_data = stock_filtered
                        print(f"âœ“ ç­›é€‰åˆ°æ–°é—»æ—¶é—´èŒƒå›´çš„è‚¡ä»·æ•°æ®: {len(stock_data)} å¤©")
                        print(f"âœ“ ç­›é€‰åè‚¡ä»·æ—¶é—´èŒƒå›´: {stock_data['date'].min()} åˆ° {stock_data['date'].max()}")
                    else:
                        print(f"âš ï¸ åœ¨æ–°é—»æ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ‰¾åˆ°è‚¡ä»·æ•°æ®")
                        print(f"   æ–°é—»æ—¶é—´: {news_start} åˆ° {news_end}")
                        print(f"   è‚¡ä»·æ—¶é—´: {stock_data['date'].min()} åˆ° {stock_data['date'].max()}")
                else:
                    raise ValueError(f"ç¼ºå°‘å¿…è¦çš„OHLCåˆ—ï¼Œå¯ç”¨åˆ—: {available_columns}")
            else:
                raise ValueError("è‚¡ä»·æ•°æ®ä¸ºç©º")

        except Exception as e:
            print(f"âš ï¸ è·å–å®é™…è‚¡ä»·æ•°æ®å¤±è´¥: {e}")
            print("âœ“ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            stock_data = create_simulated_stock_data(daily_scores)
    else:
        print("âœ“ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        stock_data = create_simulated_stock_data(daily_scores)

    # åˆå¹¶æ•°æ®
    merged_data = merge_and_calculate_indicators(daily_scores, stock_data)

    return merged_data

def create_simulated_stock_data(daily_scores):
    """åˆ›å»ºæ¨¡æ‹Ÿè‚¡ä»·æ•°æ®"""
    date_range = pd.date_range(
        start=daily_scores['date'].min() - timedelta(days=10),
        end=daily_scores['date'].max() + timedelta(days=10),
        freq='D'
    )

    # ç§»é™¤å‘¨æœ«
    date_range = [d for d in date_range if d.weekday() < 5]

    np.random.seed(42)
    n_days = len(date_range)

    # æ¨¡æ‹Ÿè‚¡ä»·èµ°åŠ¿
    returns = np.random.normal(0.001, 0.02, n_days)  # æ—¥æ”¶ç›Šç‡
    prices = 100 * np.cumprod(1 + returns)  # ç´¯ç§¯ä»·æ ¼

    stock_data = pd.DataFrame({
        'date': [d.date() for d in date_range],
        'Open': prices * (1 + np.random.normal(0, 0.005, n_days)),
        'High': prices * (1 + np.abs(np.random.normal(0.01, 0.01, n_days))),
        'Low': prices * (1 - np.abs(np.random.normal(0.01, 0.01, n_days))),
        'Close': prices,
        'Volume': np.random.lognormal(15, 0.5, n_days)
    })

    # ç¡®ä¿High >= max(Open, Close) å’Œ Low <= min(Open, Close)
    stock_data['High'] = np.maximum(stock_data['High'],
                                   np.maximum(stock_data['Open'], stock_data['Close']))
    stock_data['Low'] = np.minimum(stock_data['Low'],
                                  np.minimum(stock_data['Open'], stock_data['Close']))

    return stock_data

def merge_and_calculate_indicators(daily_scores, stock_data):
    """åˆå¹¶æ•°æ®å¹¶è®¡ç®—å¼€ç›˜å¼ºåº¦ç­‰æŒ‡æ ‡"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 2: è®¡ç®—å¼€ç›˜å¼ºåº¦åŠç›¸å…³æŒ‡æ ‡")
    print("=" * 60)

    # è°ƒè¯•ä¿¡æ¯
    print(f"âœ“ æ–°é—»æ•°æ®æ—¥æœŸèŒƒå›´: {daily_scores['date'].min()} åˆ° {daily_scores['date'].max()}")
    print(f"âœ“ è‚¡ä»·æ•°æ®æ—¥æœŸèŒƒå›´: {stock_data['date'].min()} åˆ° {stock_data['date'].max()}")
    print(f"âœ“ æ–°é—»æ•°æ®æ—¥æœŸç±»å‹: {type(daily_scores['date'].iloc[0])}")
    print(f"âœ“ è‚¡ä»·æ•°æ®æ—¥æœŸç±»å‹: {type(stock_data['date'].iloc[0])}")

    # ç¡®ä¿æ—¥æœŸæ ¼å¼ä¸€è‡´
    daily_scores['date'] = pd.to_datetime(daily_scores['date']).dt.date
    stock_data['date'] = pd.to_datetime(stock_data['date']).dt.date

    # åˆå¹¶æ•°æ® - å…ˆå°è¯•inner joinï¼Œå¦‚æœç»“æœä¸ºç©ºåˆ™å°è¯•å…¶ä»–æ–¹æ³•
    merged_data = pd.merge(daily_scores, stock_data, on='date', how='inner')

    print(f"âœ“ Inner joinç»“æœ: {len(merged_data)} å¤©")

    if len(merged_data) == 0:
        print("âš ï¸ Inner joinç»“æœä¸ºç©ºï¼Œå°è¯•æŸ¥æ‰¾é‡å æ—¥æœŸ...")

        # æŸ¥æ‰¾é‡å çš„æ—¥æœŸ
        news_dates = set(daily_scores['date'])
        stock_dates = set(stock_data['date'])
        overlap_dates = news_dates.intersection(stock_dates)

        print(f"âœ“ æ–°é—»æ•°æ®å¤©æ•°: {len(news_dates)}")
        print(f"âœ“ è‚¡ä»·æ•°æ®å¤©æ•°: {len(stock_dates)}")
        print(f"âœ“ é‡å å¤©æ•°: {len(overlap_dates)}")

        if len(overlap_dates) > 0:
            print(f"âœ“ é‡å æ—¥æœŸç¤ºä¾‹: {sorted(list(overlap_dates))[:5]}")

            # æ‰‹åŠ¨ç­›é€‰é‡å æ—¥æœŸçš„æ•°æ®
            news_subset = daily_scores[daily_scores['date'].isin(overlap_dates)].copy()
            stock_subset = stock_data[stock_data['date'].isin(overlap_dates)].copy()

            merged_data = pd.merge(news_subset, stock_subset, on='date', how='inner')
        else:
            print("âœ“ æ–°é—»æ•°æ®æ—¥æœŸç¤ºä¾‹:", sorted(list(news_dates))[:5])
            print("âœ“ è‚¡ä»·æ•°æ®æ—¥æœŸç¤ºä¾‹:", sorted(list(stock_dates))[:5])

    merged_data = merged_data.sort_values('date').reset_index(drop=True)
    print(f"âœ“ æœ€ç»ˆåˆå¹¶åæ•°æ®: {len(merged_data)} å¤©")

    # è®¡ç®—åŸºç¡€æŒ‡æ ‡
    merged_data['price_change'] = merged_data['Close'].pct_change()

    # æ ¸å¿ƒæŒ‡æ ‡ï¼šå¼€ç›˜å¼ºåº¦ç›¸å…³
    print("âœ“ è®¡ç®—å¼€ç›˜å¼ºåº¦æŒ‡æ ‡...")

    # 1. å¼€ç›˜å¼ºåº¦ï¼ˆå¼€ç›˜åˆ°æœ€é«˜ä»·çš„æ¶¨å¹…ï¼‰
    merged_data['open_strength'] = (merged_data['High'] - merged_data['Open']) / merged_data['Open']

    # 2. æ”¶ç›˜å¼ºåº¦ï¼ˆæœ€ä½ä»·åˆ°æ”¶ç›˜ä»·çš„æ¶¨å¹…ï¼‰
    merged_data['close_strength'] = (merged_data['Close'] - merged_data['Low']) / merged_data['Low']

    # 3. å¼€ç›˜ç›¸å¯¹å¼ºåº¦ï¼ˆå¼€ç›˜å¼ºåº¦ vs å…¨æ—¥æŒ¯å¹…ï¼‰
    merged_data['daily_amplitude'] = (merged_data['High'] - merged_data['Low']) / merged_data['Close']
    merged_data['open_strength_ratio'] = merged_data['open_strength'] / (merged_data['daily_amplitude'] + 1e-8)

    # 4. éš”å¤œè·³ç©º
    merged_data['overnight_gap'] = merged_data['Open'] / merged_data['Close'].shift(1) - 1

    # 5. å¼€ç›˜ååŠ¨é‡ï¼ˆå¼€ç›˜å¼ºåº¦ vs éš”å¤œè·³ç©ºï¼‰
    merged_data['open_momentum'] = merged_data['open_strength'] / (np.abs(merged_data['overnight_gap']) + 1e-8)

    # è®¡ç®—æœªæ¥å¼€ç›˜å¼ºåº¦æŒ‡æ ‡
    print("âœ“ è®¡ç®—æœªæ¥å¼€ç›˜å¼ºåº¦æŒ‡æ ‡...")

    # æœªæ¥1å¤©ã€3å¤©ã€5å¤©çš„å¼€ç›˜å¼ºåº¦
    merged_data['future_open_strength_1d'] = merged_data['open_strength'].shift(-1)
    merged_data['future_open_strength_3d'] = merged_data['open_strength'].shift(-3)
    merged_data['future_open_strength_5d'] = merged_data['open_strength'].shift(-5)

    # æœªæ¥æ”¶ç›˜å¼ºåº¦
    merged_data['future_close_strength_1d'] = merged_data['close_strength'].shift(-1)
    merged_data['future_close_strength_3d'] = merged_data['close_strength'].shift(-3)
    merged_data['future_close_strength_5d'] = merged_data['close_strength'].shift(-5)

    # æœªæ¥å¼€ç›˜ç›¸å¯¹å¼ºåº¦
    merged_data['future_open_ratio_1d'] = merged_data['open_strength_ratio'].shift(-1)
    merged_data['future_open_ratio_3d'] = merged_data['open_strength_ratio'].shift(-3)
    merged_data['future_open_ratio_5d'] = merged_data['open_strength_ratio'].shift(-5)

    # æ–°å¢ï¼šæœªæ¥æœ€é«˜ä»·ç›¸å…³æŒ‡æ ‡
    print("âœ“ è®¡ç®—æœªæ¥æœ€é«˜ä»·æŒ‡æ ‡...")

    # æœªæ¥æœ€é«˜ä»·å¼ºåº¦ï¼ˆå½“æ—¥å¼€ç›˜ä»·åˆ°æœªæ¥æœ€é«˜ä»·çš„æ¶¨å¹…ï¼‰
    merged_data['future_high_strength_1d'] = (merged_data['High'].shift(-1) - merged_data['Open']) / merged_data['Open']
    merged_data['future_high_strength_3d'] = (merged_data['High'].shift(-3) - merged_data['Open']) / merged_data['Open']
    merged_data['future_high_strength_5d'] = (merged_data['High'].shift(-5) - merged_data['Open']) / merged_data['Open']

    # æœªæ¥æœ€é«˜ä»·çªç ´ç¨‹åº¦ï¼ˆæœªæ¥æœ€é«˜ä»·ç›¸å¯¹å½“å‰æ”¶ç›˜ä»·ï¼‰
    merged_data['future_high_breakout_1d'] = (merged_data['High'].shift(-1) - merged_data['Close']) / merged_data['Close']
    merged_data['future_high_breakout_3d'] = (merged_data['High'].shift(-3) - merged_data['Close']) / merged_data['Close']
    merged_data['future_high_breakout_5d'] = (merged_data['High'].shift(-5) - merged_data['Close']) / merged_data['Close']

    # æœªæ¥ä¸Šå½±çº¿é•¿åº¦ï¼ˆæœ€é«˜ä»·ä¸æ”¶ç›˜ä»·çš„å·®å¼‚ï¼‰
    merged_data['future_upper_shadow_1d'] = (merged_data['High'].shift(-1) - merged_data['Close'].shift(-1)) / merged_data['Close'].shift(-1)
    merged_data['future_upper_shadow_3d'] = (merged_data['High'].shift(-3) - merged_data['Close'].shift(-3)) / merged_data['Close'].shift(-3)
    merged_data['future_upper_shadow_5d'] = (merged_data['High'].shift(-5) - merged_data['Close'].shift(-5)) / merged_data['Close'].shift(-5)

    # è®¡ç®—æˆäº¤é‡å’Œæ³¢åŠ¨ç‡æŒ‡æ ‡
    print("âœ“ è®¡ç®—æˆäº¤é‡å’Œæ³¢åŠ¨ç‡æŒ‡æ ‡...")

    # æˆäº¤é‡å˜åŒ–
    merged_data['volume_change'] = merged_data['Volume'].pct_change()
    merged_data['volume_ma_5'] = merged_data['Volume'].rolling(5).mean()
    merged_data['turnover_ratio'] = merged_data['Volume'] / merged_data['volume_ma_5']

    # æ³¢åŠ¨ç‡æŒ‡æ ‡
    merged_data['volatility_5d'] = merged_data['price_change'].rolling(5).std()
    merged_data['atr_5'] = calculate_atr(merged_data, 5)

    # é‡ä»·é…åˆåº¦ï¼ˆå¼€ç›˜å¼ºåº¦ä¸æˆäº¤é‡çš„å…³ç³»ï¼‰
    volume_direction = np.sign(merged_data['volume_change'])
    open_direction = np.sign(merged_data['open_strength'])
    merged_data['open_volume_sync'] = open_direction * volume_direction

    print(f"âœ“ æŒ‡æ ‡è®¡ç®—å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {merged_data.shape}")

    return merged_data

def calculate_atr(data, period=5):
    """è®¡ç®—å¹³å‡çœŸå®æ³¢åŠ¨èŒƒå›´"""
    high_low = data['High'] - data['Low']
    high_close = np.abs(data['High'] - data['Close'].shift(1))
    low_close = np.abs(data['Low'] - data['Close'].shift(1))

    true_range = np.maximum(high_low, np.maximum(high_close, low_close))
    return true_range.rolling(period).mean()

def calculate_ic_metrics(factor_values, target_values):
    """è®¡ç®—ICæŒ‡æ ‡"""
    valid_data = pd.DataFrame({'factor': factor_values, 'target': target_values}).dropna()

    if len(valid_data) < 3:
        return {
            'normal_ic': np.nan,
            'rank_ic': np.nan,
            'normal_ic_pvalue': np.nan,
            'rank_ic_pvalue': np.nan,
            'sample_size': len(valid_data)
        }

    try:
        normal_ic, normal_p = pearsonr(valid_data['factor'], valid_data['target'])
        rank_ic, rank_p = spearmanr(valid_data['factor'], valid_data['target'])

        return {
            'normal_ic': normal_ic,
            'rank_ic': rank_ic,
            'normal_ic_pvalue': normal_p,
            'rank_ic_pvalue': rank_p,
            'sample_size': len(valid_data)
        }
    except:
        return {
            'normal_ic': np.nan,
            'rank_ic': np.nan,
            'normal_ic_pvalue': np.nan,
            'rank_ic_pvalue': np.nan,
            'sample_size': len(valid_data)
        }

def analyze_opening_strength_ic(merged_data):
    """åˆ†æå¼€ç›˜å¼ºåº¦çš„ICç³»æ•°"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 3: å¼€ç›˜å¼ºåº¦ICç³»æ•°åˆ†æ")
    print("=" * 60)

    score_col = 'overall_score_mean'

    if score_col not in merged_data.columns:
        print("âš ï¸ æœªæ‰¾åˆ°è¯„åˆ†åˆ—ï¼Œè·³è¿‡ICåˆ†æ")
        return {}

    results = {}

    # å®šä¹‰è¦åˆ†æçš„å¼€ç›˜å¼ºåº¦æŒ‡æ ‡
    opening_indicators = {
        'future_open_strength_1d': 'æœªæ¥1å¤©å¼€ç›˜å¼ºåº¦',
        'future_open_strength_3d': 'æœªæ¥3å¤©å¼€ç›˜å¼ºåº¦',
        'future_open_strength_5d': 'æœªæ¥5å¤©å¼€ç›˜å¼ºåº¦',
        'future_close_strength_1d': 'æœªæ¥1å¤©æ”¶ç›˜å¼ºåº¦',
        'future_close_strength_3d': 'æœªæ¥3å¤©æ”¶ç›˜å¼ºåº¦',
        'future_close_strength_5d': 'æœªæ¥5å¤©æ”¶ç›˜å¼ºåº¦',
        'future_open_ratio_1d': 'æœªæ¥1å¤©å¼€ç›˜ç›¸å¯¹å¼ºåº¦',
        'future_open_ratio_3d': 'æœªæ¥3å¤©å¼€ç›˜ç›¸å¯¹å¼ºåº¦',
        'future_open_ratio_5d': 'æœªæ¥5å¤©å¼€ç›˜ç›¸å¯¹å¼ºåº¦'
    }

    # æ–°å¢ï¼šæœ€é«˜ä»·ç›¸å…³æŒ‡æ ‡
    high_indicators = {
        'future_high_strength_1d': 'æœªæ¥1å¤©æœ€é«˜ä»·å¼ºåº¦',
        'future_high_strength_3d': 'æœªæ¥3å¤©æœ€é«˜ä»·å¼ºåº¦',
        'future_high_strength_5d': 'æœªæ¥5å¤©æœ€é«˜ä»·å¼ºåº¦',
        'future_high_breakout_1d': 'æœªæ¥1å¤©æœ€é«˜ä»·çªç ´',
        'future_high_breakout_3d': 'æœªæ¥3å¤©æœ€é«˜ä»·çªç ´',
        'future_high_breakout_5d': 'æœªæ¥5å¤©æœ€é«˜ä»·çªç ´',
        'future_upper_shadow_1d': 'æœªæ¥1å¤©ä¸Šå½±çº¿',
        'future_upper_shadow_3d': 'æœªæ¥3å¤©ä¸Šå½±çº¿',
        'future_upper_shadow_5d': 'æœªæ¥5å¤©ä¸Šå½±çº¿'
    }

    print("1. æ–°é—»è¯„åˆ†ä¸æœªæ¥å¼€ç›˜å¼ºåº¦ICåˆ†æ:")

    for indicator, name in opening_indicators.items():
        if indicator in merged_data.columns:
            valid_data = merged_data.dropna(subset=[score_col, indicator])

            if len(valid_data) >= 3:
                ic_metrics = calculate_ic_metrics(valid_data[score_col], valid_data[indicator])
                corr_val, p_val = pearsonr(valid_data[score_col], valid_data[indicator])

                results[indicator] = {
                    'correlation': corr_val,
                    'p_value': p_val,
                    'normal_ic': ic_metrics['normal_ic'],
                    'rank_ic': ic_metrics['rank_ic'],
                    'sample_size': len(valid_data)
                }

                # æ”¹è¿›æ˜¾è‘—æ€§åˆ¤æ–­ï¼šä½¿ç”¨på€¼è€Œä¸æ˜¯ç®€å•é˜ˆå€¼
                if p_val < 0.001:
                    significance = "***ææ˜¾è‘—"
                elif p_val < 0.01:
                    significance = "**å¾ˆæ˜¾è‘—"
                elif p_val < 0.05:
                    significance = "*æ˜¾è‘—"
                elif p_val < 0.1:
                    significance = "è¾¹é™…æ˜¾è‘—"
                else:
                    significance = "ä¸æ˜¾è‘—"

                ic_strength = "å¼º" if abs(ic_metrics['rank_ic']) > 0.1 else "ä¸­ç­‰" if abs(ic_metrics['rank_ic']) > 0.05 else "å¼±"

                print(f"   {name}:")
                print(f"     ç›¸å…³ç³»æ•°: r = {corr_val:.4f} (p = {p_val:.4f}, {significance})")
                print(f"     Normal IC: {ic_metrics['normal_ic']:.4f}")
                print(f"     Rank IC: {ic_metrics['rank_ic']:.4f} ({ic_strength}é¢„æµ‹èƒ½åŠ›)")
                print(f"     æ ·æœ¬æ•°: {len(valid_data)}")
                print()
            else:
                print(f"   {name}: æ•°æ®ä¸è¶³ (ä»…{len(valid_data)}ä¸ªæ ·æœ¬)")

    # æ–°å¢ï¼šæœ€é«˜ä»·æŒ‡æ ‡åˆ†æ
    print("2. æ–°é—»è¯„åˆ†ä¸æœªæ¥æœ€é«˜ä»·æŒ‡æ ‡ICåˆ†æ:")

    for indicator, name in high_indicators.items():
        if indicator in merged_data.columns:
            valid_data = merged_data.dropna(subset=[score_col, indicator])

            if len(valid_data) >= 3:
                ic_metrics = calculate_ic_metrics(valid_data[score_col], valid_data[indicator])
                corr_val, p_val = pearsonr(valid_data[score_col], valid_data[indicator])

                results[indicator] = {
                    'correlation': corr_val,
                    'p_value': p_val,
                    'normal_ic': ic_metrics['normal_ic'],
                    'rank_ic': ic_metrics['rank_ic'],
                    'sample_size': len(valid_data)
                }

                # ä½¿ç”¨på€¼åˆ†çº§æ˜¾è‘—æ€§
                if p_val < 0.001:
                    significance = "***ææ˜¾è‘—"
                elif p_val < 0.01:
                    significance = "**å¾ˆæ˜¾è‘—"
                elif p_val < 0.05:
                    significance = "*æ˜¾è‘—"
                elif p_val < 0.1:
                    significance = "è¾¹é™…æ˜¾è‘—"
                else:
                    significance = "ä¸æ˜¾è‘—"

                ic_strength = "å¼º" if abs(ic_metrics['rank_ic']) > 0.1 else "ä¸­ç­‰" if abs(ic_metrics['rank_ic']) > 0.05 else "å¼±"

                print(f"   {name}:")
                print(f"     ç›¸å…³ç³»æ•°: r = {corr_val:.4f} (p = {p_val:.4f}, {significance})")
                print(f"     Normal IC: {ic_metrics['normal_ic']:.4f}")
                print(f"     Rank IC: {ic_metrics['rank_ic']:.4f} ({ic_strength}é¢„æµ‹èƒ½åŠ›)")
                print(f"     æ ·æœ¬æ•°: {len(valid_data)}")
                print()
            else:
                print(f"   {name}: æ•°æ®ä¸è¶³ (ä»…{len(valid_data)}ä¸ªæ ·æœ¬)")

    return results

def analyze_sentiment_thresholds(merged_data):
    """åˆ†æä¸åŒæƒ…æ„Ÿè¯„åˆ†é˜ˆå€¼çš„å½±å“"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 4: æƒ…æ„Ÿè¯„åˆ†é˜ˆå€¼åˆ†æ")
    print("=" * 60)

    score_col = 'overall_score_mean'
    target_col = 'future_open_strength_1d'

    if score_col not in merged_data.columns or target_col not in merged_data.columns:
        print("âš ï¸ ç¼ºå°‘å¿…è¦åˆ—ï¼Œè·³è¿‡é˜ˆå€¼åˆ†æ")
        return

    valid_data = merged_data.dropna(subset=[score_col, target_col])

    if len(valid_data) < 10:
        print(f"âš ï¸ æ•°æ®ä¸è¶³ (ä»…{len(valid_data)}ä¸ªæ ·æœ¬)")
        return

    # å®šä¹‰ä¸åŒçš„è¯„åˆ†é˜ˆå€¼
    thresholds = [
        (-10, -2, 'Very Negative'),
        (-2, -0.5, 'Negative'),
        (-0.5, 0.5, 'Neutral'),
        (0.5, 2, 'Positive'),
        (2, 10, 'Very Positive')
    ]

    print("1. ä¸åŒæƒ…æ„Ÿå¼ºåº¦å¯¹æœªæ¥1å¤©å¼€ç›˜å¼ºåº¦çš„å½±å“:")

    threshold_results = []

    for min_score, max_score, label in thresholds:
        subset = valid_data[
            (valid_data[score_col] >= min_score) &
            (valid_data[score_col] < max_score)
        ]

        if len(subset) >= 3:
            mean_open_strength = subset[target_col].mean()
            std_open_strength = subset[target_col].std()
            median_open_strength = subset[target_col].median()

            # è®¡ç®—ä¸ä¸­æ€§ç»„çš„å·®å¼‚
            neutral_subset = valid_data[
                (valid_data[score_col] >= -0.5) &
                (valid_data[score_col] < 0.5)
            ]

            if len(neutral_subset) >= 3:
                neutral_mean = neutral_subset[target_col].mean()
                difference = mean_open_strength - neutral_mean

                # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
                from scipy.stats import ttest_ind
                t_stat, p_val = ttest_ind(subset[target_col], neutral_subset[target_col])

                # æ”¹è¿›æ˜¾è‘—æ€§åˆ¤æ–­
                if p_val < 0.001:
                    significance = "***ææ˜¾è‘—"
                elif p_val < 0.01:
                    significance = "**å¾ˆæ˜¾è‘—"
                elif p_val < 0.05:
                    significance = "*æ˜¾è‘—"
                elif p_val < 0.1:
                    significance = "è¾¹é™…æ˜¾è‘—"
                else:
                    significance = "ä¸æ˜¾è‘—"
            else:
                difference = np.nan
                significance = "æ— æ³•æ¯”è¾ƒ"

            threshold_results.append({
                'label': label,
                'count': len(subset),
                'mean_open_strength': mean_open_strength,
                'difference_from_neutral': difference,
                'significance': significance
            })

            print(f"   {label} (è¯„åˆ†: {min_score} ~ {max_score}):")
            print(f"     æ ·æœ¬æ•°: {len(subset)}")
            print(f"     å¹³å‡å¼€ç›˜å¼ºåº¦: {mean_open_strength:.4f}")
            if not np.isnan(difference):
                print(f"     ç›¸å¯¹ä¸­æ€§ç»„å·®å¼‚: {difference:.4f} (p = {p_val:.4f}, {significance})")
            else:
                print(f"     ç›¸å¯¹ä¸­æ€§ç»„å·®å¼‚: {difference:.4f} ({significance})")
            print()
        else:
            print(f"   {label}: æ ·æœ¬ä¸è¶³ ({len(subset)}ä¸ª)")

    return threshold_results

def analyze_time_decay(merged_data):
    """åˆ†ææ—¶é—´è¡°å‡æ•ˆåº”"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 5: æ—¶é—´è¡°å‡æ•ˆåº”åˆ†æ")
    print("=" * 60)

    score_col = 'overall_score_mean'

    # åˆ†æ1å¤©ã€3å¤©ã€5å¤©çš„è¡°å‡
    periods = [
        ('future_open_strength_1d', '1å¤©'),
        ('future_open_strength_3d', '3å¤©'),
        ('future_open_strength_5d', '5å¤©')
    ]

    decay_results = []

    print("1. æ–°é—»å½±å“çš„æ—¶é—´è¡°å‡åˆ†æ:")

    for target_col, period_name in periods:
        if target_col in merged_data.columns:
            valid_data = merged_data.dropna(subset=[score_col, target_col])

            if len(valid_data) >= 3:
                ic_metrics = calculate_ic_metrics(valid_data[score_col], valid_data[target_col])
                corr_val, p_val = pearsonr(valid_data[score_col], valid_data[target_col])

                decay_results.append({
                    'period': period_name,
                    'correlation': corr_val,
                    'rank_ic': ic_metrics['rank_ic'],
                    'sample_size': len(valid_data)
                })

                significance = "**æ˜¾è‘—**" if p_val < 0.05 else "ä¸æ˜¾è‘—"

                print(f"   æœªæ¥{period_name}å¼€ç›˜å¼ºåº¦:")
                print(f"     ç›¸å…³ç³»æ•°: {corr_val:.4f} ({significance})")
                print(f"     Rank IC: {ic_metrics['rank_ic']:.4f}")
                print(f"     æ ·æœ¬æ•°: {len(valid_data)}")
                print()

    # è®¡ç®—è¡°å‡ç‡
    if len(decay_results) >= 2:
        print("2. è¡°å‡ç‡åˆ†æ:")
        base_ic = decay_results[0]['rank_ic']  # 1å¤©çš„ICä½œä¸ºåŸºå‡†

        for i, result in enumerate(decay_results[1:], 1):
            decay_rate = (base_ic - result['rank_ic']) / base_ic if base_ic != 0 else 0
            print(f"   ç›¸å¯¹1å¤©çš„è¡°å‡ç‡ ({result['period']}): {decay_rate:.2%}")

    return decay_results

def analyze_combined_indicators(merged_data):
    """åˆ†æå¼€ç›˜å¼ºåº¦ä¸å…¶ä»–æŒ‡æ ‡çš„ç»„åˆæ•ˆæœ"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 6: ç»„åˆæŒ‡æ ‡åˆ†æ")
    print("=" * 60)

    score_col = 'overall_score_mean'
    target_col = 'future_open_strength_1d'

    if any(col not in merged_data.columns for col in [score_col, target_col]):
        print("âš ï¸ ç¼ºå°‘å¿…è¦åˆ—ï¼Œè·³è¿‡ç»„åˆåˆ†æ")
        return

    # åˆ›å»ºç»„åˆæŒ‡æ ‡
    print("âœ“ åˆ›å»ºç»„åˆæŒ‡æ ‡...")

    # 1. æ–°é—»è¯„åˆ† Ã— æˆäº¤é‡å¼‚å¸¸
    merged_data['score_volume_signal'] = (merged_data[score_col] *
                                         merged_data['turnover_ratio'])

    # 2. æ–°é—»è¯„åˆ† Ã— æ³¢åŠ¨ç‡
    merged_data['score_volatility_signal'] = (merged_data[score_col] *
                                             merged_data['volatility_5d'].fillna(0))

    # 3. æ–°é—»è¯„åˆ† Ã— éš”å¤œè·³ç©º
    merged_data['score_gap_signal'] = (merged_data[score_col] *
                                      np.abs(merged_data['overnight_gap']))

    # åˆ†æç»„åˆæŒ‡æ ‡çš„æ•ˆæœ
    combined_indicators = {
        score_col: 'å•çº¯æ–°é—»è¯„åˆ†',
        'score_volume_signal': 'æ–°é—»è¯„åˆ† Ã— æˆäº¤é‡å¼‚å¸¸',
        'score_volatility_signal': 'æ–°é—»è¯„åˆ† Ã— æ³¢åŠ¨ç‡',
        'score_gap_signal': 'æ–°é—»è¯„åˆ† Ã— éš”å¤œè·³ç©º'
    }

    print("1. ç»„åˆæŒ‡æ ‡é¢„æµ‹æ•ˆæœæ¯”è¾ƒ:")

    combination_results = []

    for indicator, name in combined_indicators.items():
        if indicator in merged_data.columns:
            valid_data = merged_data.dropna(subset=[indicator, target_col])

            if len(valid_data) >= 3:
                ic_metrics = calculate_ic_metrics(valid_data[indicator], valid_data[target_col])
                corr_val, p_val = pearsonr(valid_data[indicator], valid_data[target_col])

                combination_results.append({
                    'name': name,
                    'correlation': corr_val,
                    'rank_ic': ic_metrics['rank_ic'],
                    'sample_size': len(valid_data)
                })

                significance = "**æ˜¾è‘—**" if p_val < 0.05 else "ä¸æ˜¾è‘—"

                print(f"   {name}:")
                print(f"     ç›¸å…³ç³»æ•°: {corr_val:.4f} ({significance})")
                print(f"     Rank IC: {ic_metrics['rank_ic']:.4f}")
                print(f"     æ ·æœ¬æ•°: {len(valid_data)}")
                print()

    # æ’åºæ˜¾ç¤ºæœ€ä½³ç»„åˆ
    if combination_results:
        combination_results.sort(key=lambda x: abs(x['rank_ic']), reverse=True)

        print("2. æœ€ä½³ç»„åˆæŒ‡æ ‡æ’åº (æŒ‰Rank IC):")
        for i, result in enumerate(combination_results, 1):
            print(f"   {i}. {result['name']}: Rank IC = {result['rank_ic']:.4f}")

    return combination_results

def create_visualization(merged_data):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 7: åˆ›å»ºå¯è§†åŒ–å›¾è¡¨")
    print("=" * 60)

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('News Sentiment vs Opening Strength Analysis', fontsize=16, fontweight='bold')

    score_col = 'overall_score_mean'

    # å›¾1: æ–°é—»è¯„åˆ† vs æœªæ¥1å¤©å¼€ç›˜å¼ºåº¦
    ax1 = axes[0, 0]
    if 'future_open_strength_1d' in merged_data.columns:
        valid_data = merged_data.dropna(subset=[score_col, 'future_open_strength_1d'])
        if len(valid_data) > 0:
            ax1.scatter(valid_data[score_col], valid_data['future_open_strength_1d'] * 100,
                       alpha=0.6, s=30, color='blue', edgecolors='navy', linewidth=0.5)

            # æ·»åŠ è¶‹åŠ¿çº¿
            if len(valid_data) > 2:
                z = np.polyfit(valid_data[score_col], valid_data['future_open_strength_1d'], 1)
                p = np.poly1d(z)
                ax1.plot(valid_data[score_col], p(valid_data[score_col]) * 100,
                        "r--", alpha=0.8, linewidth=2)

            corr, p_val = pearsonr(valid_data[score_col], valid_data['future_open_strength_1d'])
            ax1.set_title(f'News Score vs Future 1-Day Opening Strength\nr = {corr:.4f}, p = {p_val:.4f}',
                         fontsize=10, fontweight='bold')

    ax1.set_xlabel('News Overall Score')
    ax1.set_ylabel('Future 1-Day Opening Strength (%)')
    ax1.grid(True, alpha=0.3)
    ax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)
    ax1.axvline(x=0, color='k', linestyle='-', alpha=0.3)

    # å›¾2: æ—¶é—´è¡°å‡æ•ˆåº”
    ax2 = axes[0, 1]
    periods = ['1d', '3d', '5d']
    correlations = []

    for period in periods:
        col = f'future_open_strength_{period}'
        if col in merged_data.columns:
            valid_data = merged_data.dropna(subset=[score_col, col])
            if len(valid_data) > 2:
                corr, _ = pearsonr(valid_data[score_col], valid_data[col])
                correlations.append(abs(corr))
            else:
                correlations.append(0)
        else:
            correlations.append(0)

    if correlations:
        ax2.plot(periods, correlations, 'o-', linewidth=2, markersize=8, color='green')
        ax2.set_title('Time Decay Effect', fontsize=10, fontweight='bold')
        ax2.set_xlabel('Prediction Period')
        ax2.set_ylabel('Absolute Correlation')
        ax2.grid(True, alpha=0.3)

    # å›¾3: ä¸åŒæƒ…æ„Ÿå¼ºåº¦çš„å¼€ç›˜å¼ºåº¦åˆ†å¸ƒ
    ax3 = axes[0, 2]
    if 'future_open_strength_1d' in merged_data.columns:
        valid_data = merged_data.dropna(subset=[score_col, 'future_open_strength_1d'])

        # åˆ›å»ºæƒ…æ„Ÿåˆ†ç»„
        valid_data['sentiment_group'] = pd.cut(valid_data[score_col],
                                              bins=[-np.inf, -1, 0, 1, np.inf],
                                              labels=['Negative', 'Slightly Negative', 'Slightly Positive', 'Positive'])

        groups = []
        labels = []
        for group in ['Negative', 'Slightly Negative', 'Slightly Positive', 'Positive']:
            group_data = valid_data[valid_data['sentiment_group'] == group]['future_open_strength_1d'] * 100
            if len(group_data) > 0:
                groups.append(group_data)
                labels.append(f'{group}\n(n={len(group_data)})')

        if groups:
            ax3.boxplot(groups, labels=labels)
            ax3.set_title('Opening Strength by Sentiment Intensity', fontsize=10, fontweight='bold')
            ax3.set_ylabel('Future 1-Day Opening Strength (%)')
            ax3.grid(True, alpha=0.3)
            ax3.axhline(y=0, color='r', linestyle='--', alpha=0.5)

    # å›¾4: å¼€ç›˜å¼ºåº¦æ—¶é—´åºåˆ—
    ax4 = axes[1, 0]
    if 'date' in merged_data.columns and 'open_strength' in merged_data.columns:
        # é€‰æ‹©æœ€è¿‘30å¤©çš„æ•°æ®
        recent_data = merged_data.tail(min(30, len(merged_data)))

        ax4.plot(recent_data['date'], recent_data['open_strength'] * 100,
                'b-o', linewidth=1.5, markersize=4, alpha=0.7, label='Opening Strength')

        if score_col in recent_data.columns:
            ax4_twin = ax4.twinx()
            ax4_twin.plot(recent_data['date'], recent_data[score_col],
                         'r-s', linewidth=1.5, markersize=3, alpha=0.7, label='News Score')
            ax4_twin.set_ylabel('News Score', color='red')
            ax4_twin.tick_params(axis='y', labelcolor='red')

        ax4.set_title('Opening Strength vs News Score Time Series', fontsize=10, fontweight='bold')
        ax4.set_xlabel('Date')
        ax4.set_ylabel('Opening Strength (%)', color='blue')
        ax4.tick_params(axis='y', labelcolor='blue')
        ax4.tick_params(axis='x', rotation=45)
        ax4.grid(True, alpha=0.3)

    # å›¾5: ç»„åˆæŒ‡æ ‡æ•ˆæœæ¯”è¾ƒ
    ax5 = axes[1, 1]

    # è®¡ç®—ä¸åŒæŒ‡æ ‡çš„IC
    if 'future_open_strength_1d' in merged_data.columns:
        indicators = {
            'overall_score_mean': 'News Score',
            'score_volume_signal': 'Score Ã— Volume',
            'score_volatility_signal': 'Score Ã— Volatility'
        }

        ic_values = []
        indicator_names = []

        for indicator, name in indicators.items():
            if indicator in merged_data.columns:
                valid_data = merged_data.dropna(subset=[indicator, 'future_open_strength_1d'])
                if len(valid_data) > 2:
                    ic_metrics = calculate_ic_metrics(valid_data[indicator],
                                                     valid_data['future_open_strength_1d'])
                    ic_values.append(abs(ic_metrics['rank_ic']))
                    indicator_names.append(name)

        if ic_values:
            bars = ax5.bar(indicator_names, ic_values, alpha=0.7,
                          color=['blue', 'green', 'orange'][:len(ic_values)])
            ax5.set_title('Combined Indicators Prediction Comparison', fontsize=10, fontweight='bold')
            ax5.set_ylabel('Absolute Rank IC')
            ax5.tick_params(axis='x', rotation=45)
            ax5.grid(True, alpha=0.3)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, ic_values):
                ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                        f'{value:.3f}', ha='center', va='bottom', fontsize=8)

    # å›¾6: å¼€ç›˜å¼ºåº¦åˆ†å¸ƒç›´æ–¹å›¾
    ax6 = axes[1, 2]
    if 'open_strength' in merged_data.columns:
        open_strength_pct = merged_data['open_strength'].dropna() * 100
        ax6.hist(open_strength_pct, bins=30, alpha=0.7, color='purple', edgecolor='black')
        ax6.axvline(x=open_strength_pct.mean(), color='red', linestyle='--',
                   label=f'Mean: {open_strength_pct.mean():.2f}%')
        ax6.set_title('Opening Strength Distribution', fontsize=10, fontweight='bold')
        ax6.set_xlabel('Opening Strength (%)')
        ax6.set_ylabel('Frequency')
        ax6.legend()
        ax6.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def generate_trading_strategy(merged_data):
    """ç”ŸæˆåŸºäºå¼€ç›˜å¼ºåº¦çš„äº¤æ˜“ç­–ç•¥"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 8: äº¤æ˜“ç­–ç•¥ç”Ÿæˆ")
    print("=" * 60)

    score_col = 'overall_score_mean'

    if score_col not in merged_data.columns:
        print("âš ï¸ ç¼ºå°‘æ–°é—»è¯„åˆ†åˆ—ï¼Œæ— æ³•ç”Ÿæˆç­–ç•¥")
        return

    strategy_data = merged_data.copy()

    # ç”Ÿæˆäº¤æ˜“ä¿¡å·
    print("âœ“ ç”Ÿæˆäº¤æ˜“ä¿¡å·...")

    # ä¿¡å·1ï¼šåŸºäºæ–°é—»è¯„åˆ†çš„ç®€å•ä¿¡å·
    strategy_data['signal_simple'] = np.where(strategy_data[score_col] > 1, 1,
                                             np.where(strategy_data[score_col] < -1, -1, 0))

    # ä¿¡å·2ï¼šåŸºäºç»„åˆæŒ‡æ ‡çš„å¤åˆä¿¡å·
    if 'score_volume_signal' in strategy_data.columns:
        strategy_data['signal_combined'] = np.where(
            (strategy_data[score_col] > 0.5) & (strategy_data['turnover_ratio'] > 1.2), 1,
            np.where(
                (strategy_data[score_col] < -0.5) & (strategy_data['turnover_ratio'] > 1.2), -1, 0
            )
        )
    else:
        strategy_data['signal_combined'] = strategy_data['signal_simple']

    # è®¡ç®—ç­–ç•¥æ”¶ç›Šï¼ˆä½¿ç”¨å¼€ç›˜å¼ºåº¦ä½œä¸ºç›®æ ‡ï¼‰
    if 'future_open_strength_1d' in strategy_data.columns:
        strategy_data['strategy_return_simple'] = (strategy_data['signal_simple'] *
                                                  strategy_data['future_open_strength_1d'])
        strategy_data['strategy_return_combined'] = (strategy_data['signal_combined'] *
                                                    strategy_data['future_open_strength_1d'])

        # è®¡ç®—ç´¯ç§¯æ”¶ç›Š
        strategy_data['cumulative_return_simple'] = strategy_data['strategy_return_simple'].cumsum()
        strategy_data['cumulative_return_combined'] = strategy_data['strategy_return_combined'].cumsum()

        # ç­–ç•¥è¯„ä¼°
        print("1. ç­–ç•¥è¡¨ç°è¯„ä¼°:")

        simple_returns = strategy_data['strategy_return_simple'].dropna()
        combined_returns = strategy_data['strategy_return_combined'].dropna()

        if len(simple_returns) > 0:
            print(f"   ç®€å•ç­–ç•¥:")
            print(f"     å¹³å‡æ”¶ç›Š: {simple_returns.mean():.4f}")
            print(f"     æ”¶ç›Šæ³¢åŠ¨: {simple_returns.std():.4f}")
            print(f"     å¤æ™®æ¯”ç‡: {simple_returns.mean() / (simple_returns.std() + 1e-8):.4f}")
            print(f"     èƒœç‡: {(simple_returns > 0).mean():.2%}")
            print()

        if len(combined_returns) > 0:
            print(f"   ç»„åˆç­–ç•¥:")
            print(f"     å¹³å‡æ”¶ç›Š: {combined_returns.mean():.4f}")
            print(f"     æ”¶ç›Šæ³¢åŠ¨: {combined_returns.std():.4f}")
            print(f"     å¤æ™®æ¯”ç‡: {combined_returns.mean() / (combined_returns.std() + 1e-8):.4f}")
            print(f"     èƒœç‡: {(combined_returns > 0).mean():.2%}")

    print("âœ“ ç­–ç•¥å›æµ‹å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ–°é—»è¯„åˆ†ä¸å¼€ç›˜å¼ºåº¦æ·±åº¦åˆ†æ")
    print("=" * 80)
    print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    # æ–‡ä»¶è·¯å¾„é…ç½®
    news_data_path = r'E:\projects\myQ\scripts\news_scores_result_1y_zijin.csv'

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(news_data_path):
        print(f"âœ— é”™è¯¯: æ‰¾ä¸åˆ°æ–°é—»æ•°æ®æ–‡ä»¶ {news_data_path}")
        return

    try:
        # æ­¥éª¤1: åŠ è½½å’Œå‡†å¤‡æ•°æ®
        merged_data = load_and_prepare_data(news_data_path)

        if len(merged_data) < 10:
            print("âœ— é”™è¯¯: æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            return

        # æ­¥éª¤2: ICç³»æ•°åˆ†æ
        ic_results = analyze_opening_strength_ic(merged_data)

        # æ­¥éª¤3: æƒ…æ„Ÿé˜ˆå€¼åˆ†æ
        threshold_results = analyze_sentiment_thresholds(merged_data)

        # æ­¥éª¤4: æ—¶é—´è¡°å‡åˆ†æ
        decay_results = analyze_time_decay(merged_data)

        # æ­¥éª¤5: ç»„åˆæŒ‡æ ‡åˆ†æ
        combination_results = analyze_combined_indicators(merged_data)

        # æ­¥éª¤6: å¯è§†åŒ–
        create_visualization(merged_data)

        # æ­¥éª¤7: äº¤æ˜“ç­–ç•¥
        generate_trading_strategy(merged_data)

        print("\n" + "=" * 80)
        print("ğŸ‰ åˆ†æå®Œæˆï¼ä¸»è¦å‘ç°ï¼š")
        print("=" * 80)

        # è¾“å‡ºå…³é”®å‘ç°
        if ic_results:
            best_indicator = max(ic_results.items(),
                               key=lambda x: abs(x[1].get('rank_ic', 0)))
            print(f"â€¢ æœ€å¼ºé¢„æµ‹æŒ‡æ ‡: {best_indicator[0]}")
            print(f"  Rank IC: {best_indicator[1]['rank_ic']:.4f}")

        if combination_results:
            best_combination = max(combination_results,
                                 key=lambda x: abs(x.get('rank_ic', 0)))
            print(f"â€¢ æœ€ä½³ç»„åˆç­–ç•¥: {best_combination['name']}")
            print(f"  Rank IC: {best_combination['rank_ic']:.4f}")

        print("â€¢ å¼€ç›˜å¼ºåº¦ç›¸æ¯”ç®€å•æ”¶ç›Šç‡æ›´èƒ½åæ˜ æ–°é—»å½±å“")
        print("â€¢ å»ºè®®é‡ç‚¹å…³æ³¨å¼€ç›˜å15åˆ†é’Ÿçš„ä»·æ ¼è¡Œä¸º")
        print("â€¢ ç»“åˆæˆäº¤é‡å¼‚å¸¸å¯ä»¥æå‡é¢„æµ‹æ•ˆæœ")

    except Exception as e:
        print(f"âœ— åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()