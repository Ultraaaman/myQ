#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥æ–°é—»åˆ†æè„šæœ¬ï¼ˆæ­£å¼ç‰ˆv2 - æ‰¹é‡LLMå¤„ç† + é‡è¯•æœºåˆ¶ + é‡å¤æ£€æµ‹ï¼‰

v2.1æ›´æ–°å†…å®¹ï¼š
- âœ… æ·»åŠ é‡å¤æ–°é—»æ£€æµ‹åŠŸèƒ½ï¼Œé¿å…é‡å¤APIè°ƒç”¨
- âœ… æ™ºèƒ½å¯¹æ¯”å·²å­˜åœ¨çš„åˆ†æç»“æœæ–‡ä»¶
- âœ… åŸºäº"æ ‡é¢˜+è‚¡ç¥¨ä»£ç +æ—¶é—´"çš„é‡å¤åˆ¤æ–­é€»è¾‘
- âœ… è‡ªåŠ¨åˆå¹¶æ–°æ—§åˆ†æç»“æœ
- âœ… æ˜¾ç¤ºé¢„è®¡èŠ‚çœçš„APIè´¹ç”¨
- âœ… æ”¯æŒå¤šæ¬¡è¿è¡ŒåŒä¸€å¤©çš„åˆ†æè€Œä¸æµªè´¹è´¹ç”¨

ä½¿ç”¨åœºæ™¯ï¼š
1. æ—¥å†…å¤šæ¬¡è¿è¡Œåˆ†æï¼šåªåˆ†ææ–°å¢æ–°é—»ï¼Œè·³è¿‡å·²åˆ†æçš„
2. æ•°æ®è¡¥å……åˆ†æï¼šè‡ªåŠ¨åˆå¹¶å†å²ç»“æœå’Œæ–°ç»“æœ
3. æˆæœ¬æ§åˆ¶ï¼šé¿å…é‡å¤è°ƒç”¨æ˜‚è´µçš„LLM API

ä¿®æ”¹è®°å½•ï¼š
- 2025-09-25: æ·»åŠ é‡å¤æ£€æµ‹å’Œè´¹ç”¨èŠ‚çœåŠŸèƒ½
"""

import tushare as ts
import pandas as pd
import json
import requests
import os
import time
import random
from datetime import datetime, timedelta
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DailyNewsAnalyzer:
    def __init__(self, tushare_token, openrouter_api_key):
        """åˆå§‹åŒ–æ–°é—»åˆ†æå™¨"""
        print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–åˆ†æå™¨...")

        # åˆå§‹åŒ–API
        print(f"ğŸ”§ åˆå§‹åŒ–Tushare API...")
        self.ts_pro = ts.pro_api(tushare_token)
        print(f"âœ… Tushare APIåˆå§‹åŒ–å®Œæˆ")

        self.api_key = openrouter_api_key
        self.api_url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {openrouter_api_key}",
            "Content-Type": "application/json",
        }

        # è®¾ç½®æ–‡ä»¶è·¯å¾„
        print(f"ğŸ”§ è®¾ç½®æ–‡ä»¶è·¯å¾„...")
        self.base_dir = Path("D:/projects/q/myQ")
        self.output_dir = self.base_dir / "output" / "daily_analysis"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        print(f"âœ… è¾“å‡ºç›®å½•: {self.output_dir}")

        # åŠ è½½è‚¡ç¥¨æ± 
        print(f"ğŸ”§ åŠ è½½è‚¡ç¥¨æ± ...")
        self.stock_pool = self._load_stock_pool()
        print(f"âœ… è‚¡ç¥¨æ± åŠ è½½å®Œæˆ: {len(self.stock_pool)} åªè‚¡ç¥¨")

        print(f"ğŸ”§ æå–å…³é”®è¯...")
        self.stock_keywords = self._extract_keywords()
        print(f"âœ… å…³é”®è¯æå–å®Œæˆ: {len(self.stock_keywords)} ä¸ªè‚¡ç¥¨")

    def _load_stock_pool(self):
        """åŠ è½½è‚¡ç¥¨æ± """
        stock_pool_path = self.base_dir / "config" / "stock_pool.json"
        print(f"ğŸ“ è¯»å–è‚¡ç¥¨æ± æ–‡ä»¶: {stock_pool_path}")

        with open(stock_pool_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"ğŸ“Š åŸå§‹JSONæ•°æ®é”®: {list(data.keys())}")

        # æ”¯æŒä¸åŒçš„JSONæ ¼å¼
        if 'stocks' in data:
            stocks = data['stocks']
            print(f"âœ… ä½¿ç”¨ 'stocks' é”®")
        elif 'stock_database' in data:
            stocks = data['stock_database']
            print(f"âœ… ä½¿ç”¨ 'stock_database' é”®")
        else:
            stocks = data if isinstance(data, list) else []
            print(f"âœ… æ•°æ®æœ¬èº«å°±æ˜¯åˆ—è¡¨")

        print(f"ğŸ“Š æœ€ç»ˆè‚¡ç¥¨æ•°é‡: {len(stocks)}")
        return stocks

    def _extract_keywords(self):
        """æå–å…³é”®è¯"""
        keywords = {}

        for i, stock in enumerate(self.stock_pool):
            stock_keywords = []
            stock_keywords.append(stock['stock_name'])
            stock_keywords.append(stock['stock_code'])

            # # æå–è¡Œä¸šå’Œä¸»è¥ä¸šåŠ¡å…³é”®è¯
            # if 'industry' in stock:
            #     stock_keywords.extend(stock['industry'].split())
            # if 'main_business' in stock:
            #     business_words = stock['main_business'].replace('ï¼Œ', ' ').replace('ã€', ' ').split()
            #     business_words = [w for w in business_words if len(w) >= 2][:5]
            #     stock_keywords.extend(business_words)

            # å»é‡
            keywords[stock['stock_code']] = list(set(stock_keywords))

            # æ˜¾ç¤ºå‰3ä¸ªè‚¡ç¥¨çš„å…³é”®è¯ç”¨äºè°ƒè¯•
            if i < 3:
                print(f"  {stock['stock_name']} ({stock['stock_code']}): {keywords[stock['stock_code']]}")

        return keywords

    def get_daily_news(self, target_date=None):
        """è·å–å½“æ—¥æ–°é—»"""
        if target_date is None:
            start_date = datetime.now().strftime('%Y-%m-%d 09:00:00')
            end_date = datetime.now().strftime('%Y-%m-%d 18:00:00')
        else:
            start_date = f"{target_date} 09:00:00"
            end_date = f"{target_date} 18:00:00"

        print(f"ğŸ“° å‡†å¤‡è·å–æ–°é—»æ•°æ®...")
        print(f"   - å¼€å§‹æ—¶é—´: {start_date}")
        print(f"   - ç»“æŸæ—¶é—´: {end_date}")

        news_df = self.ts_pro.news(
            src='sina',
            start_date=start_date,
            end_date=end_date
        )

        print(f"ğŸ“Š APIè¿”å›ç»“æœ:")
        print(f"   - æ•°æ®ç±»å‹: {type(news_df)}")
        print(f"   - æ˜¯å¦ä¸ºç©º: {news_df.empty if hasattr(news_df, 'empty') else 'N/A'}")
        print(f"   - æ•°æ®å½¢çŠ¶: {news_df.shape if hasattr(news_df, 'shape') else 'N/A'}")

        if hasattr(news_df, 'columns'):
            print(f"   - åˆ—å: {list(news_df.columns)}")

        return news_df

    def match_news_to_stocks(self, news_df):
        """åŒ¹é…æ–°é—»åˆ°è‚¡ç¥¨"""
        if news_df.empty:
            print("âš ï¸ æ–°é—»æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åŒ¹é…")
            return news_df

        print(f"ğŸ” å¼€å§‹åŒ¹é…æ–°é—»åˆ°è‚¡ç¥¨...")
        matched_records = []

        for i, (_, news_row) in enumerate(news_df.iterrows()):
            if i < 2:  # æ˜¾ç¤ºå‰2æ¡æ–°é—»çš„è°ƒè¯•ä¿¡æ¯
                title = news_row.get('title') or news_row.get('Title') or ''
                print(f"  å¤„ç†ç¬¬ {i+1} æ¡æ–°é—»: {str(title)[:50] if title else '(æ— æ ‡é¢˜)'}...")

            # å®‰å…¨è·å–å­—æ®µå€¼
            title = news_row.get('title') or news_row.get('Title') or ''
            content = news_row.get('content') or news_row.get('Content') or ''

            news_title = str(title).lower() if title else ''
            news_content = str(content).lower() if content else ''
            full_text = f"{news_title} {news_content}"

            # åŒ¹é…è‚¡ç¥¨
            for stock in self.stock_pool:
                stock_code = stock['stock_code']
                keywords = self.stock_keywords.get(stock_code, [])

                # æ£€æŸ¥æ˜¯å¦åŒ¹é…
                matched = False
                matched_keyword = None
                for keyword in keywords:
                    if len(keyword) >= 2 and keyword.lower() in full_text:
                        matched = True
                        matched_keyword = keyword
                        break

                if matched:
                    record = news_row.to_dict()
                    record.update(stock)
                    matched_records.append(record)

        matched_df = pd.DataFrame(matched_records)
        print(f"ğŸ¯ åŒ¹é…å®Œæˆï¼Œæ‰¾åˆ° {len(matched_df)} æ¡è‚¡ç¥¨ç›¸å…³æ–°é—»")

        return matched_df

    def _load_existing_results(self, target_date):
        """åŠ è½½å·²å­˜åœ¨çš„åˆ†æç»“æœ"""
        detail_filename = self.output_dir / f"news_analysis_detail_{target_date.replace('-', '')}.csv"

        if detail_filename.exists():
            try:
                existing_df = pd.read_csv(detail_filename, encoding='utf-8-sig')
                print(f"ğŸ“ å‘ç°å·²æœ‰åˆ†æç»“æœæ–‡ä»¶: {len(existing_df)} æ¡è®°å½•")
                return existing_df
            except Exception as e:
                print(f"âš ï¸ è¯»å–å·²æœ‰ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
                return pd.DataFrame()
        else:
            print(f"ğŸ“ æœªå‘ç°å·²æœ‰åˆ†æç»“æœæ–‡ä»¶")
            return pd.DataFrame()

    def _detect_duplicates(self, matched_df, existing_df):
        """æ£€æµ‹å¹¶è¿‡æ»¤é‡å¤æ–°é—»"""
        if existing_df.empty:
            print(f"âœ“ æ— å†å²æ•°æ®ï¼Œæ‰€æœ‰ {len(matched_df)} æ¡æ–°é—»éƒ½å°†è¿›è¡Œåˆ†æ")
            return matched_df, pd.DataFrame()

        print(f"ğŸ” å¼€å§‹æ£€æµ‹é‡å¤æ–°é—»...")

        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ•°æ®ç»“æ„
        print(f"ğŸ” æ–°æ•°æ®åˆ—å: {list(matched_df.columns)}")
        print(f"ğŸ” å·²æœ‰æ•°æ®åˆ—å: {list(existing_df.columns)}")

        # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬æ•°æ®
        if len(matched_df) > 0:
            first_row = matched_df.iloc[0]
            print(f"ğŸ” æ–°æ•°æ®æ ·æœ¬:")
            print(f"   - title: '{str(first_row.get('title', 'N/A'))}'")
            print(f"   - stock_code: '{str(first_row.get('stock_code', 'N/A'))}'")
            print(f"   - datetime: '{str(first_row.get('datetime', 'N/A'))}'")

        if len(existing_df) > 0:
            first_existing = existing_df.iloc[0]
            print(f"ğŸ” å·²æœ‰æ•°æ®æ ·æœ¬:")
            for col in ['title', 'original_title', 'stock_code', 'datetime', 'original_date']:
                if col in existing_df.columns:
                    print(f"   - {col}: '{str(first_existing.get(col, 'N/A'))}'")

        # æ™ºèƒ½ç¡®å®šå­—æ®µæ˜ å°„ï¼Œç”±äºtitleä¸ºç©ºï¼Œæ”¹ç”¨content
        # æ£€æŸ¥æ–°æ•°æ®çš„å­—æ®µ
        new_content_col = None
        for col in ['content', 'title']:
            if col in matched_df.columns:
                # æ£€æŸ¥è¯¥å­—æ®µæ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹
                sample_values = matched_df[col].dropna().astype(str)
                valid_values = sample_values[sample_values.str.len() > 5]  # é•¿åº¦å¤§äº5çš„æ‰ç®—æœ‰æ•ˆ
                if len(valid_values) > 0:
                    new_content_col = col
                    break

        new_date_col = 'datetime' if 'datetime' in matched_df.columns else None
        new_code_col = 'stock_code' if 'stock_code' in matched_df.columns else None

        # æ£€æŸ¥å·²æœ‰æ•°æ®çš„å­—æ®µ
        existing_content_col = None
        for col in ['original_title', 'title', 'content']:
            if col in existing_df.columns:
                # æ£€æŸ¥è¯¥å­—æ®µæ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹
                sample_values = existing_df[col].dropna().astype(str)
                valid_values = sample_values[sample_values.str.len() > 5]
                if len(valid_values) > 0:
                    existing_content_col = col
                    break

        existing_date_col = None
        for col in ['original_date', 'datetime']:
            if col in existing_df.columns:
                existing_date_col = col
                break

        existing_code_col = 'stock_code' if 'stock_code' in existing_df.columns else None

        print(f"ğŸ” å­—æ®µæ˜ å°„ (ä½¿ç”¨contentæ›¿ä»£ç©ºtitle):")
        print(f"   - æ–°æ•°æ®: content='{new_content_col}', date='{new_date_col}', code='{new_code_col}'")
        print(f"   - å·²æœ‰æ•°æ®: content='{existing_content_col}', date='{existing_date_col}', code='{existing_code_col}'")

        # æ£€æŸ¥å¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨
        if not all([new_content_col, new_date_col, new_code_col, existing_content_col, existing_date_col, existing_code_col]):
            print(f"âš ï¸ ç¼ºå°‘å¿…éœ€å­—æ®µï¼Œè·³è¿‡é‡å¤æ£€æµ‹ï¼Œå…¨éƒ¨æ ‡è®°ä¸ºæ–°æ–°é—»")
            print(f"   ç¼ºå¤±å­—æ®µ: new_content={new_content_col}, new_date={new_date_col}, new_code={new_code_col}")
            print(f"            existing_content={existing_content_col}, existing_date={existing_date_col}, existing_code={existing_code_col}")
            return matched_df, pd.DataFrame()

        # åˆ›å»ºåŸºäºå†…å®¹çš„é‡å¤æ£€æµ‹é€»è¾‘
        # 1. æ¸…ç†å’Œæ ‡å‡†åŒ–å†…å®¹ï¼Œæå–æ ¸å¿ƒå†…å®¹ä½œä¸ºæŒ‡çº¹
        def create_content_fingerprint(content_series):
            # æ›´å½»åº•çš„æ¸…ç†ï¼Œå»é™¤æ ‡ç‚¹ã€æ—¶é—´æˆ³ç­‰å˜åŒ–å…ƒç´ 
            cleaned = content_series.astype(str).str.strip()
            # å»é™¤å¸¸è§çš„å˜åŒ–å…ƒç´ ï¼šæ¢è¡Œã€åˆ¶è¡¨ç¬¦ã€å¤šä½™ç©ºæ ¼
            cleaned = cleaned.str.replace('\n', ' ').str.replace('\r', '').str.replace('\t', ' ')
            # å»é™¤æ—¶é—´ç›¸å…³çš„æ•°å­—æ¨¡å¼ï¼ˆå¯èƒ½åœ¨ä¸åŒå‘å¸ƒæ—¶é—´æœ‰å·®å¼‚ï¼‰
            import re
            # å»é™¤æ—¶é—´æ ¼å¼ (å¦‚: 2024-01-01, 01:23, 12:34:56ç­‰)
            cleaned = cleaned.str.replace(r'\d{4}-\d{2}-\d{2}', '', regex=True)
            cleaned = cleaned.str.replace(r'\d{1,2}:\d{2}(:\d{2})?', '', regex=True)
            # æ ‡å‡†åŒ–å¤šä¸ªç©ºæ ¼
            cleaned = cleaned.str.replace(r'\s+', ' ', regex=True)
            # å–å‰150ä¸ªå­—ç¬¦ä½œä¸ºæ›´é•¿çš„å†…å®¹æŒ‡çº¹ï¼Œå¢åŠ å‡†ç¡®æ€§
            fingerprint = cleaned.str[:150]
            return fingerprint

        # 2. æå–æ—¥æœŸéƒ¨åˆ†ï¼ˆå¿½ç•¥æ—¶åˆ†ç§’ï¼‰
        def extract_date_part(datetime_series):
            return pd.to_datetime(datetime_series.astype(str), errors='coerce').dt.strftime('%Y-%m-%d')

        # å¤„ç†æ–°æ•°æ®
        matched_df['content_fingerprint'] = create_content_fingerprint(matched_df[new_content_col])
        matched_df['date_part'] = extract_date_part(matched_df[new_date_col])

        # å¤„ç†å·²æœ‰æ•°æ®
        existing_df['content_fingerprint'] = create_content_fingerprint(existing_df[existing_content_col])
        existing_df['date_part'] = extract_date_part(existing_df[existing_date_col])

        # å¤šé‡æ£€æµ‹ç­–ç•¥ (åŸºäºå†…å®¹æŒ‡çº¹)
        print(f"ğŸ” ä½¿ç”¨åŸºäºå†…å®¹çš„å¤šé‡æ£€æµ‹ç­–ç•¥:")

        # ç­–ç•¥1: ç²¾ç¡®åŒ¹é… (å†…å®¹æŒ‡çº¹+è‚¡ç¥¨+å®Œæ•´æ—¶é—´)
        matched_df['key_exact'] = (
            matched_df['content_fingerprint'] + '|' +
            matched_df[new_code_col].astype(str) + '|' +
            matched_df[new_date_col].astype(str)
        )

        existing_df['key_exact'] = (
            existing_df['content_fingerprint'] + '|' +
            existing_df[existing_code_col].astype(str) + '|' +
            existing_df[existing_date_col].astype(str)
        )

        # ç­–ç•¥2: å®½æ¾åŒ¹é… (å†…å®¹æŒ‡çº¹+è‚¡ç¥¨+æ—¥æœŸ)
        matched_df['key_loose'] = (
            matched_df['content_fingerprint'] + '|' +
            matched_df[new_code_col].astype(str) + '|' +
            matched_df['date_part']
        )

        existing_df['key_loose'] = (
            existing_df['content_fingerprint'] + '|' +
            existing_df[existing_code_col].astype(str) + '|' +
            existing_df['date_part']
        )

        # ç­–ç•¥3: å†…å®¹ç›¸ä¼¼åº¦ (ä»…å†…å®¹æŒ‡çº¹+è‚¡ç¥¨ï¼Œå¿½ç•¥æ—¶é—´å·®å¼‚)
        matched_df['key_content'] = (
            matched_df['content_fingerprint'] + '|' +
            matched_df[new_code_col].astype(str)
        )

        existing_df['key_content'] = (
            existing_df['content_fingerprint'] + '|' +
            existing_df[existing_code_col].astype(str)
        )

        # ç­–ç•¥4: å®½æ¾å†…å®¹åŒ¹é… (ä»…è‚¡ç¥¨ä»£ç ï¼Œç”¨äºæ£€æµ‹å†…å®¹é«˜åº¦ç›¸ä¼¼çš„æ–°é—»)
        # ä¸ºåŒä¸€è‚¡ç¥¨åˆ›å»ºç®€åŒ–çš„å†…å®¹æŒ‡çº¹ï¼ˆå‰80å­—ç¬¦ï¼‰
        def create_loose_fingerprint(content_series):
            return content_series.str[:80]  # æ›´çŸ­çš„æŒ‡çº¹ï¼Œæ•è·æ›´å¤šç›¸ä¼¼å†…å®¹

        matched_df['loose_fingerprint'] = create_loose_fingerprint(matched_df['content_fingerprint'])
        existing_df['loose_fingerprint'] = create_loose_fingerprint(existing_df['content_fingerprint'])

        matched_df['key_loose_content'] = (
            matched_df['loose_fingerprint'] + '|' +
            matched_df[new_code_col].astype(str)
        )

        existing_df['key_loose_content'] = (
            existing_df['loose_fingerprint'] + '|' +
            existing_df[existing_code_col].astype(str)
        )

        # æœ€ç»ˆçš„é‡å¤æ£€æµ‹é”®ï¼ˆä¼˜å…ˆä½¿ç”¨ç²¾ç¡®åŒ¹é…ï¼‰
        matched_df['duplicate_key'] = matched_df['key_exact']

        # å¤šç­–ç•¥é‡å¤æ£€æµ‹
        existing_keys_exact = set(existing_df['key_exact'].tolist())
        existing_keys_loose = set(existing_df['key_loose'].tolist())
        existing_keys_content = set(existing_df['key_content'].tolist())
        existing_keys_loose_content = set(existing_df['key_loose_content'].tolist())

        # åˆå§‹åŒ–é‡å¤æ ‡è®°
        matched_df['is_duplicate'] = False

        # ç­–ç•¥1: ç²¾ç¡®åŒ¹é…
        exact_matches = matched_df['key_exact'].isin(existing_keys_exact)
        matched_df.loc[exact_matches, 'is_duplicate'] = True
        exact_count = exact_matches.sum()

        # ç­–ç•¥2: å®½æ¾åŒ¹é…ï¼ˆå¯¹äºæ—¶é—´å¯èƒ½æœ‰å·®å¼‚çš„æƒ…å†µï¼‰
        loose_matches = matched_df['key_loose'].isin(existing_keys_loose) & ~matched_df['is_duplicate']
        matched_df.loc[loose_matches, 'is_duplicate'] = True
        loose_count = loose_matches.sum()

        # ç­–ç•¥3: å†…å®¹åŒ¹é…ï¼ˆå¯¹äºå¯èƒ½æœ‰å…¶ä»–ç»†å¾®å·®å¼‚çš„æƒ…å†µï¼‰
        content_matches = matched_df['key_content'].isin(existing_keys_content) & ~matched_df['is_duplicate']
        matched_df.loc[content_matches, 'is_duplicate'] = True
        content_count = content_matches.sum()

        # ç­–ç•¥4: å®½æ¾å†…å®¹åŒ¹é…ï¼ˆæœ€ç§¯æçš„ç­–ç•¥ï¼Œç”¨äºæ•è·é«˜åº¦ç›¸ä¼¼çš„å†…å®¹ï¼‰
        loose_content_matches = matched_df['key_loose_content'].isin(existing_keys_loose_content) & ~matched_df['is_duplicate']
        matched_df.loc[loose_content_matches, 'is_duplicate'] = True
        loose_content_count = loose_content_matches.sum()

        print(f"ğŸ” å¤šç­–ç•¥æ£€æµ‹ç»“æœ:")
        print(f"   - ç²¾ç¡®åŒ¹é… (å†…å®¹+è‚¡ç¥¨+æ—¶é—´): {exact_count} æ¡")
        print(f"   - å®½æ¾åŒ¹é… (å†…å®¹+è‚¡ç¥¨+æ—¥æœŸ): {loose_count} æ¡")
        print(f"   - å†…å®¹åŒ¹é… (å®Œæ•´å†…å®¹+è‚¡ç¥¨): {content_count} æ¡")
        print(f"   - å®½æ¾å†…å®¹åŒ¹é… (ç®€åŒ–å†…å®¹+è‚¡ç¥¨): {loose_content_count} æ¡")

        # æ˜¾ç¤ºæ ·æœ¬æ•°æ®ç”¨äºè°ƒè¯•
        print(f"ğŸ” æ ·æœ¬æ•°æ®å¯¹æ¯”:")
        if len(matched_df) > 0:
            sample_new = matched_df.iloc[0]
            print(f"   æ–°æ•°æ®æ ·æœ¬:")
            print(f"     - å†…å®¹æŒ‡çº¹: '{sample_new.get('content_fingerprint', 'N/A')[:50]}...'")
            print(f"     - è‚¡ç¥¨ä»£ç : '{sample_new[new_code_col]}'")
            print(f"     - æ—¥æœŸéƒ¨åˆ†: '{sample_new['date_part']}'")
            print(f"     - åŸå§‹æ—¶é—´: '{sample_new[new_date_col]}'")

        if len(existing_df) > 0:
            sample_existing = existing_df.iloc[0]
            print(f"   å·²æœ‰æ•°æ®æ ·æœ¬:")
            print(f"     - å†…å®¹æŒ‡çº¹: '{sample_existing.get('content_fingerprint', 'N/A')[:50]}...'")
            print(f"     - è‚¡ç¥¨ä»£ç : '{sample_existing[existing_code_col]}'")
            print(f"     - æ—¥æœŸéƒ¨åˆ†: '{sample_existing['date_part']}'")
            print(f"     - åŸå§‹æ—¶é—´: '{sample_existing[existing_date_col]}'")

        # æ˜¾ç¤ºåŒ¹é…ç»Ÿè®¡
        total_duplicates = matched_df['is_duplicate'].sum()
        print(f"ğŸ” æœ€ç»ˆåŒ¹é…ç»Ÿè®¡:")
        print(f"   - å·²æœ‰æ•°æ®æ€»æ•°: {len(existing_df)}")
        print(f"   - æ–°æ•°æ®æ€»æ•°: {len(matched_df)}")
        print(f"   - æ£€æµ‹åˆ°é‡å¤: {total_duplicates} æ¡")

        # åˆ†ç¦»æ–°æ—§æ–°é—»
        duplicate_df = matched_df[matched_df['is_duplicate']].copy()
        new_df = matched_df[~matched_df['is_duplicate']].copy()

        # æ¸…ç†ä¸´æ—¶åˆ—
        temp_columns = ['duplicate_key', 'is_duplicate', 'content_fingerprint', 'loose_fingerprint', 'date_part',
                       'key_exact', 'key_loose', 'key_content', 'key_loose_content']

        for col in temp_columns:
            if col in new_df.columns:
                new_df = new_df.drop(col, axis=1)
            if col in duplicate_df.columns:
                duplicate_df = duplicate_df.drop(col, axis=1)

        print(f"ğŸ“Š é‡å¤æ£€æµ‹ç»“æœ:")
        print(f"   - æ€»æ–°é—»æ•°: {len(matched_df)}")
        print(f"   - é‡å¤æ–°é—»: {len(duplicate_df)} æ¡ (è·³è¿‡APIè°ƒç”¨)")
        print(f"   - æ–°å¢æ–°é—»: {len(new_df)} æ¡ (éœ€è¦LLMåˆ†æ)")

        if len(duplicate_df) > 0:
            print(f"ğŸ’° é¢„è®¡èŠ‚çœAPIè´¹ç”¨: {len(duplicate_df)} æ¬¡LLMè°ƒç”¨")

            # æ˜¾ç¤ºå‰å‡ ä¸ªé‡å¤çš„æ–°é—»å†…å®¹ä½œä¸ºå‚è€ƒ
            print(f"ğŸ“ é‡å¤æ–°é—»ç¤ºä¾‹:")
            for i, (_, row) in enumerate(duplicate_df.head(3).iterrows()):
                content_preview = str(row[new_content_col])[:60] + ('...' if len(str(row[new_content_col])) > 60 else '')
                print(f"   {i+1}. {content_preview} ({row[new_code_col]})")

        return new_df, duplicate_df

    def score_news_with_llm(self, matched_df, target_date, batch_size=4):
        """ä½¿ç”¨å¤§æ¨¡å‹æ‰¹é‡å¯¹æ–°é—»è¿›è¡Œè¯„åˆ†ï¼ˆå¸¦é‡å¤æ£€æµ‹ï¼‰"""
        if matched_df.empty:
            print("âš ï¸ åŒ¹é…çš„æ–°é—»æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡LLMè¯„åˆ†")
            return matched_df

        print(f"ğŸ¤– å¼€å§‹ä½¿ç”¨å¤§æ¨¡å‹æ‰¹é‡è¯„åˆ†...")
        print(f"   - æ€»æ–°é—»æ•°é‡: {len(matched_df)}")

        # 1. åŠ è½½å·²æœ‰ç»“æœ
        existing_df = self._load_existing_results(target_date)

        # 2. æ£€æµ‹é‡å¤
        new_df, duplicate_df = self._detect_duplicates(matched_df, existing_df)

        # 3. å¦‚æœæ²¡æœ‰æ–°æ–°é—»ï¼Œç›´æ¥è¿”å›å·²æœ‰ç»“æœ
        if new_df.empty:
            print(f"ğŸ‰ æ‰€æœ‰æ–°é—»éƒ½å·²åˆ†æè¿‡ï¼Œæ— éœ€é‡å¤APIè°ƒç”¨ï¼")
            return existing_df

        print(f"   - éœ€è¦åˆ†æçš„æ–°æ–°é—»: {len(new_df)}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")

        scored_results = []
        total_batches = (len(new_df) + batch_size - 1) // batch_size
        print(f"   - æ€»æ‰¹æ¬¡æ•°: {total_batches}")

        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(new_df))
            batch_df = new_df.iloc[start_idx:end_idx]

            print(f"  ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} (æ–°æ–°é—» {start_idx + 1}-{end_idx})")

            batch_results = self._score_batch_llm(batch_df)
            if batch_results:
                scored_results.extend(batch_results)
                print(f"    âœ… æ‰¹æ¬¡è¯„åˆ†å®Œæˆï¼Œè·å¾— {len(batch_results)} æ¡ç»“æœ")
            else:
                print(f"    âŒ æ‰¹æ¬¡è¯„åˆ†å¤±è´¥")

            # æ‰¹æ¬¡é—´ç­‰å¾…æ—¶é—´ï¼ˆé¿å…APIé€Ÿç‡é™åˆ¶ï¼‰
            if batch_idx < total_batches - 1:
                wait_time = 2 + random.uniform(0, 1)  # 2-3ç§’é—´éš”
                print(f"    â° ç­‰å¾… {wait_time:.1f} ç§’åå¤„ç†ä¸‹ä¸€æ‰¹...")
                time.sleep(wait_time)

        # 4. åˆå¹¶æ–°æ—§ç»“æœ
        new_scored_df = pd.DataFrame(scored_results)

        if not existing_df.empty and not new_scored_df.empty:
            # åˆå¹¶å·²æœ‰ç»“æœå’Œæ–°ç»“æœ
            combined_df = pd.concat([existing_df, new_scored_df], ignore_index=True)
            print(f"ğŸ”— åˆå¹¶ç»“æœ: å·²æœ‰ {len(existing_df)} + æ–°å¢ {len(new_scored_df)} = æ€»è®¡ {len(combined_df)} æ¡")
        elif not new_scored_df.empty:
            combined_df = new_scored_df
            print(f"ğŸ“Š æ–°åˆ†æç»“æœ: {len(combined_df)} æ¡")
        elif not existing_df.empty:
            combined_df = existing_df
            print(f"ğŸ“Š ä½¿ç”¨å·²æœ‰ç»“æœ: {len(combined_df)} æ¡")
        else:
            combined_df = pd.DataFrame()
            print(f"âŒ æ²¡æœ‰ä»»ä½•æœ‰æ•ˆç»“æœ")

        print(f"ğŸ¯ LLMæ‰¹é‡è¯„åˆ†å®Œæˆï¼Œæ€»å…± {len(combined_df)} æ¡")
        return combined_df

    def _score_batch_llm(self, batch_df):
        """æ‰¹é‡è¯„åˆ†ä¸€æ‰¹æ–°é—»"""
        # æ„å»ºæ‰¹é‡prompt
        prompt = self._build_batch_prompt(batch_df)

        # è°ƒç”¨APIï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        response = self._call_llm_api_with_retry(prompt)

        if response:
            return self._parse_batch_response(response, batch_df)
        else:
            return []

    def _build_batch_prompt(self, batch_df):
        """æ„å»ºæ‰¹é‡è¯„åˆ†çš„prompt"""
        prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹å¤šæ¡æ–°é—»å¯¹å¯¹åº”ä¸ªè‚¡çš„å½±å“ã€‚

è¯·å¯¹æ¯æ¡æ–°é—»ç‹¬ç«‹åˆ†æï¼Œä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼š

1. ç›´æ¥å½±å“è¯„ä¼°ï¼ˆ-5åˆ°+5åˆ†ï¼‰
   - æ–°é—»æ˜¯å¦ç›´æ¥æåŠè¯¥å…¬å¸æˆ–å…¶æ ¸å¿ƒä¸šåŠ¡ï¼Ÿ
   - å¯¹å…¬å¸æ”¶å…¥/æˆæœ¬/åˆ©æ¶¦çš„ç›´æ¥å½±å“ï¼Ÿ

2. é—´æ¥å½±å“è¯„ä¼°ï¼ˆ-5åˆ°+5åˆ†ï¼‰
   - å¯¹è¡Œä¸šæ•´ä½“çš„å½±å“ï¼Ÿ
   - å¯¹äº§ä¸šé“¾ä¸Šä¸‹æ¸¸çš„å½±å“ï¼Ÿ
   - å¯¹ç«äº‰æ ¼å±€çš„å½±å“ï¼Ÿ

3. ç¡®å®šæ€§è¯„ä¼°ï¼ˆ0-1ï¼‰
   - å½±å“å‘ç”Ÿçš„å¯èƒ½æ€§æœ‰å¤šå¤§ï¼Ÿ

4. å½±å“æ—¶é—´çª—å£
   - ç«‹å³ã€1å‘¨å†…ã€1ä¸ªæœˆå†…ã€3ä¸ªæœˆå†…ã€6ä¸ªæœˆä»¥ä¸Š

5. ç»¼åˆè¯„åˆ†ï¼ˆ-10åˆ°+10ï¼‰
   - ç»¼åˆè€ƒè™‘ç›´æ¥å½±å“ã€é—´æ¥å½±å“ã€ç¡®å®šæ€§å¾—å‡ºæ€»åˆ†

æ–°é—»åˆ—è¡¨ï¼š
"""

        for idx, (_, row) in enumerate(batch_df.iterrows()):
            prompt += f"""
ã€æ–°é—»{idx+1}ã€‘
è‚¡ç¥¨ï¼š{row['stock_name']}({row['stock_code']})
è¡Œä¸šï¼š{row.get('industry', 'N/A')}
ä¸»è¥ï¼š{row.get('main_business', 'N/A')}
æ ‡é¢˜ï¼š{str(row.get('title', ''))}
å†…å®¹ï¼š{str(row.get('content', ''))[:600]}
æ—¶é—´ï¼š{row.get('datetime', '')}
"""

        prompt += """
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ•°ç»„æ ¼å¼è¿”å›ï¼Œæ•°ç»„ä¸­æ¯ä¸ªå¯¹è±¡å¯¹åº”ä¸€æ¡æ–°é—»ï¼š
[
  {
    "news_index": 1,
    "sentiment": "å¼ºçƒˆæ­£é¢/æ­£é¢/ä¸­æ€§åæ­£/ä¸­æ€§/ä¸­æ€§åè´Ÿ/è´Ÿé¢/å¼ºçƒˆè´Ÿé¢",
    "direct_impact_score": åˆ†æ•°,
    "direct_impact_desc": "æè¿°",
    "indirect_impact_score": åˆ†æ•°,
    "indirect_impact_desc": "æè¿°",
    "certainty": 0.xx,
    "time_to_effect": "æ—¶é—´çª—å£",
    "overall_score": ç»¼åˆåˆ†æ•°,
    "risk_factors": "ä¸»è¦é£é™©å› ç´ ",
    "action_suggestion": "å»ºè®®æ“ä½œ"
  },
  ...
]
"""

        return prompt

    def _call_llm_api_with_retry(self, prompt, max_retries=3):
        """è°ƒç”¨LLM APIï¼ˆå¸¦æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶ï¼‰"""
        base_delay = 20  # åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰

        for attempt in range(max_retries):
            try:
                print(f"    ğŸ”§ è°ƒç”¨LLM API (ç¬¬{attempt+1}æ¬¡å°è¯•)...")

                payload = {
                    "model": "deepseek/deepseek-chat-v3.1:free",
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 4000
                }

                response = requests.post(
                    self.api_url,
                    headers=self.headers,
                    json=payload,
                    timeout=60
                )

                print(f"    ğŸ“Š APIå“åº”çŠ¶æ€: {response.status_code}")

                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    print(f"    ğŸ“ è¿”å›å†…å®¹é•¿åº¦: {len(content)}")
                    return content

                elif response.status_code == 429:
                    # 429é”™è¯¯ä½¿ç”¨æ›´é•¿çš„ç­‰å¾…æ—¶é—´
                    wait_time = base_delay * (2 ** attempt) + random.uniform(0, 1)
                    print(f"    âš ï¸ APIé€Ÿç‡é™åˆ¶(429)ï¼Œç­‰å¾… {wait_time:.2f} ç§’åé‡è¯• {attempt + 1}/{max_retries}")
                    if attempt < max_retries - 1:
                        time.sleep(wait_time)
                    continue

                else:
                    # å…¶ä»–é”™è¯¯ä½¿ç”¨è¾ƒçŸ­çš„ç­‰å¾…æ—¶é—´
                    wait_time = base_delay * (1.5 ** attempt)
                    print(f"    âš ï¸ APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, ç­‰å¾… {wait_time:.2f} ç§’åé‡è¯• {attempt + 1}/{max_retries}")
                    if response.text:
                        print(f"    é”™è¯¯ä¿¡æ¯: {response.text[:200]}")
                    if attempt < max_retries - 1:
                        time.sleep(wait_time)
                    continue

            except Exception as e:
                wait_time = base_delay * (2 ** attempt)
                print(f"    âŒ APIè°ƒç”¨å‡ºé”™: {e}, ç­‰å¾… {wait_time:.2f} ç§’åé‡è¯• {attempt + 1}/{max_retries}")
                if attempt < max_retries - 1:
                    time.sleep(wait_time)
                continue

        print(f"    âŒ APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
        return None

    def _parse_batch_response(self, content, batch_df):
        """è§£ææ‰¹é‡å“åº”"""
        results = []

        try:
            # æ¸…ç†å†…å®¹
            content = content.strip()
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()

            # æŸ¥æ‰¾JSONæ•°ç»„
            start = content.find('[')
            end = content.rfind(']') + 1

            if start != -1 and end > start:
                json_str = content[start:end]

                # æ¸…ç†å¯èƒ½çš„é—®é¢˜å­—ç¬¦
                json_str = json_str.replace('\n', '').replace('\r', '').replace('\t', '')

                # å°è¯•è§£æ
                parsed_array = json.loads(json_str)

                # éªŒè¯æ˜¯å¦ä¸ºæ•°ç»„
                if isinstance(parsed_array, list):
                    for i, analysis_result in enumerate(parsed_array):
                        if i < len(batch_df):
                            # è·å–å¯¹åº”çš„æ–°é—»è¡Œ
                            row = batch_df.iloc[i]

                            # æ„å»ºç»“æœ
                            result = row.to_dict()
                            result['analysis_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                            # éªŒè¯å¿…è¦å­—æ®µ
                            required_fields = ['sentiment', 'overall_score', 'certainty']
                            for field in required_fields:
                                if field not in analysis_result:
                                    analysis_result[field] = 0 if field in ['overall_score', 'certainty'] else "ä¸­æ€§"

                            result.update(analysis_result)
                            results.append(result)

                print(f"    ğŸ“ è§£æå®Œæˆï¼Œè·å¾— {len(results)} æ¡åˆ†æç»“æœ")
                return results
            else:
                print(f"    âŒ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ•°ç»„")
                return []

        except json.JSONDecodeError as e:
            print(f"    âŒ JSONè§£æé”™è¯¯: {e}")
            print(f"    åŸå§‹å“åº”: {content[:500]}...")
            return []
        except Exception as e:
            print(f"    âŒ è§£æå‡ºé”™: {e}")
            return []

    def generate_factor_report(self, scored_df, target_date):
        """ç”Ÿæˆå› å­å¼ºåº¦æŠ¥å‘Š"""
        if scored_df.empty:
            print("âš ï¸ æ²¡æœ‰è¯„åˆ†æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆå› å­æŠ¥å‘Š")
            return

        print(f"ğŸ“Š ç”Ÿæˆå› å­å¼ºåº¦æŠ¥å‘Š...")

        # è®¡ç®—æ¯åªè‚¡ç¥¨çš„å› å­å¼ºåº¦
        stock_factors = []

        for stock_code in scored_df['stock_code'].unique():
            stock_news = scored_df[scored_df['stock_code'] == stock_code]

            stock_name = stock_news.iloc[0]['stock_name']

            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            news_count = len(stock_news)
            avg_score = stock_news['overall_score'].mean()
            max_score = stock_news['overall_score'].max()
            min_score = stock_news['overall_score'].min()
            avg_certainty = stock_news['certainty'].mean()

            # è®¡ç®—å› å­å¼ºåº¦ = å¹³å‡åˆ† * 0.4 + æœ€é«˜åˆ† * 0.3 + ç¡®å®šæ€§*10 * 0.3
            factor_strength = avg_score * 0.4 + max_score * 0.3 + avg_certainty * 10 * 0.3

            stock_factors.append({
                'stock_code': stock_code,
                'stock_name': stock_name,
                'news_count': news_count,
                'avg_score': round(avg_score, 2),
                'max_score': round(max_score, 2),
                'min_score': round(min_score, 2),
                'avg_certainty': round(avg_certainty, 3),
                'factor_strength': round(factor_strength, 2)
            })

        # è½¬æ¢ä¸ºDataFrameå¹¶æ’åº
        factor_df = pd.DataFrame(stock_factors)
        factor_df = factor_df.sort_values('factor_strength', ascending=False)

        # ä¿å­˜è¯¦ç»†ç»“æœ
        detail_filename = self.output_dir / f"news_analysis_detail_{target_date.replace('-', '')}.csv"
        scored_df.to_csv(detail_filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜: {detail_filename}")

        # ä¿å­˜å› å­æŠ¥å‘Š
        factor_filename = self.output_dir / f"factor_strength_{target_date.replace('-', '')}.csv"
        factor_df.to_csv(factor_filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ å› å­å¼ºåº¦æŠ¥å‘Šå·²ä¿å­˜: {factor_filename}")

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(factor_df, scored_df, target_date)

        print(f"ğŸ¯ å› å­æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")

    def _generate_markdown_report(self, factor_df, scored_df, target_date):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        report = f"""# æ¯æ—¥æ–°é—»å› å­åˆ†ææŠ¥å‘Š

**åˆ†ææ—¥æœŸ**: {target_date}
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†ææ–°é—»æ€»æ•°**: {len(scored_df)}
**æ¶‰åŠè‚¡ç¥¨æ•°é‡**: {len(factor_df)}

## å› å­å¼ºåº¦æ’è¡Œæ¦œ

| æ’å | è‚¡ç¥¨ä»£ç  | è‚¡ç¥¨åç§° | æ–°é—»æ•°é‡ | å¹³å‡åˆ† | æœ€é«˜åˆ† | ç¡®å®šæ€§ | **å› å­å¼ºåº¦** |
|------|----------|----------|----------|--------|--------|--------|--------------|
"""

        for i, (idx, row) in enumerate(factor_df.head(15).iterrows()):
            report += f"| {i+1} | {row['stock_code']} | {row['stock_name']} | {row['news_count']} | {row['avg_score']} | {row['max_score']} | {row['avg_certainty']} | **{row['factor_strength']}** |\n"

        report += f"""
## é«˜å…³æ³¨è‚¡ç¥¨è¯¦æƒ…

"""

        # æ˜¾ç¤ºå‰5åªé«˜å› å­å¼ºåº¦è‚¡ç¥¨çš„è¯¦ç»†æ–°é—»
        for i, (_, stock) in enumerate(factor_df.head(5).iterrows()):
            stock_code = stock['stock_code']
            stock_name = stock['stock_name']

            report += f"""### {stock_name} ({stock_code})
**å› å­å¼ºåº¦**: {stock['factor_strength']} | **æ–°é—»æ•°é‡**: {stock['news_count']}

"""

            stock_news = scored_df[scored_df['stock_code'] == stock_code].sort_values('overall_score', ascending=False)

            for _, news in stock_news.iterrows():
                sentiment = news['sentiment']
                score = news['overall_score']
                title = str(news.get('title', ''))[:60]

                report += f"- **[{sentiment}] {score}åˆ†** - {title}...\n"

            report += "\n"

        report += f"""
---
*æŠ¥å‘Šç”±æ–°é—»å› å­åˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
*å› å­å¼ºåº¦è®¡ç®—å…¬å¼: å¹³å‡åˆ† Ã— 0.4 + æœ€é«˜åˆ† Ã— 0.3 + ç¡®å®šæ€§Ã—10 Ã— 0.3*
"""

        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_filename = self.output_dir / f"factor_report_{target_date.replace('-', '')}.md"
        with open(md_filename, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"ğŸ“ MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {md_filename}")

def main():
    """æ­£å¼ç‰ˆä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ­£å¼ç‰ˆæ–°é—»åˆ†æï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰...")

    # å¯¼å…¥é…ç½®
    import sys
    sys.path.append(str(Path(__file__).parent.parent / "config"))
    from api_config import TUSHARE_TOKEN, OPENROUTER_API_KEY

    print(f"ğŸ”‘ é…ç½®æ£€æŸ¥:")
    print(f"   - Tushare Token: {'å·²é…ç½®' if TUSHARE_TOKEN != 'your_tushare_token_here' else 'æœªé…ç½®'}")
    print(f"   - OpenRouter Key: {'å·²é…ç½®' if OPENROUTER_API_KEY != 'your_openrouter_api_key_here' else 'æœªé…ç½®'}")

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = DailyNewsAnalyzer(TUSHARE_TOKEN, OPENROUTER_API_KEY)

    # å®Œæ•´æµç¨‹
    target_date = "2025-09-25"  # å¯ä»¥ä¿®æ”¹ä¸ºéœ€è¦çš„æ—¥æœŸ

    print(f"\nğŸ“° æ­¥éª¤1: è·å–æ–°é—»æ•°æ®...")
    news_df = analyzer.get_daily_news(target_date)

    if hasattr(news_df, 'empty') and not news_df.empty:
        print(f"\nğŸ” æ­¥éª¤2: åŒ¹é…è‚¡ç¥¨ç›¸å…³æ–°é—»...")
        matched_df = analyzer.match_news_to_stocks(news_df)

        if not matched_df.empty:
            print(f"\nğŸ¤– æ­¥éª¤3: LLMæ‰¹é‡æƒ…æ„Ÿåˆ†æï¼ˆå¸¦é‡å¤æ£€æµ‹ï¼‰...")
            scored_df = analyzer.score_news_with_llm(matched_df, target_date, batch_size=4)

            if not scored_df.empty:
                print(f"\nğŸ“Š æ­¥éª¤4: ç”Ÿæˆå› å­å¼ºåº¦æŠ¥å‘Š...")
                analyzer.generate_factor_report(scored_df, target_date)
            else:
                print("âŒ LLMè¯„åˆ†å¤±è´¥")
        else:
            print("âŒ æ²¡æœ‰åŒ¹é…åˆ°è‚¡ç¥¨ç›¸å…³æ–°é—»")
    else:
        print("âŒ æ²¡æœ‰æ–°é—»æ•°æ®")

    print(f"\nğŸ‰ æ­£å¼ç‰ˆåˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()