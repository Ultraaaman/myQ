#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥æ–°é—»åˆ†æè„šæœ¬ï¼ˆæ­£å¼ç‰ˆv2 - æ‰¹é‡LLMå¤„ç† + é‡è¯•æœºåˆ¶ï¼‰
"""

import tushare as ts
import pandas as pd
import json
import requests
import os
import time
import random
from datetime import datetime, timedelta
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DailyNewsAnalyzer:
    def __init__(self, tushare_token, openrouter_api_key):
        """åˆå§‹åŒ–æ–°é—»åˆ†æå™¨"""
        print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–åˆ†æå™¨...")

        # åˆå§‹åŒ–API
        print(f"ğŸ”§ åˆå§‹åŒ–Tushare API...")
        self.ts_pro = ts.pro_api(tushare_token)
        print(f"âœ… Tushare APIåˆå§‹åŒ–å®Œæˆ")

        self.api_key = openrouter_api_key
        self.api_url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {openrouter_api_key}",
            "Content-Type": "application/json",
        }

        # è®¾ç½®æ–‡ä»¶è·¯å¾„
        print(f"ğŸ”§ è®¾ç½®æ–‡ä»¶è·¯å¾„...")
        self.base_dir = Path("D:/projects/q/myQ")
        self.output_dir = self.base_dir / "output" / "daily_analysis"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        print(f"âœ… è¾“å‡ºç›®å½•: {self.output_dir}")

        # åŠ è½½è‚¡ç¥¨æ± 
        print(f"ğŸ”§ åŠ è½½è‚¡ç¥¨æ± ...")
        self.stock_pool = self._load_stock_pool()
        print(f"âœ… è‚¡ç¥¨æ± åŠ è½½å®Œæˆ: {len(self.stock_pool)} åªè‚¡ç¥¨")

        print(f"ğŸ”§ æå–å…³é”®è¯...")
        self.stock_keywords = self._extract_keywords()
        print(f"âœ… å…³é”®è¯æå–å®Œæˆ: {len(self.stock_keywords)} ä¸ªè‚¡ç¥¨")

    def _load_stock_pool(self):
        """åŠ è½½è‚¡ç¥¨æ± """
        stock_pool_path = self.base_dir / "config" / "stock_pool.json"
        print(f"ğŸ“ è¯»å–è‚¡ç¥¨æ± æ–‡ä»¶: {stock_pool_path}")

        with open(stock_pool_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"ğŸ“Š åŸå§‹JSONæ•°æ®é”®: {list(data.keys())}")

        # æ”¯æŒä¸åŒçš„JSONæ ¼å¼
        if 'stocks' in data:
            stocks = data['stocks']
            print(f"âœ… ä½¿ç”¨ 'stocks' é”®")
        elif 'stock_database' in data:
            stocks = data['stock_database']
            print(f"âœ… ä½¿ç”¨ 'stock_database' é”®")
        else:
            stocks = data if isinstance(data, list) else []
            print(f"âœ… æ•°æ®æœ¬èº«å°±æ˜¯åˆ—è¡¨")

        print(f"ğŸ“Š æœ€ç»ˆè‚¡ç¥¨æ•°é‡: {len(stocks)}")
        return stocks

    def _extract_keywords(self):
        """æå–å…³é”®è¯"""
        keywords = {}

        for i, stock in enumerate(self.stock_pool):
            stock_keywords = []
            stock_keywords.append(stock['stock_name'])
            stock_keywords.append(stock['stock_code'])

            # æå–è¡Œä¸šå’Œä¸»è¥ä¸šåŠ¡å…³é”®è¯
            if 'industry' in stock:
                stock_keywords.extend(stock['industry'].split())
            if 'main_business' in stock:
                business_words = stock['main_business'].replace('ï¼Œ', ' ').replace('ã€', ' ').split()
                business_words = [w for w in business_words if len(w) >= 2][:5]
                stock_keywords.extend(business_words)

            # å»é‡
            keywords[stock['stock_code']] = list(set(stock_keywords))

            # æ˜¾ç¤ºå‰3ä¸ªè‚¡ç¥¨çš„å…³é”®è¯ç”¨äºè°ƒè¯•
            if i < 3:
                print(f"  {stock['stock_name']} ({stock['stock_code']}): {keywords[stock['stock_code']]}")

        return keywords

    def get_daily_news(self, target_date=None):
        """è·å–å½“æ—¥æ–°é—»"""
        if target_date is None:
            start_date = datetime.now().strftime('%Y-%m-%d 09:00:00')
            end_date = datetime.now().strftime('%Y-%m-%d 18:00:00')
        else:
            start_date = f"{target_date} 09:00:00"
            end_date = f"{target_date} 18:00:00"

        print(f"ğŸ“° å‡†å¤‡è·å–æ–°é—»æ•°æ®...")
        print(f"   - å¼€å§‹æ—¶é—´: {start_date}")
        print(f"   - ç»“æŸæ—¶é—´: {end_date}")

        news_df = self.ts_pro.news(
            src='sina',
            start_date=start_date,
            end_date=end_date
        )

        print(f"ğŸ“Š APIè¿”å›ç»“æœ:")
        print(f"   - æ•°æ®ç±»å‹: {type(news_df)}")
        print(f"   - æ˜¯å¦ä¸ºç©º: {news_df.empty if hasattr(news_df, 'empty') else 'N/A'}")
        print(f"   - æ•°æ®å½¢çŠ¶: {news_df.shape if hasattr(news_df, 'shape') else 'N/A'}")

        if hasattr(news_df, 'columns'):
            print(f"   - åˆ—å: {list(news_df.columns)}")

        return news_df

    def match_news_to_stocks(self, news_df):
        """åŒ¹é…æ–°é—»åˆ°è‚¡ç¥¨"""
        if news_df.empty:
            print("âš ï¸ æ–°é—»æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åŒ¹é…")
            return news_df

        print(f"ğŸ” å¼€å§‹åŒ¹é…æ–°é—»åˆ°è‚¡ç¥¨...")
        matched_records = []

        for i, (_, news_row) in enumerate(news_df.iterrows()):
            if i < 2:  # æ˜¾ç¤ºå‰2æ¡æ–°é—»çš„è°ƒè¯•ä¿¡æ¯
                title = news_row.get('title') or news_row.get('Title') or ''
                print(f"  å¤„ç†ç¬¬ {i+1} æ¡æ–°é—»: {str(title)[:50] if title else '(æ— æ ‡é¢˜)'}...")

            # å®‰å…¨è·å–å­—æ®µå€¼
            title = news_row.get('title') or news_row.get('Title') or ''
            content = news_row.get('content') or news_row.get('Content') or ''

            news_title = str(title).lower() if title else ''
            news_content = str(content).lower() if content else ''
            full_text = f"{news_title} {news_content}"

            # åŒ¹é…è‚¡ç¥¨
            for stock in self.stock_pool:
                stock_code = stock['stock_code']
                keywords = self.stock_keywords.get(stock_code, [])

                # æ£€æŸ¥æ˜¯å¦åŒ¹é…
                matched = False
                matched_keyword = None
                for keyword in keywords:
                    if len(keyword) >= 2 and keyword.lower() in full_text:
                        matched = True
                        matched_keyword = keyword
                        break

                if matched:
                    record = news_row.to_dict()
                    record.update(stock)
                    matched_records.append(record)

        matched_df = pd.DataFrame(matched_records)
        print(f"ğŸ¯ åŒ¹é…å®Œæˆï¼Œæ‰¾åˆ° {len(matched_df)} æ¡è‚¡ç¥¨ç›¸å…³æ–°é—»")

        return matched_df

    def score_news_with_llm(self, matched_df, batch_size=4):
        """ä½¿ç”¨å¤§æ¨¡å‹æ‰¹é‡å¯¹æ–°é—»è¿›è¡Œè¯„åˆ†"""
        if matched_df.empty:
            print("âš ï¸ åŒ¹é…çš„æ–°é—»æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡LLMè¯„åˆ†")
            return matched_df

        print(f"ğŸ¤– å¼€å§‹ä½¿ç”¨å¤§æ¨¡å‹æ‰¹é‡è¯„åˆ†...")
        print(f"   - æ€»æ–°é—»æ•°é‡: {len(matched_df)}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")

        scored_results = []
        total_batches = (len(matched_df) + batch_size - 1) // batch_size
        print(f"   - æ€»æ‰¹æ¬¡æ•°: {total_batches}")

        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(matched_df))
            batch_df = matched_df.iloc[start_idx:end_idx]

            print(f"  ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} (æ–°é—» {start_idx + 1}-{end_idx})")

            batch_results = self._score_batch_llm(batch_df)
            if batch_results:
                scored_results.extend(batch_results)
                print(f"    âœ… æ‰¹æ¬¡è¯„åˆ†å®Œæˆï¼Œè·å¾— {len(batch_results)} æ¡ç»“æœ")
            else:
                print(f"    âŒ æ‰¹æ¬¡è¯„åˆ†å¤±è´¥")

            # æ‰¹æ¬¡é—´ç­‰å¾…æ—¶é—´ï¼ˆé¿å…APIé€Ÿç‡é™åˆ¶ï¼‰
            if batch_idx < total_batches - 1:
                wait_time = 2 + random.uniform(0, 1)  # 2-3ç§’é—´éš”
                print(f"    â° ç­‰å¾… {wait_time:.1f} ç§’åå¤„ç†ä¸‹ä¸€æ‰¹...")
                time.sleep(wait_time)

        scored_df = pd.DataFrame(scored_results)
        print(f"ğŸ¯ LLMæ‰¹é‡è¯„åˆ†å®Œæˆï¼Œå…± {len(scored_df)} æ¡")

        return scored_df

    def _score_batch_llm(self, batch_df):
        """æ‰¹é‡è¯„åˆ†ä¸€æ‰¹æ–°é—»"""
        # æ„å»ºæ‰¹é‡prompt
        prompt = self._build_batch_prompt(batch_df)

        # è°ƒç”¨APIï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        response = self._call_llm_api_with_retry(prompt)

        if response:
            return self._parse_batch_response(response, batch_df)
        else:
            return []

    def _build_batch_prompt(self, batch_df):
        """æ„å»ºæ‰¹é‡è¯„åˆ†çš„prompt"""
        prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹å¤šæ¡æ–°é—»å¯¹å¯¹åº”ä¸ªè‚¡çš„å½±å“ã€‚

è¯·å¯¹æ¯æ¡æ–°é—»ç‹¬ç«‹åˆ†æï¼Œä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼š

1. ç›´æ¥å½±å“è¯„ä¼°ï¼ˆ-5åˆ°+5åˆ†ï¼‰
   - æ–°é—»æ˜¯å¦ç›´æ¥æåŠè¯¥å…¬å¸æˆ–å…¶æ ¸å¿ƒä¸šåŠ¡ï¼Ÿ
   - å¯¹å…¬å¸æ”¶å…¥/æˆæœ¬/åˆ©æ¶¦çš„ç›´æ¥å½±å“ï¼Ÿ

2. é—´æ¥å½±å“è¯„ä¼°ï¼ˆ-5åˆ°+5åˆ†ï¼‰
   - å¯¹è¡Œä¸šæ•´ä½“çš„å½±å“ï¼Ÿ
   - å¯¹äº§ä¸šé“¾ä¸Šä¸‹æ¸¸çš„å½±å“ï¼Ÿ
   - å¯¹ç«äº‰æ ¼å±€çš„å½±å“ï¼Ÿ

3. ç¡®å®šæ€§è¯„ä¼°ï¼ˆ0-1ï¼‰
   - å½±å“å‘ç”Ÿçš„å¯èƒ½æ€§æœ‰å¤šå¤§ï¼Ÿ

4. å½±å“æ—¶é—´çª—å£
   - ç«‹å³ã€1å‘¨å†…ã€1ä¸ªæœˆå†…ã€3ä¸ªæœˆå†…ã€6ä¸ªæœˆä»¥ä¸Š

5. ç»¼åˆè¯„åˆ†ï¼ˆ-10åˆ°+10ï¼‰
   - ç»¼åˆè€ƒè™‘ç›´æ¥å½±å“ã€é—´æ¥å½±å“ã€ç¡®å®šæ€§å¾—å‡ºæ€»åˆ†

æ–°é—»åˆ—è¡¨ï¼š
"""

        for idx, (_, row) in enumerate(batch_df.iterrows()):
            prompt += f"""
ã€æ–°é—»{idx+1}ã€‘
è‚¡ç¥¨ï¼š{row['stock_name']}({row['stock_code']})
è¡Œä¸šï¼š{row.get('industry', 'N/A')}
ä¸»è¥ï¼š{row.get('main_business', 'N/A')}
æ ‡é¢˜ï¼š{str(row.get('title', ''))}
å†…å®¹ï¼š{str(row.get('content', ''))[:600]}
æ—¶é—´ï¼š{row.get('datetime', '')}
"""

        prompt += """
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ•°ç»„æ ¼å¼è¿”å›ï¼Œæ•°ç»„ä¸­æ¯ä¸ªå¯¹è±¡å¯¹åº”ä¸€æ¡æ–°é—»ï¼š
[
  {
    "news_index": 1,
    "sentiment": "å¼ºçƒˆæ­£é¢/æ­£é¢/ä¸­æ€§åæ­£/ä¸­æ€§/ä¸­æ€§åè´Ÿ/è´Ÿé¢/å¼ºçƒˆè´Ÿé¢",
    "direct_impact_score": åˆ†æ•°,
    "direct_impact_desc": "æè¿°",
    "indirect_impact_score": åˆ†æ•°,
    "indirect_impact_desc": "æè¿°",
    "certainty": 0.xx,
    "time_to_effect": "æ—¶é—´çª—å£",
    "overall_score": ç»¼åˆåˆ†æ•°,
    "risk_factors": "ä¸»è¦é£é™©å› ç´ ",
    "action_suggestion": "å»ºè®®æ“ä½œ"
  },
  ...
]
"""

        return prompt

    def _call_llm_api_with_retry(self, prompt, max_retries=3):
        """è°ƒç”¨LLM APIï¼ˆå¸¦æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶ï¼‰"""
        base_delay = 20  # åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰

        for attempt in range(max_retries):
            try:
                print(f"    ğŸ”§ è°ƒç”¨LLM API (ç¬¬{attempt+1}æ¬¡å°è¯•)...")

                payload = {
                    "model": "deepseek/deepseek-chat-v3.1:free",
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 4000
                }

                response = requests.post(
                    self.api_url,
                    headers=self.headers,
                    json=payload,
                    timeout=60
                )

                print(f"    ğŸ“Š APIå“åº”çŠ¶æ€: {response.status_code}")

                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    print(f"    ğŸ“ è¿”å›å†…å®¹é•¿åº¦: {len(content)}")
                    return content

                elif response.status_code == 429:
                    # 429é”™è¯¯ä½¿ç”¨æ›´é•¿çš„ç­‰å¾…æ—¶é—´
                    wait_time = base_delay * (2 ** attempt) + random.uniform(0, 1)
                    print(f"    âš ï¸ APIé€Ÿç‡é™åˆ¶(429)ï¼Œç­‰å¾… {wait_time:.2f} ç§’åé‡è¯• {attempt + 1}/{max_retries}")
                    if attempt < max_retries - 1:
                        time.sleep(wait_time)
                    continue

                else:
                    # å…¶ä»–é”™è¯¯ä½¿ç”¨è¾ƒçŸ­çš„ç­‰å¾…æ—¶é—´
                    wait_time = base_delay * (1.5 ** attempt)
                    print(f"    âš ï¸ APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, ç­‰å¾… {wait_time:.2f} ç§’åé‡è¯• {attempt + 1}/{max_retries}")
                    if response.text:
                        print(f"    é”™è¯¯ä¿¡æ¯: {response.text[:200]}")
                    if attempt < max_retries - 1:
                        time.sleep(wait_time)
                    continue

            except Exception as e:
                wait_time = base_delay * (2 ** attempt)
                print(f"    âŒ APIè°ƒç”¨å‡ºé”™: {e}, ç­‰å¾… {wait_time:.2f} ç§’åé‡è¯• {attempt + 1}/{max_retries}")
                if attempt < max_retries - 1:
                    time.sleep(wait_time)
                continue

        print(f"    âŒ APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
        return None

    def _parse_batch_response(self, content, batch_df):
        """è§£ææ‰¹é‡å“åº”"""
        results = []

        try:
            # æ¸…ç†å†…å®¹
            content = content.strip()
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()

            # æŸ¥æ‰¾JSONæ•°ç»„
            start = content.find('[')
            end = content.rfind(']') + 1

            if start != -1 and end > start:
                json_str = content[start:end]

                # æ¸…ç†å¯èƒ½çš„é—®é¢˜å­—ç¬¦
                json_str = json_str.replace('\n', '').replace('\r', '').replace('\t', '')

                # å°è¯•è§£æ
                parsed_array = json.loads(json_str)

                # éªŒè¯æ˜¯å¦ä¸ºæ•°ç»„
                if isinstance(parsed_array, list):
                    for i, analysis_result in enumerate(parsed_array):
                        if i < len(batch_df):
                            # è·å–å¯¹åº”çš„æ–°é—»è¡Œ
                            row = batch_df.iloc[i]

                            # æ„å»ºç»“æœ
                            result = row.to_dict()
                            result['analysis_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                            # éªŒè¯å¿…è¦å­—æ®µ
                            required_fields = ['sentiment', 'overall_score', 'certainty']
                            for field in required_fields:
                                if field not in analysis_result:
                                    analysis_result[field] = 0 if field in ['overall_score', 'certainty'] else "ä¸­æ€§"

                            result.update(analysis_result)
                            results.append(result)

                print(f"    ğŸ“ è§£æå®Œæˆï¼Œè·å¾— {len(results)} æ¡åˆ†æç»“æœ")
                return results
            else:
                print(f"    âŒ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ•°ç»„")
                return []

        except json.JSONDecodeError as e:
            print(f"    âŒ JSONè§£æé”™è¯¯: {e}")
            print(f"    åŸå§‹å“åº”: {content[:500]}...")
            return []
        except Exception as e:
            print(f"    âŒ è§£æå‡ºé”™: {e}")
            return []

    def generate_factor_report(self, scored_df, target_date):
        """ç”Ÿæˆå› å­å¼ºåº¦æŠ¥å‘Š"""
        if scored_df.empty:
            print("âš ï¸ æ²¡æœ‰è¯„åˆ†æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆå› å­æŠ¥å‘Š")
            return

        print(f"ğŸ“Š ç”Ÿæˆå› å­å¼ºåº¦æŠ¥å‘Š...")

        # è®¡ç®—æ¯åªè‚¡ç¥¨çš„å› å­å¼ºåº¦
        stock_factors = []

        for stock_code in scored_df['stock_code'].unique():
            stock_news = scored_df[scored_df['stock_code'] == stock_code]

            stock_name = stock_news.iloc[0]['stock_name']

            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            news_count = len(stock_news)
            avg_score = stock_news['overall_score'].mean()
            max_score = stock_news['overall_score'].max()
            min_score = stock_news['overall_score'].min()
            avg_certainty = stock_news['certainty'].mean()

            # è®¡ç®—å› å­å¼ºåº¦ = å¹³å‡åˆ† * 0.4 + æœ€é«˜åˆ† * 0.3 + ç¡®å®šæ€§*10 * 0.3
            factor_strength = avg_score * 0.4 + max_score * 0.3 + avg_certainty * 10 * 0.3

            stock_factors.append({
                'stock_code': stock_code,
                'stock_name': stock_name,
                'news_count': news_count,
                'avg_score': round(avg_score, 2),
                'max_score': round(max_score, 2),
                'min_score': round(min_score, 2),
                'avg_certainty': round(avg_certainty, 3),
                'factor_strength': round(factor_strength, 2)
            })

        # è½¬æ¢ä¸ºDataFrameå¹¶æ’åº
        factor_df = pd.DataFrame(stock_factors)
        factor_df = factor_df.sort_values('factor_strength', ascending=False)

        # ä¿å­˜è¯¦ç»†ç»“æœ
        detail_filename = self.output_dir / f"news_analysis_detail_{target_date.replace('-', '')}.csv"
        scored_df.to_csv(detail_filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜: {detail_filename}")

        # ä¿å­˜å› å­æŠ¥å‘Š
        factor_filename = self.output_dir / f"factor_strength_{target_date.replace('-', '')}.csv"
        factor_df.to_csv(factor_filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ å› å­å¼ºåº¦æŠ¥å‘Šå·²ä¿å­˜: {factor_filename}")

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(factor_df, scored_df, target_date)

        print(f"ğŸ¯ å› å­æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")

    def _generate_markdown_report(self, factor_df, scored_df, target_date):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        report = f"""# æ¯æ—¥æ–°é—»å› å­åˆ†ææŠ¥å‘Š

**åˆ†ææ—¥æœŸ**: {target_date}
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†ææ–°é—»æ€»æ•°**: {len(scored_df)}
**æ¶‰åŠè‚¡ç¥¨æ•°é‡**: {len(factor_df)}

## å› å­å¼ºåº¦æ’è¡Œæ¦œ

| æ’å | è‚¡ç¥¨ä»£ç  | è‚¡ç¥¨åç§° | æ–°é—»æ•°é‡ | å¹³å‡åˆ† | æœ€é«˜åˆ† | ç¡®å®šæ€§ | **å› å­å¼ºåº¦** |
|------|----------|----------|----------|--------|--------|--------|--------------|
"""

        for i, (idx, row) in enumerate(factor_df.head(15).iterrows()):
            report += f"| {i+1} | {row['stock_code']} | {row['stock_name']} | {row['news_count']} | {row['avg_score']} | {row['max_score']} | {row['avg_certainty']} | **{row['factor_strength']}** |\n"

        report += f"""
## é«˜å…³æ³¨è‚¡ç¥¨è¯¦æƒ…

"""

        # æ˜¾ç¤ºå‰5åªé«˜å› å­å¼ºåº¦è‚¡ç¥¨çš„è¯¦ç»†æ–°é—»
        for i, (_, stock) in enumerate(factor_df.head(5).iterrows()):
            stock_code = stock['stock_code']
            stock_name = stock['stock_name']

            report += f"""### {stock_name} ({stock_code})
**å› å­å¼ºåº¦**: {stock['factor_strength']} | **æ–°é—»æ•°é‡**: {stock['news_count']}

"""

            stock_news = scored_df[scored_df['stock_code'] == stock_code].sort_values('overall_score', ascending=False)

            for _, news in stock_news.iterrows():
                sentiment = news['sentiment']
                score = news['overall_score']
                title = str(news.get('title', ''))[:60]

                report += f"- **[{sentiment}] {score}åˆ†** - {title}...\n"

            report += "\n"

        report += f"""
---
*æŠ¥å‘Šç”±æ–°é—»å› å­åˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
*å› å­å¼ºåº¦è®¡ç®—å…¬å¼: å¹³å‡åˆ† Ã— 0.4 + æœ€é«˜åˆ† Ã— 0.3 + ç¡®å®šæ€§Ã—10 Ã— 0.3*
"""

        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_filename = self.output_dir / f"factor_report_{target_date.replace('-', '')}.md"
        with open(md_filename, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"ğŸ“ MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {md_filename}")

def main():
    """æ­£å¼ç‰ˆä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ­£å¼ç‰ˆæ–°é—»åˆ†æï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰...")

    # å¯¼å…¥é…ç½®
    import sys
    sys.path.append(str(Path(__file__).parent.parent / "config"))
    from api_config import TUSHARE_TOKEN, OPENROUTER_API_KEY

    print(f"ğŸ”‘ é…ç½®æ£€æŸ¥:")
    print(f"   - Tushare Token: {'å·²é…ç½®' if TUSHARE_TOKEN != 'your_tushare_token_here' else 'æœªé…ç½®'}")
    print(f"   - OpenRouter Key: {'å·²é…ç½®' if OPENROUTER_API_KEY != 'your_openrouter_api_key_here' else 'æœªé…ç½®'}")

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = DailyNewsAnalyzer(TUSHARE_TOKEN, OPENROUTER_API_KEY)

    # å®Œæ•´æµç¨‹
    target_date = "2024-12-20"  # å¯ä»¥ä¿®æ”¹ä¸ºéœ€è¦çš„æ—¥æœŸ

    print(f"\nğŸ“° æ­¥éª¤1: è·å–æ–°é—»æ•°æ®...")
    news_df = analyzer.get_daily_news(target_date)

    if hasattr(news_df, 'empty') and not news_df.empty:
        print(f"\nğŸ” æ­¥éª¤2: åŒ¹é…è‚¡ç¥¨ç›¸å…³æ–°é—»...")
        matched_df = analyzer.match_news_to_stocks(news_df)

        if not matched_df.empty:
            print(f"\nğŸ¤– æ­¥éª¤3: LLMæ‰¹é‡æƒ…æ„Ÿåˆ†æ...")
            scored_df = analyzer.score_news_with_llm(matched_df, batch_size=4)

            if not scored_df.empty:
                print(f"\nğŸ“Š æ­¥éª¤4: ç”Ÿæˆå› å­å¼ºåº¦æŠ¥å‘Š...")
                analyzer.generate_factor_report(scored_df, target_date)
            else:
                print("âŒ LLMè¯„åˆ†å¤±è´¥")
        else:
            print("âŒ æ²¡æœ‰åŒ¹é…åˆ°è‚¡ç¥¨ç›¸å…³æ–°é—»")
    else:
        print("âŒ æ²¡æœ‰æ–°é—»æ•°æ®")

    print(f"\nğŸ‰ æ­£å¼ç‰ˆåˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()